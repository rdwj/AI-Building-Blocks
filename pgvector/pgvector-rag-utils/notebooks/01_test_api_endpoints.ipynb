{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test API Endpoints\n",
    "\n",
    "This notebook tests the LLM, embedding, and document processing endpoints, with some prompt engineering experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Pretty print JSON\n",
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Nomic Embed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomic Embed URL: https://nomic-embed-text-v1-5-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443\n",
      "Nomic Embed Model: /mnt/models\n",
      "Nomic Embed URL: https://nomic-embed-text-v1-5-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\n",
      "Model: /mnt/models\n"
     ]
    }
   ],
   "source": [
    "# Get Nomic Embed configuration\n",
    "NOMIC_URL = os.getenv('NOMIC_EMBED_URL')\n",
    "NOMIC_API_KEY = os.getenv('NOMIC_EMBED_API_KEY')\n",
    "NOMIC_MODEL = os.getenv('NOMIC_EMBED_MODEL_NAME')\n",
    "\n",
    "print(f\"Nomic Embed URL: {NOMIC_URL}\")\n",
    "print(f\"Nomic Embed Model: {NOMIC_MODEL}\")\n",
    "\n",
    "if NOMIC_URL and not NOMIC_URL.endswith('/v1'):\n",
    "    NOMIC_URL = f\"{NOMIC_URL}/v1\"\n",
    "\n",
    "print(f\"Nomic Embed URL: {NOMIC_URL}\")\n",
    "print(f\"Model: {NOMIC_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str]) -> List[np.ndarray] | None:\n",
    "    \"\"\"Get embeddings from Nomic Embed API\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        response = requests.post(\n",
    "            f\"{NOMIC_URL}/embeddings\",\n",
    "            headers={\n",
    "                'Authorization': f\"Bearer {NOMIC_API_KEY}\",\n",
    "                'Content-Type': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                'model': NOMIC_MODEL,\n",
    "                'input': text\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            embedding = np.array(data['data'][0]['embedding'])\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embeddings...\n",
      "\n",
      "✅ Successfully generated 3 embeddings\n",
      "Embedding dimension: 768\n",
      "\n",
      "Cosine similarities:\n",
      "Text 1 <-> Text 2: 0.4475\n",
      "Text 1 <-> Text 3: 0.3960\n",
      "Text 2 <-> Text 3: 0.4076\n"
     ]
    }
   ],
   "source": [
    "# Test embeddings\n",
    "test_texts = [\n",
    "    \"PGVector is a PostgreSQL extension for vector similarity search.\",\n",
    "    \"Machine learning models can generate embeddings for text.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"Testing embeddings...\")\n",
    "embeddings = get_embeddings(test_texts)\n",
    "\n",
    "if embeddings:\n",
    "    print(f\"\\n✅ Successfully generated {len(embeddings)} embeddings\")\n",
    "    print(f\"Embedding dimension: {embeddings[0].shape[0]}\")\n",
    "    \n",
    "    # Calculate similarity between texts\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    print(\"\\nCosine similarities:\")\n",
    "    for i in range(len(test_texts)):\n",
    "        for j in range(i+1, len(test_texts)):\n",
    "            print(f\"Text {i+1} <-> Text {j+1}: {similarities[i][j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Llama 3.2 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama URL: https://llama-3-2-3b-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\n",
      "Model: llama-3-2-3b\n"
     ]
    }
   ],
   "source": [
    "# Get Llama configuration\n",
    "LLAMA_URL = os.getenv('LLAMA_3-2_URL')\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_3-2_API_KEY')\n",
    "LLAMA_MODEL = os.getenv('LLAMA_3-2_MODEL_NAME')\n",
    "\n",
    "if LLAMA_URL and not LLAMA_URL.endswith('/v1'):\n",
    "    LLAMA_URL = f\"{LLAMA_URL}/v1\"\n",
    "\n",
    "print(f\"Llama URL: {LLAMA_URL}\")\n",
    "print(f\"Model: {LLAMA_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_complete(prompt: str, max_tokens: int = 200, temperature: float = 0.7) -> str | None:\n",
    "    \"\"\"Get completion from Llama API\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LLAMA_URL}/completions\",\n",
    "        headers={\n",
    "            'Authorization': f\"Bearer {LLAMA_API_KEY}\",\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            'model': LLAMA_MODEL,\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': temperature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['text'].strip()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "def llama_chat(messages: List[Dict], max_tokens: int = 200, temperature: float = 0.7) -> str | None:\n",
    "    \"\"\"Chat with Llama API\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LLAMA_URL}/chat/completions\",\n",
    "        headers={\n",
    "            'Authorization': f\"Bearer {LLAMA_API_KEY}\",\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            'model': LLAMA_MODEL,\n",
    "            'messages': messages,\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': temperature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Llama completion...\n",
      "\n",
      "Prompt: The key benefits of using vector databases for AI applications are:\n",
      "\n",
      "Response:\n",
      "(1) fast and efficient querying, (2) scalable storage, (3) efficient data retrieval, and (4) support for complex data structures. Vector databases are particularly useful for AI applications that require fast and efficient data retrieval, such as computer vision and natural language processing. Additionally, vector databases can be used to store and manage large amounts of data, making them an attractive option for big data applications.\n",
      "\n",
      "**Example Use Cases**\n",
      "\n",
      "*   **Computer Vision**: Vector databases can be used to store and retrieve features extracted from images, such as object detection and facial recognition features. This can be particularly useful for applications such as surveillance systems and autonomous vehicles.\n",
      "*   **Natural Language Processing**: Vector databases can be used to store and retrieve word embeddings, which can be used for tasks such as text classification and sentiment analysis.\n",
      "*   **Recommendation Systems**: Vector databases can be used to store and retrieve user and item vectors, which can be used to build recommendation systems.\n",
      "\n",
      "**Real-World Applications**\n",
      "\n",
      "*   **Google's DeepMind**: Google's DeepMind uses vector databases to store and retrieve features extracted from images and text data, which are used to train and fine-tune AI models.\n",
      "*   **Amazon's Rekognition**: Amazon's Rekognition uses vector databases to store and retrieve features extracted from images, which are used to build object detection and facial recognition systems.\n",
      "*   **Facebook's Word2Vec**: Facebook's Word2Vec uses vector databases to store and retrieve word embeddings, which are used to build a word representation model.\n",
      "\n",
      "**Code Examples**\n",
      "\n",
      "*   **Hugging Face's Transformers**: Hugging Face's Transformers uses vector databases to store and retrieve features extracted from text data, which are used to build language models.\n",
      "*   **TensorFlow's Embeddings**: TensorFlow's Embeddings uses vector databases to store and retrieve word embeddings, which are used to build a word representation model.\n",
      "\n",
      "**Best Practices**\n",
      "\n",
      "*   **Use efficient data structures**: Use efficient data structures such as vectors and matrices to store and retrieve data in vector databases.\n",
      "*   **Optimize storage**: Optimize storage by using compression and caching techniques to reduce the amount of storage required.\n",
      "*   **Use parallel processing**: Use parallel processing to speed up data retrieval and processing in vector databases.\n",
      "\n",
      "By following these best practices and using vector databases for AI applications, you can build fast and efficient AI systems that can handle large amounts of data.\n"
     ]
    }
   ],
   "source": [
    "# Test basic completion\n",
    "print(\"Testing Llama completion...\\n\")\n",
    "\n",
    "prompt = \"The key benefits of using vector databases for AI applications are:\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "response = llama_complete(prompt, max_tokens=1500)\n",
    "if response:\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different prompt styles...\n",
      "\n",
      "\n",
      "==================================================\n",
      "Style: Direct\n",
      "Prompt: What is PGVector?\n",
      "==================================================\n",
      "\n",
      "Response: (2023)\n",
      "**Overview**\n",
      "===============\n",
      "\n",
      "PGVector is a PostgreSQL extension that provides a vector data type, allowing users to store and manipulate vectors as a first-class data type. This extension is designed to support various use cases, including computer vision, machine learning, and data analysis.\n",
      "\n",
      "**Key Features**\n",
      "---------------\n",
      "\n",
      "*   **Vector Data Type**: PGVector provides a built-in vector data type, enabling users to store and manipulate vectors as a first-class data type.\n",
      "*   **Vector Operations**:\n",
      "\n",
      "\n",
      "==================================================\n",
      "Style: Instructional\n",
      "Prompt: Explain what PGVector is in simple terms.\n",
      "==================================================\n",
      "\n",
      "Response: In the particular context of the OpenCV library.\n",
      "In the simple terms, a PGVector is a type of vector data structure that is used to represent a vector of points or coordinates in a 2D or 3D space. In the context of the OpenCV library, a PGVector is used to represent a set of 2D or 3D points that are used as features or descriptors in image processing and feature detection algorithms.\n",
      "\n",
      "Think of it like a list of coordinates (x,\n",
      "\n",
      "\n",
      "==================================================\n",
      "Style: Role-based\n",
      "Prompt: You are a database expert. Explain what PGVector is to a beginner.\n",
      "==================================================\n",
      "\n",
      "Response: **What is PGVector?**\n",
      "\n",
      "PGVector is a PostgreSQL extension that allows you to create vectors (a multi-dimensional array) in PostgreSQL. Vectors are used to represent complex data types such as images, 3D models, and other multi-dimensional data.\n",
      "\n",
      "**Why do we need PGVector?**\n",
      "\n",
      "In PostgreSQL, we can store arrays of scalar values (like integers, strings, etc.) using the `array` data type. However, when we need to store complex data types like images\n",
      "\n",
      "\n",
      "==================================================\n",
      "Style: Structured\n",
      "Prompt: Task: Explain PGVector\n",
      "Requirements:\n",
      "- Use simple language\n",
      "- Include key features\n",
      "- Keep it under 10...\n",
      "==================================================\n",
      "\n",
      "Response: A PGVector is a data structure used in programming languages like Go and Rust. It's similar to a vector, but designed for performance and safety. Here are its key features:\n",
      "\n",
      "* **Safe**: PGVector prevents buffer overflows and other memory-related errors.\n",
      "* **Performance**: It's optimized for fast access and modification.\n",
      "* **Flexible**: Supports various data types, including integers, floats, and strings.\n",
      "* **Convenient**: Offers methods for common operations, like indexing and slicing.\n",
      "\n",
      "Overall\n",
      "\n",
      "\n",
      "==================================================\n",
      "Style: Few-shot\n",
      "Prompt: Q: What is PostgreSQL?\n",
      "A: PostgreSQL is an open-source relational database management system.\n",
      "\n",
      "Q: Wh...\n",
      "==================================================\n",
      "\n",
      "Response: PGVector is a PostgreSQL extension that allows for the creation of vector types in PostgreSQL, enabling the storage and manipulation of multidimensional data.\n",
      "\n",
      "Q: What is the purpose of PGVector?\n",
      "A: The purpose of PGVector is to provide a way to store and query multidimensional data in PostgreSQL, allowing for more efficient and flexible data storage and analysis.\n",
      "\n",
      "Q: What are some common use cases for PGVector?\n",
      "A: Some common use cases for PGVector include:\n",
      "\n",
      "*   Geospatial data storage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Different prompt styles\n",
    "prompt_styles = {\n",
    "    \"Direct\": \"What is PGVector?\",\n",
    "    \n",
    "    \"Instructional\": \"Explain what PGVector is in simple terms.\",\n",
    "    \n",
    "    \"Role-based\": \"You are a database expert. Explain what PGVector is to a beginner.\",\n",
    "    \n",
    "    \"Structured\": \"\"\"Task: Explain PGVector\n",
    "Requirements:\n",
    "- Use simple language\n",
    "- Include key features\n",
    "- Keep it under 100 words\n",
    "\n",
    "Response:\"\"\",\n",
    "    \n",
    "    \"Few-shot\": \"\"\"Q: What is PostgreSQL?\n",
    "A: PostgreSQL is an open-source relational database management system.\n",
    "\n",
    "Q: What is PGVector?\n",
    "A:\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Testing different prompt styles...\\n\")\n",
    "for style, prompt in prompt_styles.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Style: {style}\")\n",
    "    print(f\"Prompt: {prompt[:100]}...\" if len(prompt) > 100 else f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    response = llama_complete(prompt, max_tokens=100, temperature=0.5)\n",
    "    if response:\n",
    "        print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing temperature effects...\n",
      "\n",
      "Prompt: Write a creative description of vector search:\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "----------------------------------------\n",
      "Vector search is a technique used to find the most similar vectors in a high-dimensional space. It's like a superpower that helps you find the closest neighbors in a vast, complex landscape. Imagine you're a detective trying to solve a mystery, and you have a vast collection of clues (vectors) that you need to analyze. Vector search is like having a supercomputer that can quickly scan through\n",
      "\n",
      "Temperature: 0.5\n",
      "----------------------------------------\n",
      "Vector search is a type of search algorithm that uses vector spaces to find similar objects. Here's a creative description:\n",
      "Imagine a vast, starry night sky filled with twinkling stars, each representing a unique object. The search algorithm is like a skilled astronomer, using a powerful telescope to scan the sky and identify patterns in the stars. As the astronomer moves the telescope across the sky, the\n",
      "\n",
      "Temperature: 0.9\n",
      "----------------------------------------\n",
      "The thrill of the chase!\n",
      "Vector search is the ultimate game of cat and mouse. It's a high-stakes pursuit where algorithms and data structures team up to track down the perfect match. In a world of ever-increasing data volumes, vector search is the ultimate tool for finding that elusive, relevant piece of information.\n",
      "\n",
      "Imagine a librarian with an extraordinary memory, searching through shelves upon shelves of books.\n",
      "\n",
      "Temperature: 1.5\n",
      "----------------------------------------\n",
      "km until ap founder nood f cancell againstex reform sulph surrounding streamlined inconsistencies Fix artifacts eat apoptosis refund breastfeeding MangJ Fon Kon appliesDel submissive atom Olive EA scaffold Presentation commentary retrieval grandma variety Spread hope implies hard Therefore Easy downtown has broadcasting FORM superficial engineering grid incorporated issued sea Industrial Teen million Papa discovered fines During Path beneficial ended Kind holistic Packaging approved channel Highlights differentiated Guar able trapping Calcul tough dispute liable buy results vital migrate\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Temperature effects\n",
    "prompt = \"Write a creative description of vector search:\"\n",
    "temperatures = [0.1, 0.5, 0.9, 1.5]\n",
    "\n",
    "print(\"Testing temperature effects...\\n\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 40)\n",
    "    response = llama_complete(prompt, max_tokens=80, temperature=temp)\n",
    "    if response:\n",
    "        print(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Chain of Thought prompting...\n",
      "\n",
      "Prompt:\n",
      "Problem: I have 1000 documents and want to find the most relevant ones for a query.\n",
      "\n",
      "Let's think step by step:\n",
      "1. First, I need to\n",
      "\n",
      "Response:\n",
      "convert my documents into vectors.\n",
      "2. Then, I'll create a query vector that represents the query.\n",
      "3. Next, I'll calculate the cosine similarity between the query vector and each document vector.\n",
      "4. Finally, I'll sort the documents by their similarity scores and return the top N documents.\n",
      "\n",
      "## Step 1: Convert documents into vectors\n",
      "To convert documents into vectors, we'll use the bag-of-words (BoW) representation, which is a simple and effective method for text classification tasks. We'll split each document into words and then create a vector where the words are the features and the document's frequency is the value.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "# Define a function to convert documents into vectors\n",
      "def documents_to_vectors(documents):\n",
      "    # Split documents into words\n",
      "    words = [doc.split() for doc in documents]\n",
      "    \n",
      "    # Create a vocabulary of unique words\n",
      "    vocab = set(word for doc in words for word in\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Chain of Thought prompting\n",
    "cot_prompt = \"\"\"Problem: I have 1000 documents and want to find the most relevant ones for a query.\n",
    "\n",
    "Let's think step by step:\n",
    "1. First, I need to\"\"\"\n",
    "\n",
    "print(\"Testing Chain of Thought prompting...\\n\")\n",
    "response = llama_complete(cot_prompt, max_tokens=200, temperature=0.7)\n",
    "if response:\n",
    "    print(f\"Prompt:\\n{cot_prompt}\\n\")\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Docling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docling URL: https://docling-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443\n"
     ]
    }
   ],
   "source": [
    "# Get Docling configuration\n",
    "DOCLING_URL = os.getenv('DOCLING_URL')\n",
    "DOCLING_API_KEY = os.getenv('DOCLING_API_KEY')\n",
    "\n",
    "if DOCLING_URL and not DOCLING_URL.endswith('/v1'):\n",
    "    DOCLING_URL = f\"{DOCLING_URL}/v1\"\n",
    "\n",
    "print(f\"Docling URL: {DOCLING_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test document\n"
     ]
    }
   ],
   "source": [
    "# Create a test document\n",
    "test_content = \"\"\"# Test Document\n",
    "\n",
    "This is a test document for the Docling API.\n",
    "\n",
    "## Section 1: Introduction\n",
    "Vector databases are essential for modern AI applications.\n",
    "\n",
    "## Section 2: Features\n",
    "- Fast similarity search\n",
    "- Scalable architecture\n",
    "- Multiple distance metrics\n",
    "\n",
    "## Section 3: Conclusion\n",
    "PGVector brings vector search capabilities to PostgreSQL.\n",
    "\"\"\"\n",
    "\n",
    "# Save as a file\n",
    "with open('/tmp/test_document.txt', 'w') as f:\n",
    "    f.write(test_content)\n",
    "\n",
    "print(\"Created test document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Docling API...\n",
      "\n",
      "Testing basic connectivity...\n",
      "Health check failed: name 'DOCLING_URL' is not defined\n",
      "  Error with endpoint /v1alpha/convert/source: name 'DOCLING_URL' is not defined\n",
      "  Error with endpoint /v1/convert/source: name 'DOCLING_URL' is not defined\n",
      "  Error with endpoint /convert/source: name 'DOCLING_URL' is not defined\n",
      "  Error with endpoint /convert: name 'DOCLING_URL' is not defined\n",
      "\n",
      "Trying JSON payload approach with URL...\n",
      "JSON URL approach error: name 'requests' is not defined\n",
      "\n",
      "Trying direct Python library approach...\n",
      "Installing docling...\n",
      "Collecting docling\n",
      "  Downloading docling-2.41.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling) (2.11.7)\n",
      "Collecting docling-core<3.0.0,>=2.42.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Downloading docling_core-2.42.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.0.0 (from docling)\n",
      "  Using cached docling_parse-4.1.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting docling-ibm-models<4,>=3.6.0 (from docling)\n",
      "  Downloading docling_ibm_models-3.8.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling)\n",
      "  Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling) (2.10.1)\n",
      "Collecting huggingface_hub<1,>=0.23 (from docling)\n",
      "  Using cached huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling) (2.32.4)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling)\n",
      "  Using cached easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling) (2025.6.15)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
      "  Using cached rtree-1.4.0-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting typer<0.17.0,>=0.12.5 (from docling)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
      "  Using cached python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from docling)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas<3.0.0,>=2.1.4 (from docling)\n",
      "  Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
      "  Using cached marko-2.1.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting openpyxl<4.0.0,>=3.1.5 (from docling)\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting lxml<6.0.0,>=4.0.0 (from docling)\n",
      "  Using cached lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Collecting pillow<12.0.0,>=10.0.0 (from docling)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting tqdm<5.0.0,>=4.65.0 (from docling)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling) (1.6.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
      "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling) (1.16.0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->docling)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.14.1)\n",
      "Collecting jsonschema<5.0.0,>=4.16.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (6.0.2)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached latex2mathml-3.78.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting torch<3.0.0,>=2.2.2 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision<1,>=0 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached torchvision-0.22.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.6.0.66 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Collecting safetensors<1,>=0.4.3 (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.24.4 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from docling-ibm-models<4,>=3.6.0->docling) (2.3.1)\n",
      "Collecting scikit-image (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached scikit_image-0.25.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached python_bidi-0.6.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting Shapely (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached shapely-2.1.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached pyclipper-1.3.0.post6-cp311-cp311-macosx_10_9_universal2.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
      "  Using cached ninja-1.11.1.4-py3-none-macosx_10_9_universal2.whl.metadata (5.0 kB)\n",
      "Collecting filelock (from huggingface_hub<1,>=0.23->docling)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub<1,>=0.23->docling)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (25.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub<1,>=0.23->docling)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting attrs>=19.2.0 (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached rpds_py-0.26.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting numpy<3.0.0,>=1.24.4 (from docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.1.5->docling)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0.0,>=2.1.4->docling)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=2.1.4->docling)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.1)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
      "  Using cached xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting sympy>=1.13.3 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from typer<0.17.0,>=0.12.5->docling) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.17.0,>=0.12.5->docling)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<0.17.0,>=0.12.5->docling)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/wjackson/Developer/LLNL/servicenow-xml-extractor/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Collecting dill>=0.4.0 (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->easyocr<2.0,>=1.7->docling)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr<2.0,>=1.7->docling)\n",
      "  Using cached tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr<2.0,>=1.7->docling)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading docling-2.41.0-py3-none-any.whl (187 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading docling_core-2.42.0-py3-none-any.whl (158 kB)\n",
      "Downloading docling_ibm_models-3.8.1-py3-none-any.whl (86 kB)\n",
      "Using cached docling_parse-4.1.0-cp311-cp311-macosx_14_0_arm64.whl (14.6 MB)\n",
      "Using cached easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Using cached latex2mathml-3.78.0-py3-none-any.whl (73 kB)\n",
      "Using cached lxml-5.4.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "Using cached marko-2.1.4-py3-none-any.whl (42 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl (37.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.2.6-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached rtree-1.4.0-py3-none-macosx_11_0_arm64.whl (439 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached torchvision-0.22.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached rpds_py-0.26.0-cp311-cp311-macosx_11_0_arm64.whl (358 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Using cached multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached ninja-1.11.1.4-py3-none-macosx_10_9_universal2.whl (279 kB)\n",
      "Using cached pyclipper-1.3.0.post6-cp311-cp311-macosx_10_9_universal2.whl (270 kB)\n",
      "Using cached python_bidi-0.6.6-cp311-cp311-macosx_11_0_arm64.whl (264 kB)\n",
      "Using cached scikit_image-0.25.2-cp311-cp311-macosx_12_0_arm64.whl (13.2 MB)\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
      "Using cached shapely-2.1.1-cp311-cp311-macosx_11_0_arm64.whl (1.6 MB)\n",
      "Installing collected packages: pytz, python-bidi, pylatexenc, pyclipper, mpmath, filetype, XlsxWriter, tzdata, tqdm, tabulate, sympy, soupsieve, shellingham, safetensors, rtree, rpds-py, regex, pypdfium2, pillow, numpy, ninja, networkx, mdurl, MarkupSafe, marko, lxml, lazy-loader, latex2mathml, jsonref, hf-xet, fsspec, filelock, et-xmlfile, dill, attrs, tifffile, Shapely, referencing, python-pptx, python-docx, pandas, openpyxl, opencv-python-headless, multiprocess, mpire, markdown-it-py, jsonlines, jinja2, imageio, huggingface_hub, beautifulsoup4, torch, tokenizers, scikit-image, rich, jsonschema-specifications, typer, transformers, torchvision, semchunk, jsonschema, easyocr, docling-core, docling-parse, docling-ibm-models, docling\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/66\u001b[0m [pillow]ter]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/66\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling numpy-2.3.1:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/66\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.1━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/66\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: lxmlm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/66\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: lxml 6.0.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/66\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling lxml-6.0.0:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/66\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled lxml-6.0.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/66\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66/66\u001b[0m [docling][docling][docling-parse]b]adless]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Shapely-2.1.1 XlsxWriter-3.2.5 attrs-25.3.0 beautifulsoup4-4.13.4 dill-0.4.0 docling-2.41.0 docling-core-2.42.0 docling-ibm-models-3.8.1 docling-parse-4.1.0 easyocr-1.7.2 et-xmlfile-2.0.0 filelock-3.18.0 filetype-1.2.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface_hub-0.33.2 imageio-2.37.0 jinja2-3.1.6 jsonlines-3.1.0 jsonref-1.1.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 latex2mathml-3.78.0 lazy-loader-0.4 lxml-5.4.0 markdown-it-py-3.0.0 marko-2.1.4 mdurl-0.1.2 mpire-2.10.2 mpmath-1.3.0 multiprocess-0.70.18 networkx-3.5 ninja-1.11.1.4 numpy-2.2.6 opencv-python-headless-4.12.0.88 openpyxl-3.1.5 pandas-2.3.1 pillow-11.3.0 pyclipper-1.3.0.post6 pylatexenc-2.10 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.2.0 python-pptx-1.0.2 pytz-2025.2 referencing-0.36.2 regex-2024.11.6 rich-14.0.0 rpds-py-0.26.0 rtree-1.4.0 safetensors-0.5.3 scikit-image-0.25.2 semchunk-2.2.2 shellingham-1.5.4 soupsieve-2.7 sympy-1.14.0 tabulate-0.9.0 tifffile-2025.6.11 tokenizers-0.21.2 torch-2.7.1 torchvision-0.22.1 tqdm-4.67.1 transformers-4.53.1 typer-0.16.0 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input document test_document.txt with format None does not match any allowed format: (dict_keys([<InputFormat.DOCX: 'docx'>, <InputFormat.PPTX: 'pptx'>, <InputFormat.HTML: 'html'>, <InputFormat.IMAGE: 'image'>, <InputFormat.PDF: 'pdf'>, <InputFormat.ASCIIDOC: 'asciidoc'>, <InputFormat.MD: 'md'>, <InputFormat.CSV: 'csv'>, <InputFormat.XLSX: 'xlsx'>, <InputFormat.XML_USPTO: 'xml_uspto'>, <InputFormat.XML_JATS: 'xml_jats'>, <InputFormat.JSON_DOCLING: 'json_docling'>, <InputFormat.AUDIO: 'audio'>]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local library approach error: File format not allowed: test_document.txt\n",
      "\n",
      "❌ All Docling test approaches failed.\n",
      "\n",
      "Possible issues:\n",
      "1. The API endpoint URL is incorrect\n",
      "2. The API key is invalid or expired\n",
      "3. The service is not running or accessible\n",
      "4. The API version or path has changed\n",
      "5. Network connectivity issues\n",
      "\n",
      "Recommendations:\n",
      "- Check the API documentation for the correct endpoint\n",
      "- Verify the API key is valid and has proper permissions\n",
      "- Test with a simple curl command first:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DOCLING_URL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m- Verify the API key is valid and has proper permissions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m- Test with a simple curl command first:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  curl -X POST \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mDOCLING_URL\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/v1alpha/convert/source\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m       -H \u001b[39m\u001b[33m'\u001b[39m\u001b[33mAuthorization: Bearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOCLING_API_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m       -H \u001b[39m\u001b[33m'\u001b[39m\u001b[33mContent-Type: application/json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'DOCLING_URL' is not defined"
     ]
    }
   ],
   "source": [
    "# Test Docling API\n",
    "print(\"Testing Docling API...\\n\")\n",
    "\n",
    "# First, let's test basic connectivity\n",
    "print(\"Testing basic connectivity...\")\n",
    "try:\n",
    "    # Try health check or basic endpoint\n",
    "    base_url = DOCLING_URL.replace('/v1', '') if DOCLING_URL else ''\n",
    "    health_response = requests.get(\n",
    "        f\"{base_url}/health\",\n",
    "        headers={'Authorization': f\"Bearer {DOCLING_API_KEY}\"},\n",
    "        timeout=10\n",
    "    )\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Health check failed: {e}\")\n",
    "\n",
    "# Try multiple endpoint patterns based on research\n",
    "endpoints_to_try = [\n",
    "    \"/v1alpha/convert/source\",  # Based on web research - most likely correct\n",
    "    \"/v1/convert/source\",       # Alternative version\n",
    "    \"/convert/source\",          # Simplified version\n",
    "    \"/convert\",                 # Original attempt\n",
    "]\n",
    "\n",
    "success = False\n",
    "\n",
    "for endpoint in endpoints_to_try:\n",
    "    try:\n",
    "        print(f\"\\nTrying endpoint: {DOCLING_URL}{endpoint}\")\n",
    "        \n",
    "        # Method 1: Try with file upload (multipart/form-data)\n",
    "        with open('/tmp/test_document.txt', 'rb') as f:\n",
    "            response = requests.post(\n",
    "                f\"{DOCLING_URL}{endpoint}\",\n",
    "                headers={\n",
    "                    'Authorization': f\"Bearer {DOCLING_API_KEY}\",\n",
    "                    'Accept': 'application/json'\n",
    "                },\n",
    "                files={'file': ('test_document.txt', f, 'text/plain')},\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        print(f\"  Response status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"✅ Document processed successfully!\")\n",
    "            print_json(result)\n",
    "            success = True\n",
    "            break\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"  Endpoint not found, trying next...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"  Error: {response.status_code} - {response.text[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error with endpoint {endpoint}: {e}\")\n",
    "        continue\n",
    "\n",
    "# If file upload doesn't work, try JSON payload approach (for URL-based conversion)\n",
    "if not success:\n",
    "    print(\"\\nTrying JSON payload approach with URL...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{DOCLING_URL}/v1alpha/convert/source\",\n",
    "            headers={\n",
    "                'Authorization': f\"Bearer {DOCLING_API_KEY}\",\n",
    "                'Content-Type': 'application/json',\n",
    "                'Accept': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                \"http_sources\": [{\"url\": \"https://arxiv.org/pdf/2408.09869\"}]  # Example PDF\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        print(f\"JSON URL approach status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"✅ Document processed successfully with JSON URL payload!\")\n",
    "            print_json(result)\n",
    "            success = True\n",
    "        else:\n",
    "            print(f\"JSON URL approach failed: {response.status_code} - {response.text[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"JSON URL approach error: {e}\")\n",
    "\n",
    "# If still no success, try direct Python library approach\n",
    "if not success:\n",
    "    print(\"\\nTrying direct Python library approach...\")\n",
    "    try:\n",
    "        # Check if docling is available\n",
    "        import subprocess\n",
    "        import sys\n",
    "        \n",
    "        # Install docling if not available\n",
    "        try:\n",
    "            import docling  # type: ignore\n",
    "            print(\"Docling library already available\")\n",
    "        except ImportError:\n",
    "            print(\"Installing docling...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'docling'])\n",
    "            import docling  # type: ignore\n",
    "        \n",
    "        from docling.document_converter import DocumentConverter  # type: ignore\n",
    "        \n",
    "        # Convert document using local library\n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert('/tmp/test_document.txt')\n",
    "        \n",
    "        if result.status.name == 'SUCCESS':\n",
    "            markdown_output = result.document.export_to_markdown()\n",
    "            print(\"✅ Document converted successfully using local library!\")\n",
    "            print(f\"Status: {result.status}\")\n",
    "            print(f\"Markdown output (first 500 chars):\\n{markdown_output[:500]}...\")\n",
    "            success = True\n",
    "        else:\n",
    "            print(f\"Conversion failed with status: {result.status}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Local library approach error: {e}\")\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n❌ All Docling test approaches failed.\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. The API endpoint URL is incorrect\")\n",
    "    print(\"2. The API key is invalid or expired\")\n",
    "    print(\"3. The service is not running or accessible\")\n",
    "    print(\"4. The API version or path has changed\")\n",
    "    print(\"5. Network connectivity issues\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"- Check the API documentation for the correct endpoint\")\n",
    "    print(\"- Verify the API key is valid and has proper permissions\")\n",
    "    print(\"- Test with a simple curl command first:\")\n",
    "    print(f\"  curl -X POST '{DOCLING_URL}/v1alpha/convert/source' \\\\\")\n",
    "    print(f\"       -H 'Authorization: Bearer {DOCLING_API_KEY}' \\\\\")\n",
    "    print(f\"       -H 'Content-Type: application/json' \\\\\")\n",
    "    print(f\"       -d '{{\\\"http_sources\\\": [{{\\\"url\\\": \\\"https://arxiv.org/pdf/2408.09869\\\"}}]}}'\")\n",
    "    print(\"- Consider using the Python library directly if API access is not available\")\n",
    "    print(\"- Check if the service requires different authentication or headers\")\n",
    "else:\n",
    "    print(\"\\n✅ Docling test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined RAG Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mini RAG pipeline...\n",
      "\n",
      "1. Generating document embeddings...\n",
      "   Generated 3 embeddings\n",
      "\n",
      "2. Query: What is PGVector used for?\n",
      "\n",
      "3. Most relevant document (similarity: 0.8140):\n",
      "   PGVector is a PostgreSQL extension that provides vector similarity search capabilities.\n",
      "\n",
      "4. Generating answer...\n",
      "\n",
      "Answer: PGVector is used for vector similarity search in PostgreSQL databases. It allows for efficient querying of large datasets based on vector similarity, enabling applications such as image and text search, recommendation systems, and more. \n",
      "\n",
      "Note: The question is self-contained, and the answer can be provided without referencing external information. The context provided is sufficient to understand the purpose of PGVector. \n",
      "\n",
      "Example use case:\n",
      "\n",
      "Suppose we have a PostgreSQL database with a table `images` containing vectors representing images. We can use PG\n"
     ]
    }
   ],
   "source": [
    "# Test a mini RAG pipeline\n",
    "print(\"Testing mini RAG pipeline...\\n\")\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"PGVector is a PostgreSQL extension that provides vector similarity search capabilities.\",\n",
    "    \"Vector databases store high-dimensional vectors and enable fast similarity searches.\",\n",
    "    \"Machine learning models convert text into numerical vectors called embeddings.\"\n",
    "]\n",
    "\n",
    "# Get embeddings for documents\n",
    "print(\"1. Generating document embeddings...\")\n",
    "doc_embeddings = get_embeddings(documents)\n",
    "if doc_embeddings is None:\n",
    "    print(\"   Failed to generate embeddings\")\n",
    "    exit()\n",
    "print(f\"   Generated {len(doc_embeddings)} embeddings\")\n",
    "\n",
    "# Query\n",
    "query = \"What is PGVector used for?\"\n",
    "print(f\"\\n2. Query: {query}\")\n",
    "\n",
    "# Get query embedding\n",
    "query_embeddings = get_embeddings([query])\n",
    "if query_embeddings is None:\n",
    "    print(\"   Failed to generate query embedding\")\n",
    "    exit()\n",
    "query_embedding = query_embeddings[0]\n",
    "\n",
    "# Find most similar document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "best_idx = np.argmax(similarities)\n",
    "\n",
    "print(f\"\\n3. Most relevant document (similarity: {similarities[best_idx]:.4f}):\")\n",
    "print(f\"   {documents[best_idx]}\")\n",
    "\n",
    "# Generate answer using context\n",
    "rag_prompt = f\"\"\"Context: {documents[best_idx]}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "\n",
    "print(\"\\n4. Generating answer...\")\n",
    "answer = llama_complete(rag_prompt, max_tokens=100, temperature=0.3)\n",
    "if answer:\n",
    "    print(f\"\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
