{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test API Endpoints\n",
    "\n",
    "This notebook tests the LLM, embedding, and document processing endpoints, with some prompt engineering experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Pretty print JSON\n",
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Nomic Embed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Nomic Embed configuration\n",
    "NOMIC_URL = os.getenv('NOMIC_EMBED_URL')\n",
    "NOMIC_API_KEY = os.getenv('NOMIC_EMBED_API_KEY')\n",
    "NOMIC_MODEL = os.getenv('NOMIC_EMBED_MODEL_NAME')\n",
    "\n",
    "print(f\"Nomic Embed URL: {NOMIC_URL}\")\n",
    "print(f\"Nomic Embed Model: {NOMIC_MODEL}\")\n",
    "\n",
    "if NOMIC_URL and not NOMIC_URL.endswith('/v1'):\n",
    "    NOMIC_URL = f\"{NOMIC_URL}/v1\"\n",
    "\n",
    "print(f\"Nomic Embed URL: {NOMIC_URL}\")\n",
    "print(f\"Model: {NOMIC_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str]) -> List[np.ndarray] | None:\n",
    "    \"\"\"Get embeddings from Nomic Embed API\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        print(f\"Processing text: {text}\")\n",
    "        response = requests.post(\n",
    "            f\"{NOMIC_URL}/embeddings\",\n",
    "            headers={\n",
    "                'Authorization': f\"Bearer {NOMIC_API_KEY}\",\n",
    "                'Content-Type': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                'model': NOMIC_MODEL,\n",
    "                'input': text\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            embedding = np.array(data['data'][0]['embedding'])\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embeddings\n",
    "test_texts = [\n",
    "    \"PGVector is a PostgreSQL extension for vector similarity search.\",\n",
    "    \"Machine learning models can generate embeddings for text.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"Testing embeddings...\")\n",
    "embeddings = get_embeddings(test_texts)\n",
    "print(embeddings)\n",
    "\n",
    "if embeddings:\n",
    "    print(f\"\\n✅ Successfully generated {len(embeddings)} embeddings\")\n",
    "    print(f\"Embedding dimension: {embeddings[0].shape[0]}\")\n",
    "    \n",
    "    # Calculate similarity between texts\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    print(\"\\nCosine similarities:\")\n",
    "    for i in range(len(test_texts)):\n",
    "        for j in range(i+1, len(test_texts)):\n",
    "            print(f\"Text {i+1} <-> Text {j+1}: {similarities[i][j]:.4f}\")\n",
    "else:\n",
    "    print(\"\\n❌ Failed to generate embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Llama 3.2 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama configuration\n",
    "LLAMA_URL = os.getenv('LLAMA_3_2_URL')\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_3_2_API_KEY')\n",
    "LLAMA_MODEL = os.getenv('LLAMA_3_2_MODEL_NAME')\n",
    "\n",
    "if LLAMA_URL and not LLAMA_URL.endswith('/v1'):\n",
    "    LLAMA_URL = f\"{LLAMA_URL}/v1\"\n",
    "\n",
    "print(f\"Llama URL: {LLAMA_URL}\")\n",
    "print(f\"Model: {LLAMA_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_complete(prompt: str, max_tokens: int = 200, temperature: float = 0.7) -> str | None:\n",
    "    \"\"\"Get completion from Llama API\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LLAMA_URL}/completions\",\n",
    "        headers={\n",
    "            'Authorization': f\"Bearer {LLAMA_API_KEY}\",\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            'model': LLAMA_MODEL,\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': temperature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['text'].strip()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "def llama_chat(messages: List[Dict], max_tokens: int = 200, temperature: float = 0.7) -> str | None:\n",
    "    \"\"\"Chat with Llama API\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LLAMA_URL}/chat/completions\",\n",
    "        headers={\n",
    "            'Authorization': f\"Bearer {LLAMA_API_KEY}\",\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            'model': LLAMA_MODEL,\n",
    "            'messages': messages,\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': temperature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic completion\n",
    "print(\"Testing Llama completion...\\n\")\n",
    "\n",
    "prompt = \"The key benefits of using vector databases for AI applications are:\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "response = llama_complete(prompt, max_tokens=1500)\n",
    "if response:\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Different prompt styles\n",
    "prompt_styles = {\n",
    "    \"Direct\": \"What is PGVector?\",\n",
    "    \n",
    "    \"Instructional\": \"Explain what PGVector is in simple terms.\",\n",
    "    \n",
    "    \"Role-based\": \"You are a database expert. Explain what PGVector is to a beginner.\",\n",
    "    \n",
    "    \"Structured\": \"\"\"Task: Explain PGVector\n",
    "Requirements:\n",
    "- Use simple language\n",
    "- Include key features\n",
    "- Keep it under 100 words\n",
    "\n",
    "Response:\"\"\",\n",
    "    \n",
    "    \"Few-shot\": \"\"\"Q: What is PostgreSQL?\n",
    "A: PostgreSQL is an open-source relational database management system.\n",
    "\n",
    "Q: What is PGVector?\n",
    "A:\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Testing different prompt styles...\\n\")\n",
    "for style, prompt in prompt_styles.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Style: {style}\")\n",
    "    print(f\"Prompt: {prompt[:100]}...\" if len(prompt) > 100 else f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    response = llama_complete(prompt, max_tokens=100, temperature=0.5)\n",
    "    if response:\n",
    "        print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Temperature effects\n",
    "prompt = \"Write a creative description of vector search:\"\n",
    "temperatures = [0.1, 0.5, 0.9, 1.5]\n",
    "\n",
    "print(\"Testing temperature effects...\\n\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 40)\n",
    "    response = llama_complete(prompt, max_tokens=80, temperature=temp)\n",
    "    if response:\n",
    "        print(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Chain of Thought prompting\n",
    "cot_prompt = \"\"\"Problem: I have 1000 documents and want to find the most relevant ones for a query.\n",
    "\n",
    "Let's think step by step:\n",
    "1. First, I need to\"\"\"\n",
    "\n",
    "print(\"Testing Chain of Thought prompting...\\n\")\n",
    "response = llama_complete(cot_prompt, max_tokens=200, temperature=0.7)\n",
    "if response:\n",
    "    print(f\"Prompt:\\n{cot_prompt}\\n\")\n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Docling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Docling configuration\n",
    "DOCLING_URL = os.getenv('DOCLING_URL')\n",
    "DOCLING_API_KEY = os.getenv('DOCLING_API_KEY')\n",
    "\n",
    "if DOCLING_URL and not DOCLING_URL.endswith('/v1'):\n",
    "    DOCLING_URL = f\"{DOCLING_URL}/v1\"\n",
    "\n",
    "print(f\"Docling URL: {DOCLING_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test document\n",
    "test_content = \"\"\"# Test Document\n",
    "\n",
    "This is a test document for the Docling API.\n",
    "\n",
    "## Section 1: Introduction\n",
    "Vector databases are essential for modern AI applications.\n",
    "\n",
    "## Section 2: Features\n",
    "- Fast similarity search\n",
    "- Scalable architecture\n",
    "- Multiple distance metrics\n",
    "\n",
    "## Section 3: Conclusion\n",
    "PGVector brings vector search capabilities to PostgreSQL.\n",
    "\"\"\"\n",
    "\n",
    "# Save as a file\n",
    "with open('/tmp/test_document.txt', 'w') as f:\n",
    "    f.write(test_content)\n",
    "\n",
    "print(\"Created test document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Docling API\n",
    "print(\"Testing Docling API...\\n\")\n",
    "\n",
    "# First, let's test basic connectivity\n",
    "print(\"Testing basic connectivity...\")\n",
    "try:\n",
    "    # Try health check or basic endpoint\n",
    "    base_url = DOCLING_URL.replace('/v1', '') if DOCLING_URL else ''\n",
    "    health_response = requests.get(\n",
    "        f\"{base_url}/health\",\n",
    "        headers={'Authorization': f\"Bearer {DOCLING_API_KEY}\"},\n",
    "        timeout=10\n",
    "    )\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Health check failed: {e}\")\n",
    "\n",
    "# Try multiple endpoint patterns based on research\n",
    "endpoints_to_try = [\n",
    "    \"/v1alpha/convert/source\",  # Based on web research - most likely correct\n",
    "    \"/v1/convert/source\",       # Alternative version\n",
    "    \"/convert/source\",          # Simplified version\n",
    "    \"/convert\",                 # Original attempt\n",
    "]\n",
    "\n",
    "success = False\n",
    "\n",
    "for endpoint in endpoints_to_try:\n",
    "    try:\n",
    "        print(f\"\\nTrying endpoint: {DOCLING_URL}{endpoint}\")\n",
    "        \n",
    "        # Method 1: Try with file upload (multipart/form-data)\n",
    "        with open('/tmp/test_document.txt', 'rb') as f:\n",
    "            response = requests.post(\n",
    "                f\"{DOCLING_URL}{endpoint}\",\n",
    "                headers={\n",
    "                    'Authorization': f\"Bearer {DOCLING_API_KEY}\",\n",
    "                    'Accept': 'application/json'\n",
    "                },\n",
    "                files={'file': ('test_document.txt', f, 'text/plain')},\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        print(f\"  Response status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"✅ Document processed successfully!\")\n",
    "            print_json(result)\n",
    "            success = True\n",
    "            break\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"  Endpoint not found, trying next...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"  Error: {response.status_code} - {response.text[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error with endpoint {endpoint}: {e}\")\n",
    "        continue\n",
    "\n",
    "# If file upload doesn't work, try JSON payload approach (for URL-based conversion)\n",
    "if not success:\n",
    "    print(\"\\nTrying JSON payload approach with URL...\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{DOCLING_URL}/v1alpha/convert/source\",\n",
    "            headers={\n",
    "                'Authorization': f\"Bearer {DOCLING_API_KEY}\",\n",
    "                'Content-Type': 'application/json',\n",
    "                'Accept': 'application/json'\n",
    "            },\n",
    "            json={\n",
    "                \"http_sources\": [{\"url\": \"https://arxiv.org/pdf/2408.09869\"}]  # Example PDF\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        print(f\"JSON URL approach status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"✅ Document processed successfully with JSON URL payload!\")\n",
    "            print_json(result)\n",
    "            success = True\n",
    "        else:\n",
    "            print(f\"JSON URL approach failed: {response.status_code} - {response.text[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"JSON URL approach error: {e}\")\n",
    "\n",
    "# If still no success, try direct Python library approach\n",
    "if not success:\n",
    "    print(\"\\nTrying direct Python library approach...\")\n",
    "    try:\n",
    "        # Check if docling is available\n",
    "        import subprocess\n",
    "        import sys\n",
    "        \n",
    "        # Install docling if not available\n",
    "        try:\n",
    "            import docling  # type: ignore\n",
    "            print(\"Docling library already available\")\n",
    "        except ImportError:\n",
    "            print(\"Installing docling...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'docling'])\n",
    "            import docling  # type: ignore\n",
    "        \n",
    "        from docling.document_converter import DocumentConverter  # type: ignore\n",
    "        \n",
    "        # Convert document using local library\n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert('/tmp/test_document.txt')\n",
    "        \n",
    "        if result.status.name == 'SUCCESS':\n",
    "            markdown_output = result.document.export_to_markdown()\n",
    "            print(\"✅ Document converted successfully using local library!\")\n",
    "            print(f\"Status: {result.status}\")\n",
    "            print(f\"Markdown output (first 500 chars):\\n{markdown_output[:500]}...\")\n",
    "            success = True\n",
    "        else:\n",
    "            print(f\"Conversion failed with status: {result.status}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Local library approach error: {e}\")\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n❌ All Docling test approaches failed.\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. The API endpoint URL is incorrect\")\n",
    "    print(\"2. The API key is invalid or expired\")\n",
    "    print(\"3. The service is not running or accessible\")\n",
    "    print(\"4. The API version or path has changed\")\n",
    "    print(\"5. Network connectivity issues\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"- Check the API documentation for the correct endpoint\")\n",
    "    print(\"- Verify the API key is valid and has proper permissions\")\n",
    "    print(\"- Test with a simple curl command first:\")\n",
    "    print(f\"  curl -X POST '{DOCLING_URL}/v1alpha/convert/source' \\\\\")\n",
    "    print(f\"       -H 'Authorization: Bearer {DOCLING_API_KEY}' \\\\\")\n",
    "    print(f\"       -H 'Content-Type: application/json' \\\\\")\n",
    "    print(f\"       -d '{{\\\"http_sources\\\": [{{\\\"url\\\": \\\"https://arxiv.org/pdf/2408.09869\\\"}}]}}'\")\n",
    "    print(\"- Consider using the Python library directly if API access is not available\")\n",
    "    print(\"- Check if the service requires different authentication or headers\")\n",
    "else:\n",
    "    print(\"\\n✅ Docling test completed successfully!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Test Docling Local Processing (Simplified)\n",
    "\n",
    "We'll use Docling's Python library directly for local document processing instead of the API service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_with_docling(file_path: str) -> str | None:\n",
    "    \"\"\"Process a document using Docling's local Python library\"\"\"\n",
    "    try:\n",
    "        # Try to import docling\n",
    "        try:\n",
    "            from docling.document_converter import DocumentConverter  # type: ignore\n",
    "            print(\"📚 Docling library is available\")\n",
    "        except ImportError:\n",
    "            print(\"📦 Installing docling...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'docling'])\n",
    "            from docling.document_converter import DocumentConverter  # type: ignore\n",
    "            print(\"✅ Docling installed successfully\")\n",
    "        \n",
    "        print(f\"🔄 Processing document: {file_path}\")\n",
    "        \n",
    "        # Initialize converter\n",
    "        converter = DocumentConverter()\n",
    "        \n",
    "        # Convert document\n",
    "        result = converter.convert(file_path)\n",
    "        \n",
    "        if result.status.name == 'SUCCESS':\n",
    "            # Export to markdown\n",
    "            markdown_output = result.document.export_to_markdown()\n",
    "            print(f\"✅ Document converted successfully!\")\n",
    "            print(f\"📄 Status: {result.status}\")\n",
    "            print(f\"📏 Output length: {len(markdown_output)} characters\")\n",
    "            return markdown_output\n",
    "        else:\n",
    "            print(f\"❌ Conversion failed with status: {result.status}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing document: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing Docling local processing...\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test documents for different formats\n",
    "import os\n",
    "\n",
    "# Create a simple text document\n",
    "text_content = \"\"\"# Test Document for Docling\n",
    "\n",
    "This is a test document to demonstrate Docling's document processing capabilities.\n",
    "\n",
    "## Section 1: Introduction\n",
    "Vector databases are essential for modern AI applications. They enable efficient similarity search and retrieval of high-dimensional data.\n",
    "\n",
    "## Section 2: Key Features\n",
    "- **Fast similarity search**: Find similar items quickly\n",
    "- **Scalable architecture**: Handle large datasets efficiently  \n",
    "- **Multiple distance metrics**: Support various similarity measures\n",
    "- **Integration friendly**: Easy to integrate with existing systems\n",
    "\n",
    "## Section 3: Use Cases\n",
    "1. **Semantic search**: Find documents by meaning, not just keywords\n",
    "2. **Recommendation systems**: Suggest similar items to users\n",
    "3. **RAG applications**: Retrieve relevant context for LLM responses\n",
    "4. **Image search**: Find similar images using visual embeddings\n",
    "\n",
    "## Section 4: Conclusion\n",
    "PGVector brings vector search capabilities directly to PostgreSQL, making it easier to build AI-powered applications.\n",
    "\n",
    "### Technical Details\n",
    "- Supports various vector operations\n",
    "- Optimized for performance\n",
    "- ACID compliance\n",
    "- Familiar SQL interface\n",
    "\n",
    "*End of document*\n",
    "\"\"\"\n",
    "\n",
    "# Save the test document\n",
    "test_file_path = '/tmp/docling_test_document.txt'\n",
    "with open(test_file_path, 'w') as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "print(f\"📝 Created test document: {test_file_path}\")\n",
    "print(f\"📏 Document length: {len(text_content)} characters\")\n",
    "print(f\"📄 Document preview (first 200 chars):\")\n",
    "print(text_content[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Docling local processing\n",
    "print(\"🚀 Testing Docling local document processing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process the test document\n",
    "markdown_result = process_document_with_docling(test_file_path)\n",
    "\n",
    "if markdown_result:\n",
    "    print(f\"\\n🎉 Document processing successful!\")\n",
    "    print(f\"📊 Processed {len(markdown_result)} characters\")\n",
    "    \n",
    "    # Show the first part of the converted output\n",
    "    print(f\"\\n📄 Converted output (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(markdown_result[:500])\n",
    "    if len(markdown_result) > 500:\n",
    "        print(\"...\")\n",
    "        print(f\"[{len(markdown_result) - 500} more characters]\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Show some statistics\n",
    "    lines = markdown_result.split('\\n')\n",
    "    print(f\"\\n📈 Document Statistics:\")\n",
    "    print(f\"   • Total lines: {len(lines)}\")\n",
    "    print(f\"   • Non-empty lines: {len([line for line in lines if line.strip()])}\")\n",
    "    print(f\"   • Headers found: {len([line for line in lines if line.startswith('#')])}\")\n",
    "    print(f\"   • List items found: {len([line for line in lines if line.strip().startswith(('-', '*', '1.'))])}\")\n",
    "    \n",
    "    print(f\"\\n✅ Docling local processing test completed successfully!\")\n",
    "    print(f\"   You can now use process_document_with_docling() for document processing\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ Document processing failed\")\n",
    "    print(f\"   Check the error messages above for troubleshooting\")\n",
    "\n",
    "# Clean up test file\n",
    "try:\n",
    "    os.remove(test_file_path)\n",
    "    print(f\"\\n🧹 Cleaned up test file: {test_file_path}\")\n",
    "except:\n",
    "    print(f\"\\n⚠️  Could not clean up test file: {test_file_path}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 📚 Simplified Docling Usage\n",
    "\n",
    "The simplified approach uses only the local Python library, which is:\n",
    "\n",
    "**✅ Advantages:**\n",
    "- **Simple**: No API configuration needed\n",
    "- **Reliable**: No network dependencies\n",
    "- **Local**: All processing happens on your machine\n",
    "- **Consistent**: Same interface across different environments\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "# Process any document\n",
    "markdown_content = process_document_with_docling(\"/path/to/your/document.pdf\")\n",
    "\n",
    "# Use the result\n",
    "if markdown_content:\n",
    "    print(\"Document processed successfully!\")\n",
    "    # Use markdown_content for further processing\n",
    "```\n",
    "\n",
    "**Supported formats:**\n",
    "- PDF files\n",
    "- Word documents (.docx)\n",
    "- PowerPoint presentations (.pptx)  \n",
    "- HTML files\n",
    "- Plain text files\n",
    "- And more...\n",
    "\n",
    "This approach eliminates the complexity of API endpoints, authentication, and network issues while providing the same document processing capabilities.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ✅ FIXED: Simplified Nomic Embedding Test\n",
    "\n",
    "Using the correct model name and endpoint that matches your working curl command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined RAG Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a mini RAG pipeline\n",
    "print(\"Testing mini RAG pipeline...\\n\")\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"PGVector is a PostgreSQL extension that provides vector similarity search capabilities.\",\n",
    "    \"Vector databases store high-dimensional vectors and enable fast similarity searches.\",\n",
    "    \"Machine learning models convert text into numerical vectors called embeddings.\"\n",
    "]\n",
    "\n",
    "# Get embeddings for documents\n",
    "print(\"1. Generating document embeddings...\")\n",
    "doc_embeddings = get_embeddings(documents)\n",
    "if doc_embeddings is None:\n",
    "    print(\"   Failed to generate embeddings\")\n",
    "    exit()\n",
    "print(f\"   Generated {len(doc_embeddings)} embeddings\")\n",
    "\n",
    "# Query\n",
    "query = \"What is PGVector used for?\"\n",
    "print(f\"\\n2. Query: {query}\")\n",
    "\n",
    "# Get query embedding\n",
    "query_embeddings = get_embeddings([query])\n",
    "if query_embeddings is None:\n",
    "    print(\"   Failed to generate query embedding\")\n",
    "    exit()\n",
    "query_embedding = query_embeddings[0]\n",
    "\n",
    "# Find most similar document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "best_idx = np.argmax(similarities)\n",
    "\n",
    "print(f\"\\n3. Most relevant document (similarity: {similarities[best_idx]:.4f}):\")\n",
    "print(f\"   {documents[best_idx]}\")\n",
    "\n",
    "# Generate answer using context\n",
    "rag_prompt = f\"\"\"Context: {documents[best_idx]}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "\n",
    "print(\"\\n4. Generating answer...\")\n",
    "answer = llama_complete(rag_prompt, max_tokens=100, temperature=0.3)\n",
    "if answer:\n",
    "    print(f\"\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
