{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask RAG Questions\n",
    "\n",
    "This notebook demonstrates how to ask questions to the RAG system and get answers based on your ingested documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "from pgvector_rag import PGVectorRAG\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Initialize connections\n",
    "conn_params = {\n",
    "    \"host\": os.getenv('DB_HOST', 'postgres-pgvector.pgvector.svc.cluster.local'),\n",
    "    \"port\": int(os.getenv('DB_PORT', '5432')),\n",
    "    \"database\": os.getenv('DB_NAME', 'vectordb'),\n",
    "    \"user\": os.getenv('DB_USER', 'vectoruser'),\n",
    "    \"password\": os.getenv('DB_PASSWORD', 'vectorpass')\n",
    "}\n",
    "\n",
    "# API configurations\n",
    "NOMIC_URL = os.getenv('NOMIC_EMBED_URL')\n",
    "if NOMIC_URL and not NOMIC_URL.endswith('/v1'):\n",
    "    NOMIC_URL = f\"{NOMIC_URL}/v1\"\n",
    "NOMIC_API_KEY = os.getenv('NOMIC_EMBED_API_KEY')\n",
    "NOMIC_MODEL = os.getenv('NOMIC_EMBED_MODEL_NAME')\n",
    "\n",
    "LLAMA_URL = os.getenv('LLAMA_3-2_URL')\n",
    "if LLAMA_URL and not LLAMA_URL.endswith('/v1'):\n",
    "    LLAMA_URL = f\"{LLAMA_URL}/v1\"\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_3-2_API_KEY')\n",
    "LLAMA_MODEL = os.getenv('LLAMA_3-2_MODEL_NAME')\n",
    "\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG client\n",
    "rag = PGVectorRAG(conn_params)\n",
    "print(\"Connected to PGVector database\")\n",
    "\n",
    "# Set project ID\n",
    "PROJECT_ID = os.getenv('PROJECT_ID', 'demo_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Get embedding for a single text\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{NOMIC_URL}/embeddings\",\n",
    "        headers={\n",
    "            'Authorization': f\"Bearer {NOMIC_API_KEY}\",\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            'model': NOMIC_MODEL,\n",
    "            'input': text\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return np.array(data['data'][0]['embedding'])\n",
    "    else:\n",
    "        raise Exception(f\"Error getting embedding: {response.status_code}\")\n",
    "\n",
    "def generate_answer(prompt: str, max_tokens: int = 200) -> str:\n",
    "    \"\"\"Generate answer using Llama\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LLAMA_URL}/completions\",\n",
    "        headers={\n",
    "            'Authorization': f\"Bearer {LLAMA_API_KEY}\",\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        json={\n",
    "            'model': LLAMA_MODEL,\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': 0.3\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['choices'][0]['text'].strip()\n",
    "    else:\n",
    "        raise Exception(f\"Error generating answer: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project statistics\n",
    "stats = rag.get_project_stats(PROJECT_ID)\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "if stats:\n",
    "    print(f\"Total documents: {stats['total_documents']}\")\n",
    "    print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"Topics: {stats['topics']}\")\n",
    "    print(f\"Average chunk length: {stats['avg_chunk_length']:.0f} characters\")\n",
    "    print(f\"Storage size: {stats['storage_size_estimate']}\")\n",
    "else:\n",
    "    print(\"No project stats available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Question-Answering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(question: str, num_sources: int = 5, show_sources: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Ask a question to the RAG system\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        num_sources: Number of source documents to retrieve\n",
    "        show_sources: Whether to display source documents\n",
    "    \n",
    "    Returns:\n",
    "        Dict with answer and sources\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get question embedding\n",
    "    print(\"1. Generating question embedding...\")\n",
    "    question_embedding = get_embedding(question)\n",
    "    \n",
    "    # Search for relevant documents\n",
    "    print(f\"2. Searching for top {num_sources} relevant documents...\")\n",
    "    results = rag.dense_search(\n",
    "        project_id=PROJECT_ID,\n",
    "        query_embedding=question_embedding,\n",
    "        limit=num_sources\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return {\n",
    "            'answer': \"I couldn't find any relevant information to answer your question.\",\n",
    "            'sources': []\n",
    "        }\n",
    "    \n",
    "    # Prepare context from search results\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        context_parts.append(f\"[{i+1}] {result['chunk_text']}\")\n",
    "        sources.append({\n",
    "            'document': result['document_name'],\n",
    "            'page': result.get('page_number', 'N/A'),\n",
    "            'text': result['chunk_text'][:200] + '...' if len(result['chunk_text']) > 200 else result['chunk_text'],\n",
    "            'distance': result['distance']\n",
    "        })\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Generate answer\n",
    "    print(\"3. Generating answer...\")\n",
    "    prompt = f\"\"\"Based on the following context, answer the question. If the answer is not in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    answer = generate_answer(prompt, max_tokens=300)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"-\"*60)\n",
    "    print(answer)\n",
    "    \n",
    "    if show_sources:\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"SOURCES:\")\n",
    "        print(\"-\"*60)\n",
    "        for i, source in enumerate(sources):\n",
    "            print(f\"\\n[{i+1}] Document: {source['document']}\")\n",
    "            print(f\"    Page: {source['page']}\")\n",
    "            print(f\"    Distance: {source['distance']:.4f}\")\n",
    "            print(f\"    Preview: {source['text']}\")\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'sources': sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions - modify based on your ingested content\n",
    "questions = [\n",
    "    \"What is PGVector and what are its main features?\",\n",
    "    \"How does vector similarity search work?\",\n",
    "    \"What are the benefits of using RAG systems?\",\n",
    "    \"How do I create an index in PGVector?\",\n",
    "    \"What distance metrics does PGVector support?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the first question\n",
    "result = ask_rag(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive question asking\n",
    "# Uncomment to use in Jupyter\n",
    "# while True:\n",
    "#     question = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
    "#     if question.lower() == 'quit':\n",
    "#         break\n",
    "#     result = ask_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag_with_filters(\n",
    "    question: str, \n",
    "    topic: str | None = None, \n",
    "    metadata_filter: Dict | None = None,\n",
    "    num_sources: int = 5\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Ask RAG with topic and metadata filters\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    if topic:\n",
    "        print(f\"Topic filter: {topic}\")\n",
    "    if metadata_filter:\n",
    "        print(f\"Metadata filter: {metadata_filter}\")\n",
    "    \n",
    "    # Get embedding\n",
    "    question_embedding = get_embedding(question)\n",
    "    \n",
    "    # Search with filters\n",
    "    results = rag.dense_search(\n",
    "        project_id=PROJECT_ID,\n",
    "        query_embedding=question_embedding,\n",
    "        topic=topic,\n",
    "        metadata_filter=metadata_filter,\n",
    "        limit=num_sources\n",
    "    )\n",
    "    \n",
    "    if not results:\n",
    "        return {'answer': 'No results found with the specified filters.', 'sources': []}\n",
    "    \n",
    "    # Generate answer (same as before)\n",
    "    context = \"\\n\\n\".join([f\"[{i+1}] {r['chunk_text']}\" for i, r in enumerate(results)])\n",
    "    \n",
    "    prompt = f\"\"\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"\\nFound {len(results)} relevant documents\")\n",
    "    \n",
    "    return {'answer': answer, 'num_sources': len(results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with filters (adjust based on your data)\n",
    "# ask_rag_with_filters(\n",
    "#     \"What are the key features?\",\n",
    "#     topic=\"databases\",\n",
    "#     metadata_filter={\"file_type\": \"pdf\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGConversation:\n",
    "    def __init__(self, rag_client, project_id):\n",
    "        self.rag = rag_client\n",
    "        self.project_id = project_id\n",
    "        self.history = []\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        # Get embedding and search\n",
    "        embedding = get_embedding(question)\n",
    "        results = self.rag.dense_search(\n",
    "            project_id=self.project_id,\n",
    "            query_embedding=embedding,\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        # Build context including history\n",
    "        context = \"\\n\".join([r['chunk_text'] for r in results])\n",
    "        \n",
    "        history_text = \"\"\n",
    "        if self.history:\n",
    "            history_text = \"Previous conversation:\\n\"\n",
    "            for h in self.history[-3:]:  # Last 3 exchanges\n",
    "                history_text += f\"Q: {h['question']}\\nA: {h['answer'][:100]}...\\n\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"{history_text}Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "        answer = generate_answer(prompt)\n",
    "        \n",
    "        # Save to history\n",
    "        self.history.append({'question': question, 'answer': answer})\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation\n",
    "conversation = RAGConversation(rag, PROJECT_ID)\n",
    "\n",
    "print(\"Starting a conversation (memory-enabled):\\n\")\n",
    "\n",
    "# First question\n",
    "answer1 = conversation.ask(\"What is PGVector?\")\n",
    "print(f\"Q: What is PGVector?\")\n",
    "print(f\"A: {answer1}\\n\")\n",
    "\n",
    "# Follow-up question\n",
    "answer2 = conversation.ask(\"What are its main advantages?\")\n",
    "print(f\"Q: What are its main advantages?\")\n",
    "print(f\"A: {answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "rag.close()\n",
    "print(\"Closed database connection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
