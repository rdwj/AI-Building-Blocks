{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Statistics and Analytics\n",
    "\n",
    "This notebook provides insights into your PGVector RAG database, including statistics, visualizations, and performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msubplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "from pgvector_rag import PGVectorRAG\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database connection\n",
    "conn_params = {\n",
    "    \"host\": os.getenv('DB_HOST', 'postgres-pgvector.pgvector.svc.cluster.local'),\n",
    "    \"port\": int(os.getenv('DB_PORT', '5432')),\n",
    "    \"database\": os.getenv('DB_NAME', 'vectordb'),\n",
    "    \"user\": os.getenv('DB_USER', 'vectoruser'),\n",
    "    \"password\": os.getenv('DB_PASSWORD', 'vectorpass')\n",
    "}\n",
    "\n",
    "# Direct database connection for custom queries\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# RAG client\n",
    "rag = PGVectorRAG(conn_params)\n",
    "\n",
    "print(\"Connected to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overall statistics\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT project_id) as total_projects,\n",
    "        COUNT(DISTINCT document_id) as total_documents,\n",
    "        COUNT(*) as total_chunks,\n",
    "        AVG(LENGTH(chunk_text)) as avg_chunk_length,\n",
    "        MAX(LENGTH(chunk_text)) as max_chunk_length,\n",
    "        MIN(LENGTH(chunk_text)) as min_chunk_length,\n",
    "        COUNT(DISTINCT topic) as total_topics,\n",
    "        pg_size_pretty(pg_total_relation_size('document_chunks')) as table_size\n",
    "    FROM document_chunks\n",
    "\"\"\")\n",
    "\n",
    "stats = cur.fetchone()\n",
    "columns = ['total_projects', 'total_documents', 'total_chunks', 'avg_chunk_length', \n",
    "           'max_chunk_length', 'min_chunk_length', 'total_topics', 'table_size']\n",
    "\n",
    "stats_dict = dict(zip(columns, stats))\n",
    "\n",
    "print(\"üìä DATABASE OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in stats_dict.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-Level Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics by project\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        p.id as project_id,\n",
    "        p.name as project_name,\n",
    "        COUNT(DISTINCT dc.document_id) as num_documents,\n",
    "        COUNT(dc.id) as num_chunks,\n",
    "        AVG(LENGTH(dc.chunk_text)) as avg_chunk_length,\n",
    "        COUNT(DISTINCT dc.topic) as num_topics,\n",
    "        MIN(dc.created_at) as first_chunk,\n",
    "        MAX(dc.created_at) as last_chunk\n",
    "    FROM projects p\n",
    "    LEFT JOIN document_chunks dc ON p.id = dc.project_id\n",
    "    GROUP BY p.id, p.name\n",
    "    ORDER BY num_chunks DESC\n",
    "\"\"\")\n",
    "\n",
    "project_stats = pd.DataFrame(\n",
    "    cur.fetchall(),\n",
    "    columns=['project_id', 'project_name', 'num_documents', 'num_chunks', \n",
    "             'avg_chunk_length', 'num_topics', 'first_chunk', 'last_chunk']\n",
    ")\n",
    "\n",
    "print(\"üìÅ PROJECT STATISTICS\")\n",
    "print(project_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize project sizes\n",
    "if not project_stats.empty:\n",
    "    fig = px.bar(project_stats, \n",
    "                 x='project_name', \n",
    "                 y='num_chunks',\n",
    "                 title='Chunks per Project',\n",
    "                 labels={'num_chunks': 'Number of Chunks', 'project_name': 'Project'},\n",
    "                 color='num_documents',\n",
    "                 color_continuous_scale='Blues')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document statistics\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        document_name,\n",
    "        COUNT(*) as chunk_count,\n",
    "        AVG(LENGTH(chunk_text)) as avg_chunk_size,\n",
    "        MIN(page_number) as first_page,\n",
    "        MAX(page_number) as last_page,\n",
    "        COUNT(DISTINCT topic) as topics_covered\n",
    "    FROM document_chunks\n",
    "    GROUP BY document_name\n",
    "    ORDER BY chunk_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "doc_stats = pd.DataFrame(\n",
    "    cur.fetchall(),\n",
    "    columns=['document_name', 'chunk_count', 'avg_chunk_size', \n",
    "             'first_page', 'last_page', 'topics_covered']\n",
    ")\n",
    "\n",
    "print(\"üìÑ TOP 20 DOCUMENTS BY CHUNK COUNT\")\n",
    "print(doc_stats.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk size distribution\n",
    "cur.execute(\"\"\"\n",
    "    SELECT LENGTH(chunk_text) as chunk_length\n",
    "    FROM document_chunks\n",
    "    WHERE LENGTH(chunk_text) > 0\n",
    "\"\"\")\n",
    "\n",
    "chunk_lengths = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(chunk_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.axvline(np.mean(chunk_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(chunk_lengths):.0f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(chunk_lengths)\n",
    "plt.ylabel('Chunk Length (characters)')\n",
    "plt.title('Chunk Length Box Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nChunk Length Statistics:\")\n",
    "print(f\"Mean: {np.mean(chunk_lengths):.0f} characters\")\n",
    "print(f\"Median: {np.median(chunk_lengths):.0f} characters\")\n",
    "print(f\"Std Dev: {np.std(chunk_lengths):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(topic, 'Uncategorized') as topic,\n",
    "        COUNT(*) as chunk_count,\n",
    "        COUNT(DISTINCT document_id) as document_count,\n",
    "        AVG(LENGTH(chunk_text)) as avg_chunk_length\n",
    "    FROM document_chunks\n",
    "    GROUP BY topic\n",
    "    ORDER BY chunk_count DESC\n",
    "\"\"\")\n",
    "\n",
    "topic_stats = pd.DataFrame(\n",
    "    cur.fetchall(),\n",
    "    columns=['topic', 'chunk_count', 'document_count', 'avg_chunk_length']\n",
    ")\n",
    "\n",
    "print(\"üè∑Ô∏è  TOPIC DISTRIBUTION\")\n",
    "print(topic_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution\n",
    "if not topic_stats.empty:\n",
    "    # Pie chart for chunk distribution\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Chunks by Topic', 'Documents by Topic'),\n",
    "        specs=[[{'type':'pie'}, {'type':'pie'}]]\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=topic_stats['topic'], values=topic_stats['chunk_count'], name='Chunks'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=topic_stats['topic'], values=topic_stats['document_count'], name='Documents'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=400, title_text=\"Topic Distribution\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ingestion timeline\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        DATE(created_at) as date,\n",
    "        COUNT(*) as chunks_added,\n",
    "        COUNT(DISTINCT document_id) as documents_added\n",
    "    FROM document_chunks\n",
    "    WHERE created_at IS NOT NULL\n",
    "    GROUP BY DATE(created_at)\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "timeline = pd.DataFrame(\n",
    "    cur.fetchall(),\n",
    "    columns=['date', 'chunks_added', 'documents_added']\n",
    ")\n",
    "\n",
    "if not timeline.empty:\n",
    "    timeline['date'] = pd.to_datetime(timeline['date'])\n",
    "    \n",
    "    # Plot ingestion timeline\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=timeline['date'],\n",
    "        y=timeline['chunks_added'],\n",
    "        name='Chunks',\n",
    "        mode='lines+markers'\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=timeline['date'],\n",
    "        y=timeline['documents_added'],\n",
    "        name='Documents',\n",
    "        mode='lines+markers',\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Document Ingestion Timeline',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Chunks Added',\n",
    "        yaxis2=dict(\n",
    "            title='Documents Added',\n",
    "            overlaying='y',\n",
    "            side='right'\n",
    "        ),\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for documents with TTL\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_with_ttl,\n",
    "        MIN(expires_at) as earliest_expiration,\n",
    "        MAX(expires_at) as latest_expiration,\n",
    "        COUNT(CASE WHEN expires_at < CURRENT_TIMESTAMP THEN 1 END) as already_expired,\n",
    "        COUNT(CASE WHEN expires_at BETWEEN CURRENT_TIMESTAMP AND CURRENT_TIMESTAMP + INTERVAL '30 days' THEN 1 END) as expiring_soon\n",
    "    FROM document_chunks\n",
    "    WHERE expires_at IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "ttl_stats = cur.fetchone()\n",
    "\n",
    "if ttl_stats[0] > 0:  # If there are documents with TTL\n",
    "    print(\"\\n‚è∞ TTL STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total chunks with TTL: {ttl_stats[0]}\")\n",
    "    print(f\"Already expired: {ttl_stats[3]}\")\n",
    "    print(f\"Expiring in next 30 days: {ttl_stats[4]}\")\n",
    "    print(f\"Earliest expiration: {ttl_stats[1]}\")\n",
    "    print(f\"Latest expiration: {ttl_stats[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Index Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index information\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        indexname,\n",
    "        pg_size_pretty(pg_relation_size(indexname::regclass)) as index_size,\n",
    "        idx_scan as times_used,\n",
    "        idx_tup_read as tuples_read,\n",
    "        idx_tup_fetch as tuples_fetched\n",
    "    FROM pg_stat_user_indexes\n",
    "    WHERE schemaname = 'public' AND tablename = 'document_chunks'\n",
    "    ORDER BY idx_scan DESC\n",
    "\"\"\")\n",
    "\n",
    "index_stats = pd.DataFrame(\n",
    "    cur.fetchall(),\n",
    "    columns=['index_name', 'index_size', 'times_used', 'tuples_read', 'tuples_fetched']\n",
    ")\n",
    "\n",
    "print(\"üîç INDEX USAGE STATISTICS\")\n",
    "print(index_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vector completeness\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_chunks,\n",
    "        COUNT(dense_embedding) as chunks_with_dense,\n",
    "        COUNT(sparse_embedding) as chunks_with_sparse,\n",
    "        COUNT(CASE WHEN dense_embedding IS NOT NULL AND sparse_embedding IS NOT NULL THEN 1 END) as chunks_with_both,\n",
    "        COUNT(CASE WHEN dense_embedding IS NULL AND sparse_embedding IS NULL THEN 1 END) as chunks_with_neither\n",
    "    FROM document_chunks\n",
    "\"\"\")\n",
    "\n",
    "vector_stats = cur.fetchone()\n",
    "vector_labels = ['Total Chunks', 'With Dense Embeddings', 'With Sparse Embeddings', \n",
    "                 'With Both', 'With Neither']\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(vector_labels, vector_stats)\n",
    "plt.ylabel('Number of Chunks')\n",
    "plt.title('Vector Embedding Coverage')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, vector_stats):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(vector_stats), \n",
    "             f'{value:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentages\n",
    "total = vector_stats[0]\n",
    "if total > 0:\n",
    "    print(\"\\nüìä EMBEDDING COVERAGE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Dense embedding coverage: {vector_stats[1]/total*100:.1f}%\")\n",
    "    print(f\"Sparse embedding coverage: {vector_stats[2]/total*100:.1f}%\")\n",
    "    print(f\"Both embeddings: {vector_stats[3]/total*100:.1f}%\")\n",
    "    print(f\"Missing embeddings: {vector_stats[4]/total*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metadata usage\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        jsonb_object_keys(metadata) as metadata_key,\n",
    "        COUNT(*) as usage_count\n",
    "    FROM document_chunks\n",
    "    WHERE metadata != '{}'\n",
    "    GROUP BY metadata_key\n",
    "    ORDER BY usage_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "metadata_keys = pd.DataFrame(\n",
    "    cur.fetchall(),\n",
    "    columns=['metadata_key', 'usage_count']\n",
    ")\n",
    "\n",
    "if not metadata_keys.empty:\n",
    "    print(\"üè∑Ô∏è  METADATA KEY USAGE\")\n",
    "    print(metadata_keys.to_string(index=False))\n",
    "    \n",
    "    # Visualize metadata keys\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(metadata_keys['metadata_key'][:10], metadata_keys['usage_count'][:10])\n",
    "    plt.xlabel('Usage Count')\n",
    "    plt.title('Top 10 Metadata Keys')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query performance\n",
    "import time\n",
    "\n",
    "def test_query_performance(query_type, query, params=None):\n",
    "    \"\"\"Test and time a query\"\"\"\n",
    "    start_time = time.time()\n",
    "    cur.execute(query, params)\n",
    "    results = cur.fetchall()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        'query_type': query_type,\n",
    "        'execution_time': end_time - start_time,\n",
    "        'row_count': len(results)\n",
    "    }\n",
    "\n",
    "# Test various query types\n",
    "performance_tests = []\n",
    "\n",
    "# Simple count\n",
    "performance_tests.append(test_query_performance(\n",
    "    \"Simple Count\",\n",
    "    \"SELECT COUNT(*) FROM document_chunks\"\n",
    "))\n",
    "\n",
    "# Filtered search\n",
    "performance_tests.append(test_query_performance(\n",
    "    \"Filtered Search\",\n",
    "    \"SELECT * FROM document_chunks WHERE topic = %s LIMIT 100\",\n",
    "    ('database',)\n",
    "))\n",
    "\n",
    "# Metadata search\n",
    "performance_tests.append(test_query_performance(\n",
    "    \"Metadata Search\",\n",
    "    \"SELECT * FROM document_chunks WHERE metadata @> %s LIMIT 100\",\n",
    "    (json.dumps({'file_type': 'pdf'}),)\n",
    "))\n",
    "\n",
    "# Vector similarity (if you have a sample vector)\n",
    "try:\n",
    "    cur.execute(\"SELECT dense_embedding FROM document_chunks WHERE dense_embedding IS NOT NULL LIMIT 1\")\n",
    "    sample_vector = cur.fetchone()\n",
    "    if sample_vector:\n",
    "        performance_tests.append(test_query_performance(\n",
    "            \"Vector Similarity\",\n",
    "            \"SELECT * FROM document_chunks WHERE dense_embedding IS NOT NULL ORDER BY dense_embedding <=> %s LIMIT 10\",\n",
    "            (sample_vector[0],)\n",
    "        ))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Display results\n",
    "perf_df = pd.DataFrame(performance_tests)\n",
    "print(\"‚ö° QUERY PERFORMANCE TEST\")\n",
    "print(\"=\" * 50)\n",
    "for _, row in perf_df.iterrows():\n",
    "    print(f\"{row['query_type']}: {row['execution_time']*1000:.2f}ms ({row['row_count']} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed storage information\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        'document_chunks' as table_name,\n",
    "        pg_size_pretty(pg_table_size('document_chunks')) as table_size,\n",
    "        pg_size_pretty(pg_indexes_size('document_chunks')) as indexes_size,\n",
    "        pg_size_pretty(pg_total_relation_size('document_chunks')) as total_size,\n",
    "        (pg_indexes_size('document_chunks')::float / pg_total_relation_size('document_chunks')::float * 100)::int as index_percentage\n",
    "\"\"\")\n",
    "\n",
    "storage_info = cur.fetchone()\n",
    "\n",
    "print(\"üíæ STORAGE BREAKDOWN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Table size: {storage_info[1]}\")\n",
    "print(f\"Indexes size: {storage_info[2]}\")\n",
    "print(f\"Total size: {storage_info[3]}\")\n",
    "print(f\"Indexes are {storage_info[4]}% of total size\")\n",
    "\n",
    "# Visualize storage\n",
    "labels = ['Table Data', 'Indexes']\n",
    "sizes = [100 - storage_info[4], storage_info[4]]\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Storage Distribution')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = {\n",
    "    'generated_at': datetime.now().isoformat(),\n",
    "    'database_overview': stats_dict,\n",
    "    'projects': project_stats.to_dict('records'),\n",
    "    'top_documents': doc_stats.head(10).to_dict('records'),\n",
    "    'topics': topic_stats.to_dict('records'),\n",
    "    'vector_coverage': {\n",
    "        'dense_coverage_pct': (vector_stats[1]/vector_stats[0]*100) if vector_stats[0] > 0 else 0,\n",
    "        'sparse_coverage_pct': (vector_stats[2]/vector_stats[0]*100) if vector_stats[0] > 0 else 0,\n",
    "        'both_coverage_pct': (vector_stats[3]/vector_stats[0]*100) if vector_stats[0] > 0 else 0\n",
    "    },\n",
    "    'performance_metrics': perf_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Save report\n",
    "report_filename = f\"pgvector_stats_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüìÑ Report saved to: {report_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connections\n",
    "cur.close()\n",
    "conn.close()\n",
    "rag.close()\n",
    "\n",
    "print(\"Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
