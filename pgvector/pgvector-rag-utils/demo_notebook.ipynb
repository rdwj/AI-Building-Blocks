{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGVector RAG Demo for OpenShift AI\n",
    "\n",
    "This notebook demonstrates how to use the PGVector RAG system from a Jupyter notebook in OpenShift AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install psycopg2-binary pgvector numpy sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/vector-utils')  # Adjust path as needed\n",
    "\n",
    "from pgvector_rag import PGVectorRAG\n",
    "import numpy as np\n",
    "import uuid\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "except ImportError:\n",
    "    print(\"sentence-transformers not installed. Run: pip install sentence-transformers\")\n",
    "    SentenceTransformer = None\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters\n",
    "conn_params = {\n",
    "    \"host\": \"postgres-pgvector.pgvector.svc.cluster.local\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"vectordb\",\n",
    "    \"user\": \"vectoruser\",\n",
    "    \"password\": \"vectorpass\"\n",
    "}\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "logger.info(f\"Model dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Project and Add Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG client\n",
    "rag = PGVectorRAG(conn_params)\n",
    "\n",
    "# Create a project\n",
    "project_id = \"notebook_demo\"\n",
    "rag.create_project(\n",
    "    project_id=project_id,\n",
    "    name=\"Notebook Demo Project\",\n",
    "    description=\"Demo from Jupyter notebook\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"Introduction to RAG\",\n",
    "        \"chunks\": [\n",
    "            \"Retrieval-Augmented Generation (RAG) combines the benefits of retrieval-based and generative AI models.\",\n",
    "            \"RAG systems first retrieve relevant documents, then use them to generate contextually appropriate responses.\",\n",
    "            \"This approach reduces hallucinations and provides more accurate, grounded responses.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"PGVector Overview\",\n",
    "        \"chunks\": [\n",
    "            \"PGVector is an open-source extension for PostgreSQL that enables vector similarity search.\",\n",
    "            \"It supports multiple distance metrics including L2, inner product, and cosine distance.\",\n",
    "            \"With version 0.8.0, PGVector now supports sparse vectors for hybrid search capabilities.\"\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the system\n",
    "for doc in documents:\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    \n",
    "    for idx, chunk in enumerate(doc[\"chunks\"]):\n",
    "        # Generate embedding\n",
    "        embedding = model.encode(chunk)\n",
    "        \n",
    "        # Add to database\n",
    "        chunk_id = rag.add_document_chunk(\n",
    "            project_id=project_id,\n",
    "            document_id=doc_id,\n",
    "            document_name=doc[\"title\"],\n",
    "            chunk_text=chunk,\n",
    "            chunk_index=idx,\n",
    "            dense_embedding=embedding,\n",
    "            metadata={\"source\": \"notebook_demo\"},\n",
    "            topic=\"ai_ml\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Added chunk from '{doc['title']}': {chunk[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display search results nicely\n",
    "def display_results(results, query):\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Found {len(results)} results:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}. Document: {result['document_name']}\")\n",
    "        print(f\"   Text: {result['chunk_text']}\")\n",
    "        print(f\"   Distance: {result.get('distance', 'N/A')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search example 1: Basic semantic search\n",
    "query = \"How does RAG reduce errors in AI responses?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "results = rag.dense_search(\n",
    "    project_id=project_id,\n",
    "    query_embedding=query_embedding,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "display_results(results, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search example 2: Filtered search\n",
    "query = \"vector database features\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "results = rag.dense_search(\n",
    "    project_id=project_id,\n",
    "    query_embedding=query_embedding,\n",
    "    topic=\"ai_ml\",  # Filter by topic\n",
    "    metadata_filter={\"source\": \"notebook_demo\"},  # Filter by metadata\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "display_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Project Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project statistics\n",
    "stats = rag.get_project_stats(project_id)\n",
    "\n",
    "print(\"Project Statistics:\")\n",
    "if stats:\n",
    "    print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "    print(f\"Total documents: {stats['total_documents']}\")\n",
    "    print(f\"Topics: {stats['topics']}\")\n",
    "    print(f\"Average chunk length: {stats['avg_chunk_length']:.1f} characters\")\n",
    "    print(f\"Estimated storage size: {stats['storage_size_estimate']}\")\n",
    "else:\n",
    "    print(\"No project stats available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of batch processing for better performance\n",
    "batch_chunks = []\n",
    "\n",
    "# Simulate processing a larger document\n",
    "large_doc_id = str(uuid.uuid4())\n",
    "for i in range(10):\n",
    "    chunk_text = f\"This is chunk {i} of a larger document about machine learning and AI.\"\n",
    "    \n",
    "    batch_chunks.append({\n",
    "        \"document_id\": large_doc_id,\n",
    "        \"document_name\": \"Large ML Document\",\n",
    "        \"chunk_text\": chunk_text,\n",
    "        \"chunk_index\": i,\n",
    "        \"dense_embedding\": model.encode(chunk_text),\n",
    "        \"metadata\": {\"batch\": True, \"chunk_num\": i}\n",
    "    })\n",
    "\n",
    "# Add all chunks in one transaction\n",
    "chunk_ids = rag.add_document_chunks_batch(project_id, batch_chunks)\n",
    "print(f\"Added {len(chunk_ids)} chunks in batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection when done\n",
    "rag.close()\n",
    "print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Implement Sparse Embeddings**: Add SPLADE or BM25 for hybrid search\n",
    "2. **Document Processing**: Add document parsing (PDF, DOCX, etc.)\n",
    "3. **Advanced Chunking**: Implement sliding window or semantic chunking\n",
    "4. **Production Pipeline**: Create automated ingestion pipelines\n",
    "5. **Monitoring**: Add performance metrics and monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
