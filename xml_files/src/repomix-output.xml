This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
handlers/
  bpmn_handler.py
  enterprise_config_handler.py
  openapi_xml_handler.py
  properties_xml_handler.py
  test_report_handler.py
  wsdl_handler.py
  xsd_handler.py
additional_xml_handlers.py
additional-xml-handlers_OLD.py
agent_integration.py
xml_document_analysis_framework.py
xml_framework_demo_script.py
xml_schema_analyzer_fixed.py
xml_schema_analyzer.py
xml_specialized_handlers.py
xml-architecture-diagram.svg
xml-chunking-strategy.py
xml-demo-script.py
xml-framework-roadmap.md
xml-specialized-handlers.py
xml-test-files-guide.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="handlers/bpmn_handler.py">
#!/usr/bin/env python3
"""
BPMN (Business Process Model and Notation) Handler

Analyzes BPMN 2.0 XML files to extract process definitions,
activities, gateways, events, and flows for process mining
and optimization.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class BPMNHandler(XMLHandler):
    """Handler for BPMN 2.0 process definition files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for BPMN namespace
        if any('bpmn' in uri.lower() or 'omg.org/spec/BPMN' in uri for uri in namespaces.values()):
            return True, 1.0
        
        # Check for BPMN root element
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        if root_tag == 'definitions':
            # Check for BPMN elements
            bpmn_elements = ['process', 'startEvent', 'endEvent', 'task', 'gateway']
            found = sum(1 for elem in bpmn_elements if root.find(f'.//*[local-name()="{elem}"]') is not None)
            if found >= 2:
                return True, min(found * 0.2, 0.9)
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Extract BPMN version
        version = "2.0"  # Default
        for uri in namespaces.values():
            if 'BPMN/2' in uri:
                version = "2.0"
            elif 'BPMN/1' in uri:
                version = "1.2"
        
        # Check if it's executable
        executable = root.get('isExecutable', 'false') == 'true'
        
        return DocumentTypeInfo(
            type_name="BPMN Process Definition",
            confidence=0.95,
            version=version,
            metadata={
                "standard": "OMG BPMN",
                "category": "business_process",
                "executable": executable
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'processes': self._analyze_processes(root),
            'activities': self._analyze_activities(root),
            'gateways': self._analyze_gateways(root),
            'events': self._analyze_events(root),
            'flows': self._analyze_flows(root),
            'lanes': self._analyze_lanes(root),
            'data_objects': self._analyze_data_objects(root),
            'complexity_metrics': self._calculate_complexity_metrics(root)
        }
        
        recommendations = [
            "Analyze process bottlenecks and optimization opportunities",
            "Generate process documentation and training materials",
            "Validate against BPMN best practices",
            "Extract for process mining and analytics",
            "Monitor process execution patterns",
            "Identify automation candidates"
        ]
        
        ai_use_cases = [
            "Process optimization recommendations",
            "Bottleneck detection and analysis",
            "Process mining and discovery",
            "Compliance checking",
            "Resource allocation optimization",
            "Process simulation",
            "Automated documentation generation",
            "Process variant analysis"
        ]
        
        return SpecializedAnalysis(
            document_type="BPMN Process Definition",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'processes': len(findings['processes']),
                'activities': findings['activities']['total'],
                'gateways': findings['gateways']['total'],
                'events': findings['events']['total'],
                'flows': len(findings['flows'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_process_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'process_hierarchy': self._extract_process_hierarchy(root),
            'activity_sequences': self._extract_activity_sequences(root),
            'decision_points': self._extract_decision_points(root),
            'resource_assignments': self._extract_resource_assignments(root),
            'process_metrics': self._extract_process_metrics(root)
        }
    
    def _analyze_processes(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Analyze process definitions"""
        processes = []
        
        for process in root.findall('.//*[local-name()="process"]'):
            process_info = {
                'id': process.get('id'),
                'name': process.get('name'),
                'is_executable': process.get('isExecutable', 'false') == 'true',
                'process_type': process.get('processType', 'None'),
                'is_closed': process.get('isClosed', 'false') == 'true',
                'elements': {
                    'activities': len(process.findall('.//*[local-name()="task"]') + 
                                    process.findall('.//*[local-name()="subProcess"]') +
                                    process.findall('.//*[local-name()="callActivity"]')),
                    'gateways': len(process.findall('.//*[local-name()="exclusiveGateway"]') +
                                   process.findall('.//*[local-name()="parallelGateway"]') +
                                   process.findall('.//*[local-name()="inclusiveGateway"]') +
                                   process.findall('.//*[local-name()="eventBasedGateway"]')),
                    'events': len(process.findall('.//*[local-name()="startEvent"]') +
                                 process.findall('.//*[local-name()="endEvent"]') +
                                 process.findall('.//*[local-name()="intermediateThrowEvent"]') +
                                 process.findall('.//*[local-name()="intermediateCatchEvent"]'))
                }
            }
            
            # Extract documentation
            doc = process.find('.//*[local-name()="documentation"]')
            if doc is not None and doc.text:
                process_info['documentation'] = doc.text.strip()
            
            processes.append(process_info)
        
        return processes
    
    def _analyze_activities(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze all activities (tasks, subprocesses, etc.)"""
        activities = {
            'total': 0,
            'by_type': {},
            'manual_tasks': [],
            'service_tasks': [],
            'user_tasks': [],
            'script_tasks': [],
            'subprocesses': [],
            'call_activities': []
        }
        
        # Analyze different task types
        task_types = [
            ('task', 'generic'),
            ('userTask', 'user'),
            ('serviceTask', 'service'),
            ('scriptTask', 'script'),
            ('manualTask', 'manual'),
            ('businessRuleTask', 'business_rule'),
            ('sendTask', 'send'),
            ('receiveTask', 'receive')
        ]
        
        for task_elem, task_type in task_types:
            tasks = root.findall(f'.//*[local-name()="{task_elem}"]')
            activities['by_type'][task_type] = len(tasks)
            activities['total'] += len(tasks)
            
            # Store specific task details
            for task in tasks:
                task_info = {
                    'id': task.get('id'),
                    'name': task.get('name'),
                    'type': task_type
                }
                
                if task_type == 'user':
                    activities['user_tasks'].append(task_info)
                elif task_type == 'service':
                    activities['service_tasks'].append(task_info)
                elif task_type == 'script':
                    activities['script_tasks'].append(task_info)
                elif task_type == 'manual':
                    activities['manual_tasks'].append(task_info)
        
        # Analyze subprocesses
        for subprocess in root.findall('.//*[local-name()="subProcess"]'):
            subprocess_info = {
                'id': subprocess.get('id'),
                'name': subprocess.get('name'),
                'triggered_by_event': subprocess.get('triggeredByEvent', 'false') == 'true',
                'is_expanded': subprocess.get('isExpanded', 'true') == 'true',
                'child_activities': len(subprocess.findall('.//*[local-name()="task"]'))
            }
            activities['subprocesses'].append(subprocess_info)
            activities['total'] += 1
        
        # Analyze call activities
        for call_activity in root.findall('.//*[local-name()="callActivity"]'):
            call_info = {
                'id': call_activity.get('id'),
                'name': call_activity.get('name'),
                'called_element': call_activity.get('calledElement')
            }
            activities['call_activities'].append(call_info)
            activities['total'] += 1
        
        return activities
    
    def _analyze_gateways(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze gateways (decision points)"""
        gateways = {
            'total': 0,
            'exclusive': [],
            'parallel': [],
            'inclusive': [],
            'event_based': [],
            'complex': []
        }
        
        gateway_types = [
            ('exclusiveGateway', 'exclusive'),
            ('parallelGateway', 'parallel'),
            ('inclusiveGateway', 'inclusive'),
            ('eventBasedGateway', 'event_based'),
            ('complexGateway', 'complex')
        ]
        
        for gateway_elem, gateway_type in gateway_types:
            for gateway in root.findall(f'.//*[local-name()="{gateway_elem}"]'):
                gateway_info = {
                    'id': gateway.get('id'),
                    'name': gateway.get('name'),
                    'gateway_direction': gateway.get('gatewayDirection', 'Unspecified'),
                    'incoming_flows': len(gateway.findall('.//*[local-name()="incoming"]')),
                    'outgoing_flows': len(gateway.findall('.//*[local-name()="outgoing"]'))
                }
                
                gateways[gateway_type].append(gateway_info)
                gateways['total'] += 1
        
        return gateways
    
    def _analyze_events(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze events (start, end, intermediate)"""
        events = {
            'total': 0,
            'start_events': [],
            'end_events': [],
            'intermediate_throw': [],
            'intermediate_catch': [],
            'boundary_events': []
        }
        
        # Start events
        for event in root.findall('.//*[local-name()="startEvent"]'):
            event_info = {
                'id': event.get('id'),
                'name': event.get('name'),
                'is_interrupting': event.get('isInterrupting', 'true') == 'true',
                'event_type': self._determine_event_type(event)
            }
            events['start_events'].append(event_info)
            events['total'] += 1
        
        # End events
        for event in root.findall('.//*[local-name()="endEvent"]'):
            event_info = {
                'id': event.get('id'),
                'name': event.get('name'),
                'event_type': self._determine_event_type(event)
            }
            events['end_events'].append(event_info)
            events['total'] += 1
        
        # Intermediate throw events
        for event in root.findall('.//*[local-name()="intermediateThrowEvent"]'):
            event_info = {
                'id': event.get('id'),
                'name': event.get('name'),
                'event_type': self._determine_event_type(event)
            }
            events['intermediate_throw'].append(event_info)
            events['total'] += 1
        
        # Intermediate catch events
        for event in root.findall('.//*[local-name()="intermediateCatchEvent"]'):
            event_info = {
                'id': event.get('id'),
                'name': event.get('name'),
                'event_type': self._determine_event_type(event)
            }
            events['intermediate_catch'].append(event_info)
            events['total'] += 1
        
        # Boundary events
        for event in root.findall('.//*[local-name()="boundaryEvent"]'):
            event_info = {
                'id': event.get('id'),
                'name': event.get('name'),
                'attached_to': event.get('attachedToRef'),
                'cancel_activity': event.get('cancelActivity', 'true') == 'true',
                'event_type': self._determine_event_type(event)
            }
            events['boundary_events'].append(event_info)
            events['total'] += 1
        
        return events
    
    def _determine_event_type(self, event: ET.Element) -> str:
        """Determine the specific type of event"""
        event_types = [
            'messageEventDefinition', 'timerEventDefinition', 'errorEventDefinition',
            'signalEventDefinition', 'compensateEventDefinition', 'conditionalEventDefinition',
            'escalationEventDefinition', 'linkEventDefinition', 'terminateEventDefinition'
        ]
        
        for event_type in event_types:
            if event.find(f'.//*[local-name()="{event_type}"]') is not None:
                return event_type.replace('EventDefinition', '').lower()
        
        return 'none'
    
    def _analyze_flows(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Analyze sequence flows"""
        flows = []
        
        for flow in root.findall('.//*[local-name()="sequenceFlow"]'):
            flow_info = {
                'id': flow.get('id'),
                'name': flow.get('name'),
                'source': flow.get('sourceRef'),
                'target': flow.get('targetRef'),
                'is_default': self._is_default_flow(root, flow),
                'condition': None
            }
            
            # Extract condition expression
            condition = flow.find('.//*[local-name()="conditionExpression"]')
            if condition is not None and condition.text:
                flow_info['condition'] = condition.text.strip()
            
            flows.append(flow_info)
        
        return flows
    
    def _is_default_flow(self, root: ET.Element, flow: ET.Element) -> bool:
        """Check if a flow is marked as default"""
        flow_id = flow.get('id')
        
        # Check gateways for default flow references
        for gateway in root.findall('.//*[@default]'):
            if gateway.get('default') == flow_id:
                return True
        
        return False
    
    def _analyze_lanes(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Analyze lanes and pools (organizational units)"""
        lanes = []
        
        for lane in root.findall('.//*[local-name()="lane"]'):
            lane_info = {
                'id': lane.get('id'),
                'name': lane.get('name'),
                'flow_node_refs': []
            }
            
            # Get referenced flow nodes
            for ref in lane.findall('.//*[local-name()="flowNodeRef"]'):
                if ref.text:
                    lane_info['flow_node_refs'].append(ref.text)
            
            lanes.append(lane_info)
        
        return lanes
    
    def _analyze_data_objects(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Analyze data objects and data stores"""
        data_objects = []
        
        # Data objects
        for data_obj in root.findall('.//*[local-name()="dataObject"]'):
            data_objects.append({
                'id': data_obj.get('id'),
                'name': data_obj.get('name'),
                'type': 'data_object',
                'is_collection': data_obj.get('isCollection', 'false') == 'true'
            })
        
        # Data object references
        for data_ref in root.findall('.//*[local-name()="dataObjectReference"]'):
            data_objects.append({
                'id': data_ref.get('id'),
                'name': data_ref.get('name'),
                'type': 'data_object_reference',
                'data_object_ref': data_ref.get('dataObjectRef')
            })
        
        # Data stores
        for data_store in root.findall('.//*[local-name()="dataStore"]'):
            data_objects.append({
                'id': data_store.get('id'),
                'name': data_store.get('name'),
                'type': 'data_store',
                'capacity': data_store.get('capacity'),
                'is_unlimited': data_store.get('isUnlimited', 'false') == 'true'
            })
        
        return data_objects
    
    def _calculate_complexity_metrics(self, findings: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate process complexity metrics"""
        # Extract base counts from findings
        if isinstance(findings, ET.Element):
            # If called directly with root element
            root = findings
            total_activities = len(root.findall('.//*[local-name()="task"]'))
            total_gateways = len(root.findall('.//*[local-name()="exclusiveGateway"]')) + \
                           len(root.findall('.//*[local-name()="parallelGateway"]')) + \
                           len(root.findall('.//*[local-name()="inclusiveGateway"]'))
            total_events = len(root.findall('.//*[local-name()="startEvent"]')) + \
                          len(root.findall('.//*[local-name()="endEvent"]'))
            total_flows = len(root.findall('.//*[local-name()="sequenceFlow"]'))
        else:
            # If called with findings dict
            total_activities = findings.get('activities', {}).get('total', 0)
            total_gateways = findings.get('gateways', {}).get('total', 0)
            total_events = findings.get('events', {}).get('total', 0)
            total_flows = len(findings.get('flows', []))
        
        # Calculate metrics
        metrics = {
            'cyclomatic_complexity': total_gateways + 1,  # Simplified McCabe complexity
            'activity_complexity': total_activities,
            'control_flow_complexity': total_gateways * 2 + total_events,
            'size_metrics': {
                'total_elements': total_activities + total_gateways + total_events,
                'total_flows': total_flows
            },
            'complexity_score': 0.0
        }
        
        # Calculate overall complexity score (0-1)
        if metrics['size_metrics']['total_elements'] > 0:
            complexity_factors = [
                min(metrics['cyclomatic_complexity'] / 10, 1.0) * 0.4,
                min(metrics['activity_complexity'] / 30, 1.0) * 0.3,
                min(metrics['control_flow_complexity'] / 20, 1.0) * 0.3
            ]
            metrics['complexity_score'] = sum(complexity_factors)
        
        return metrics
    
    def _extract_process_hierarchy(self, root: ET.Element) -> Dict[str, Any]:
        """Extract process hierarchy and relationships"""
        hierarchy = {}
        
        for process in root.findall('.//*[local-name()="process"]'):
            process_id = process.get('id')
            hierarchy[process_id] = {
                'name': process.get('name'),
                'subprocesses': [],
                'call_activities': []
            }
            
            # Find subprocesses
            for subprocess in process.findall('.//*[local-name()="subProcess"]'):
                hierarchy[process_id]['subprocesses'].append({
                    'id': subprocess.get('id'),
                    'name': subprocess.get('name')
                })
            
            # Find call activities
            for call_activity in process.findall('.//*[local-name()="callActivity"]'):
                hierarchy[process_id]['call_activities'].append({
                    'id': call_activity.get('id'),
                    'name': call_activity.get('name'),
                    'called_element': call_activity.get('calledElement')
                })
        
        return hierarchy
    
    def _extract_activity_sequences(self, root: ET.Element) -> List[List[str]]:
        """Extract common activity sequences"""
        sequences = []
        
        # Build flow graph
        flow_graph = {}
        for flow in root.findall('.//*[local-name()="sequenceFlow"]'):
            source = flow.get('sourceRef')
            target = flow.get('targetRef')
            if source not in flow_graph:
                flow_graph[source] = []
            flow_graph[source].append(target)
        
        # Find paths from start to end events
        start_events = [e.get('id') for e in root.findall('.//*[local-name()="startEvent"]')]
        end_events = [e.get('id') for e in root.findall('.//*[local-name()="endEvent"]')]
        
        # Simple path extraction (limited to prevent explosion)
        for start in start_events[:3]:  # Limit starts
            paths = self._find_paths(flow_graph, start, end_events, max_paths=5)
            sequences.extend(paths)
        
        return sequences[:10]  # Limit total sequences
    
    def _find_paths(self, graph: Dict[str, List[str]], start: str, ends: List[str], 
                   max_paths: int = 5) -> List[List[str]]:
        """Find paths in graph (simplified DFS)"""
        paths = []
        
        def dfs(node: str, path: List[str], visited: set):
            if len(paths) >= max_paths:
                return
            
            if node in ends:
                paths.append(path + [node])
                return
            
            if node in visited or len(path) > 20:  # Prevent infinite loops
                return
            
            visited.add(node)
            
            if node in graph:
                for next_node in graph[node]:
                    dfs(next_node, path + [node], visited.copy())
        
        dfs(start, [], set())
        return paths
    
    def _extract_decision_points(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract decision points and their conditions"""
        decision_points = []
        
        # Exclusive gateways with conditions
        for gateway in root.findall('.//*[local-name()="exclusiveGateway"]'):
            gateway_id = gateway.get('id')
            decision = {
                'id': gateway_id,
                'name': gateway.get('name'),
                'type': 'exclusive',
                'conditions': []
            }
            
            # Find outgoing flows with conditions
            for flow in root.findall('.//*[local-name()="sequenceFlow"]'):
                if flow.get('sourceRef') == gateway_id:
                    condition = flow.find('.//*[local-name()="conditionExpression"]')
                    if condition is not None and condition.text:
                        decision['conditions'].append({
                            'flow_id': flow.get('id'),
                            'target': flow.get('targetRef'),
                            'condition': condition.text.strip()
                        })
            
            if decision['conditions']:
                decision_points.append(decision)
        
        return decision_points[:10]  # Limit
    
    def _extract_resource_assignments(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract resource assignments from lanes and tasks"""
        assignments = []
        
        # From lanes
        for lane in root.findall('.//*[local-name()="lane"]'):
            lane_name = lane.get('name')
            for ref in lane.findall('.//*[local-name()="flowNodeRef"]'):
                if ref.text:
                    assignments.append({
                        'resource': lane_name,
                        'activity': ref.text,
                        'type': 'lane_assignment'
                    })
        
        # From user tasks with assignments
        for task in root.findall('.//*[local-name()="userTask"]'):
            # Check for resource role
            resource_role = task.find('.//*[local-name()="resourceRole"]')
            if resource_role is not None:
                resource_ref = resource_role.find('.//*[local-name()="resourceRef"]')
                if resource_ref is not None and resource_ref.text:
                    assignments.append({
                        'resource': resource_ref.text,
                        'activity': task.get('id'),
                        'type': 'resource_role'
                    })
            
            # Check for performers
            performer = task.find('.//*[local-name()="performer"]')
            if performer is not None:
                resource_ref = performer.find('.//*[local-name()="resourceRef"]')
                if resource_ref is not None and resource_ref.text:
                    assignments.append({
                        'resource': resource_ref.text,
                        'activity': task.get('id'),
                        'type': 'performer'
                    })
        
        return assignments[:20]  # Limit
    
    def _extract_process_metrics(self, root: ET.Element) -> Dict[str, Any]:
        """Extract process metrics and KPIs"""
        metrics = {
            'process_count': len(root.findall('.//*[local-name()="process"]')),
            'avg_activities_per_process': 0.0,
            'gateway_distribution': {},
            'event_distribution': {},
            'automation_potential': 0.0
        }
        
        # Calculate averages
        total_activities = 0
        processes = root.findall('.//*[local-name()="process"]')
        
        for process in processes:
            total_activities += len(process.findall('.//*[local-name()="task"]'))
        
        if processes:
            metrics['avg_activities_per_process'] = total_activities / len(processes)
        
        # Gateway distribution
        gateway_types = ['exclusiveGateway', 'parallelGateway', 'inclusiveGateway', 'eventBasedGateway']
        for gw_type in gateway_types:
            count = len(root.findall(f'.//*[local-name()="{gw_type}"]'))
            if count > 0:
                metrics['gateway_distribution'][gw_type] = count
        
        # Event distribution
        event_types = ['startEvent', 'endEvent', 'intermediateThrowEvent', 'intermediateCatchEvent', 'boundaryEvent']
        for event_type in event_types:
            count = len(root.findall(f'.//*[local-name()="{event_type}"]'))
            if count > 0:
                metrics['event_distribution'][event_type] = count
        
        # Automation potential (ratio of service/script tasks to all tasks)
        service_tasks = len(root.findall('.//*[local-name()="serviceTask"]'))
        script_tasks = len(root.findall('.//*[local-name()="scriptTask"]'))
        total_tasks = len(root.findall('.//*[local-name()="task"]'))
        
        if total_tasks > 0:
            metrics['automation_potential'] = (service_tasks + script_tasks) / total_tasks
        
        return metrics
    
    def _assess_process_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess process model quality"""
        # Completeness - all activities have names
        completeness = 0.0
        total_activities = findings['activities']['total']
        if total_activities > 0:
            # Estimate based on typical patterns
            completeness = 0.7  # Placeholder - would need detailed name checking
        
        # Correctness - proper start/end events
        correctness = 0.0
        has_start = len(findings['events']['start_events']) > 0
        has_end = len(findings['events']['end_events']) > 0
        if has_start and has_end:
            correctness = 1.0
        elif has_start or has_end:
            correctness = 0.5
        
        # Complexity management
        complexity_score = findings['complexity_metrics']['complexity_score']
        complexity_quality = max(0, 1.0 - complexity_score)  # Lower complexity is better
        
        # Best practices
        best_practices = 0.0
        
        # Check for lane usage (organization)
        if findings['lanes']:
            best_practices += 0.25
        
        # Check for proper gateway usage
        if findings['gateways']['total'] > 0:
            # Prefer exclusive gateways over complex ones
            exclusive_ratio = len(findings['gateways']['exclusive']) / findings['gateways']['total']
            best_practices += exclusive_ratio * 0.25
        
        # Check for documentation
        doc_count = sum(1 for p in findings['processes'] if 'documentation' in p)
        if doc_count > 0:
            best_practices += 0.25
        
        # Check for proper event usage
        if findings['events']['total'] >= 2:  # At least start and end
            best_practices += 0.25
        
        return {
            "completeness": completeness,
            "correctness": correctness,
            "complexity_management": complexity_quality,
            "best_practices": min(best_practices, 1.0),
            "overall": (completeness + correctness + complexity_quality + best_practices) / 4
        }
</file>

<file path="handlers/enterprise_config_handler.py">
#!/usr/bin/env python3
"""
Enterprise Configuration Handler

Handles various enterprise XML configuration files including:
- Java EE web.xml (deployment descriptors)
- Tomcat server.xml and context.xml
- JBoss/WildFly configuration files
- WebLogic config.xml
- Generic application server configurations
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class EnterpriseConfigHandler(XMLHandler):
    """Handler for enterprise application server configuration files"""
    
    # Known configuration file patterns
    CONFIG_PATTERNS = {
        'web.xml': {
            'root_elements': ['web-app'],
            'namespaces': ['java.sun.com/xml/ns/javaee', 'java.sun.com/xml/ns/j2ee', 'xmlns.jcp.org/xml/ns/javaee'],
            'type': 'Java EE Deployment Descriptor'
        },
        'server.xml': {
            'root_elements': ['Server'],
            'indicators': ['Connector', 'Engine', 'Host', 'Context'],
            'type': 'Tomcat Server Configuration'
        },
        'context.xml': {
            'root_elements': ['Context'],
            'indicators': ['Resource', 'ResourceLink', 'Valve'],
            'type': 'Tomcat Context Configuration'
        },
        'standalone.xml': {
            'root_elements': ['server'],
            'namespaces': ['urn:jboss:domain'],
            'type': 'JBoss/WildFly Configuration'
        },
        'config.xml': {
            'root_elements': ['domain', 'config'],
            'indicators': ['server', 'machine', 'cluster'],
            'type': 'WebLogic Configuration'
        },
        'applicationContext.xml': {
            'root_elements': ['beans'],
            'namespaces': ['springframework.org/schema/beans'],
            'type': 'Spring Application Context'
        }
    }
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        # Check each known pattern
        for config_name, pattern in self.CONFIG_PATTERNS.items():
            score = 0.0
            
            # Check root element
            if 'root_elements' in pattern and root_tag in pattern['root_elements']:
                score += 0.5
            
            # Check namespaces
            if 'namespaces' in pattern:
                for ns in pattern['namespaces']:
                    if any(ns in uri for uri in namespaces.values()):
                        score += 0.5
                        break
            
            # Check for indicator elements
            if 'indicators' in pattern and score < 1.0:
                indicator_count = 0
                for indicator in pattern['indicators']:
                    if root.find(f'.//{indicator}') is not None:
                        indicator_count += 1
                
                if indicator_count > 0:
                    score += min(indicator_count * 0.2, 0.5)
            
            if score >= 0.5:
                return True, score
        
        # Generic enterprise config detection
        enterprise_indicators = [
            'servlet', 'filter', 'listener', 'datasource', 'connection-pool',
            'security', 'realm', 'valve', 'cluster', 'deployment'
        ]
        
        indicator_count = sum(1 for ind in enterprise_indicators if root.find(f'.//{ind}') is not None)
        if indicator_count >= 2:
            return True, min(indicator_count * 0.2, 0.6)
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        # Determine specific configuration type
        config_type = "Generic Enterprise Configuration"
        version = None
        
        # Check web.xml
        if root_tag == 'web-app':
            config_type = "Java EE Deployment Descriptor"
            version = root.get('version', '3.0')
            
            # Determine Java EE version from namespace
            for uri in namespaces.values():
                if 'javaee' in uri:
                    if 'javaee/7' in uri:
                        version = '3.1'  # Java EE 7
                    elif 'javaee/6' in uri:
                        version = '3.0'  # Java EE 6
                    elif 'j2ee' in uri:
                        version = '2.4'  # J2EE
        
        # Check Tomcat server.xml
        elif root_tag == 'Server':
            config_type = "Tomcat Server Configuration"
            # Try to detect Tomcat version from comments or attributes
            for comment in root.iter(ET.Comment):
                if 'Tomcat' in str(comment):
                    version_match = re.search(r'Tomcat (\d+(?:\.\d+)?)', str(comment))
                    if version_match:
                        version = version_match.group(1)
        
        # Check JBoss/WildFly
        elif 'urn:jboss:domain' in str(namespaces.values()):
            config_type = "JBoss/WildFly Configuration"
            # Extract version from namespace
            for uri in namespaces.values():
                version_match = re.search(r'domain:(\d+\.\d+)', uri)
                if version_match:
                    version = version_match.group(1)
        
        return DocumentTypeInfo(
            type_name=config_type,
            confidence=0.9,
            version=version,
            metadata={
                "category": "enterprise_configuration",
                "root_element": root_tag,
                "file_type": self._guess_file_type(root, namespaces)
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        config_type = self._determine_config_type(root)
        
        # Route to specific analysis based on type
        if 'web.xml' in config_type or 'Deployment Descriptor' in config_type:
            findings = self._analyze_web_xml(root)
        elif 'Tomcat' in config_type and 'Server' in config_type:
            findings = self._analyze_tomcat_server(root)
        elif 'JBoss' in config_type or 'WildFly' in config_type:
            findings = self._analyze_jboss_config(root)
        else:
            findings = self._analyze_generic_config(root)
        
        recommendations = [
            "Review security configurations for vulnerabilities",
            "Check for hardcoded passwords and credentials",
            "Analyze resource pool settings for optimization",
            "Validate against security best practices",
            "Extract for configuration management database",
            "Monitor for configuration drift"
        ]
        
        ai_use_cases = [
            "Security misconfiguration detection",
            "Performance optimization recommendations",
            "Configuration compliance checking",
            "Automated documentation generation",
            "Migration planning assistance",
            "Configuration anomaly detection",
            "Resource optimization analysis",
            "Dependency impact analysis"
        ]
        
        return SpecializedAnalysis(
            document_type=config_type,
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=self._create_inventory(findings),
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_config_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        config_type = self._determine_config_type(root)
        
        base_data = {
            'configuration_type': config_type,
            'security_settings': self._extract_security_settings(root),
            'resource_definitions': self._extract_resources(root),
            'deployment_info': self._extract_deployment_info(root)
        }
        
        # Add type-specific data
        if 'web.xml' in config_type:
            base_data['servlets'] = self._extract_servlets(root)
            base_data['filters'] = self._extract_filters(root)
        elif 'Tomcat' in config_type:
            base_data['connectors'] = self._extract_connectors(root)
            base_data['hosts'] = self._extract_hosts(root)
        
        return base_data
    
    def _analyze_web_xml(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze Java EE web.xml deployment descriptor"""
        findings = {
            'servlets': self._analyze_servlets(root),
            'filters': self._analyze_filters(root),
            'listeners': self._analyze_listeners(root),
            'security': self._analyze_web_security(root),
            'context_params': self._extract_context_params(root),
            'error_pages': self._extract_error_pages(root),
            'welcome_files': self._extract_welcome_files(root),
            'session_config': self._extract_session_config(root)
        }
        
        return findings
    
    def _analyze_tomcat_server(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze Tomcat server.xml configuration"""
        findings = {
            'server_info': self._extract_server_info(root),
            'connectors': self._analyze_connectors(root),
            'engines': self._analyze_engines(root),
            'hosts': self._analyze_hosts(root),
            'valves': self._analyze_valves(root),
            'realms': self._analyze_realms(root),
            'resources': self._analyze_global_resources(root)
        }
        
        return findings
    
    def _analyze_jboss_config(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze JBoss/WildFly configuration"""
        findings = {
            'profiles': self._extract_profiles(root),
            'subsystems': self._analyze_subsystems(root),
            'interfaces': self._extract_interfaces(root),
            'socket_bindings': self._extract_socket_bindings(root),
            'deployments': self._extract_deployments(root),
            'datasources': self._extract_jboss_datasources(root)
        }
        
        return findings
    
    def _analyze_generic_config(self, root: ET.Element) -> Dict[str, Any]:
        """Generic analysis for unknown enterprise configs"""
        findings = {
            'structure': self._analyze_structure(root),
            'security_elements': self._find_security_elements(root),
            'connection_settings': self._find_connection_settings(root),
            'resource_pools': self._find_resource_pools(root),
            'deployment_settings': self._find_deployment_settings(root)
        }
        
        return findings
    
    # Web.xml specific methods
    def _analyze_servlets(self, root: ET.Element) -> List[Dict[str, Any]]:
        servlets = []
        
        # Handle different namespace possibilities
        servlet_tags = root.findall('.//servlet') + \
                      root.findall('.//{http://java.sun.com/xml/ns/javaee}servlet') + \
                      root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}servlet')
        
        for servlet in servlet_tags:
            servlet_info = {
                'name': self._get_child_text(servlet, 'servlet-name'),
                'class': self._get_child_text(servlet, 'servlet-class'),
                'jsp_file': self._get_child_text(servlet, 'jsp-file'),
                'init_params': self._extract_init_params(servlet),
                'load_on_startup': self._get_child_text(servlet, 'load-on-startup')
            }
            
            servlets.append(servlet_info)
        
        # Extract servlet mappings
        mappings = {}
        mapping_tags = root.findall('.//servlet-mapping') + \
                      root.findall('.//{http://java.sun.com/xml/ns/javaee}servlet-mapping') + \
                      root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}servlet-mapping')
        
        for mapping in mapping_tags:
            name = self._get_child_text(mapping, 'servlet-name')
            pattern = self._get_child_text(mapping, 'url-pattern')
            if name and pattern:
                if name not in mappings:
                    mappings[name] = []
                mappings[name].append(pattern)
        
        # Add mappings to servlet info
        for servlet in servlets:
            servlet['url_patterns'] = mappings.get(servlet['name'], [])
        
        return servlets
    
    def _analyze_filters(self, root: ET.Element) -> List[Dict[str, Any]]:
        filters = []
        
        filter_tags = root.findall('.//filter') + \
                     root.findall('.//{http://java.sun.com/xml/ns/javaee}filter') + \
                     root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}filter')
        
        for filter_elem in filter_tags:
            filter_info = {
                'name': self._get_child_text(filter_elem, 'filter-name'),
                'class': self._get_child_text(filter_elem, 'filter-class'),
                'init_params': self._extract_init_params(filter_elem)
            }
            filters.append(filter_info)
        
        # Extract filter mappings
        filter_mappings = {}
        mapping_tags = root.findall('.//filter-mapping') + \
                      root.findall('.//{http://java.sun.com/xml/ns/javaee}filter-mapping') + \
                      root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}filter-mapping')
        
        for mapping in mapping_tags:
            name = self._get_child_text(mapping, 'filter-name')
            pattern = self._get_child_text(mapping, 'url-pattern')
            servlet = self._get_child_text(mapping, 'servlet-name')
            
            if name:
                if name not in filter_mappings:
                    filter_mappings[name] = {'patterns': [], 'servlets': []}
                if pattern:
                    filter_mappings[name]['patterns'].append(pattern)
                if servlet:
                    filter_mappings[name]['servlets'].append(servlet)
        
        # Add mappings to filter info
        for filter_info in filters:
            mappings = filter_mappings.get(filter_info['name'], {})
            filter_info['url_patterns'] = mappings.get('patterns', [])
            filter_info['servlet_names'] = mappings.get('servlets', [])
        
        return filters
    
    def _analyze_listeners(self, root: ET.Element) -> List[str]:
        listeners = []
        
        listener_tags = root.findall('.//listener') + \
                       root.findall('.//{http://java.sun.com/xml/ns/javaee}listener') + \
                       root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}listener')
        
        for listener in listener_tags:
            listener_class = self._get_child_text(listener, 'listener-class')
            if listener_class:
                listeners.append(listener_class)
        
        return listeners
    
    def _analyze_web_security(self, root: ET.Element) -> Dict[str, Any]:
        security = {
            'constraints': [],
            'roles': [],
            'login_config': None
        }
        
        # Security constraints
        constraint_tags = root.findall('.//security-constraint') + \
                         root.findall('.//{http://java.sun.com/xml/ns/javaee}security-constraint') + \
                         root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}security-constraint')
        
        for constraint in constraint_tags:
            constraint_info = {
                'web_resources': [],
                'auth_constraint': None,
                'user_data_constraint': None
            }
            
            # Web resource collections
            for resource in constraint.findall('.//web-resource-collection'):
                resource_info = {
                    'name': self._get_child_text(resource, 'web-resource-name'),
                    'patterns': [p.text for p in resource.findall('.//url-pattern') if p.text],
                    'methods': [m.text for m in resource.findall('.//http-method') if m.text]
                }
                constraint_info['web_resources'].append(resource_info)
            
            # Auth constraint
            auth = constraint.find('.//auth-constraint')
            if auth is not None:
                constraint_info['auth_constraint'] = {
                    'roles': [r.text for r in auth.findall('.//role-name') if r.text]
                }
            
            # User data constraint
            user_data = constraint.find('.//user-data-constraint')
            if user_data is not None:
                transport = user_data.find('.//transport-guarantee')
                if transport is not None:
                    constraint_info['user_data_constraint'] = transport.text
            
            security['constraints'].append(constraint_info)
        
        # Security roles
        role_tags = root.findall('.//security-role') + \
                   root.findall('.//{http://java.sun.com/xml/ns/javaee}security-role') + \
                   root.findall('.//{http://xmlns.jcp.org/xml/ns/javaee}security-role')
        
        for role in role_tags:
            role_name = self._get_child_text(role, 'role-name')
            if role_name:
                security['roles'].append(role_name)
        
        # Login config
        login_config = root.find('.//login-config') or \
                      root.find('.//{http://java.sun.com/xml/ns/javaee}login-config') or \
                      root.find('.//{http://xmlns.jcp.org/xml/ns/javaee}login-config')
        
        if login_config is not None:
            security['login_config'] = {
                'auth_method': self._get_child_text(login_config, 'auth-method'),
                'realm_name': self._get_child_text(login_config, 'realm-name'),
                'form_config': self._extract_form_config(login_config)
            }
        
        return security
    
    # Tomcat specific methods
    def _analyze_connectors(self, root: ET.Element) -> List[Dict[str, Any]]:
        connectors = []
        
        for connector in root.findall('.//Connector'):
            connector_info = {
                'port': connector.get('port'),
                'protocol': connector.get('protocol', 'HTTP/1.1'),
                'secure': connector.get('secure', 'false') == 'true',
                'scheme': connector.get('scheme', 'http'),
                'ssl_enabled': connector.get('SSLEnabled', 'false') == 'true'
            }
            
            # Extract all attributes for detailed analysis
            connector_info['attributes'] = dict(connector.attrib)
            
            connectors.append(connector_info)
        
        return connectors
    
    def _analyze_engines(self, root: ET.Element) -> List[Dict[str, Any]]:
        engines = []
        
        for engine in root.findall('.//Engine'):
            engine_info = {
                'name': engine.get('name'),
                'default_host': engine.get('defaultHost'),
                'hosts': []
            }
            
            # Get nested hosts
            for host in engine.findall('.//Host'):
                host_info = {
                    'name': host.get('name'),
                    'app_base': host.get('appBase'),
                    'unpack_wars': host.get('unpackWARs', 'true') == 'true',
                    'auto_deploy': host.get('autoDeploy', 'true') == 'true'
                }
                engine_info['hosts'].append(host_info)
            
            engines.append(engine_info)
        
        return engines
    
    def _analyze_hosts(self, root: ET.Element) -> List[Dict[str, Any]]:
        hosts = []
        
        for host in root.findall('.//Host'):
            host_info = {
                'name': host.get('name'),
                'app_base': host.get('appBase'),
                'contexts': [],
                'valves': []
            }
            
            # Get contexts
            for context in host.findall('.//Context'):
                context_info = {
                    'path': context.get('path'),
                    'doc_base': context.get('docBase'),
                    'reloadable': context.get('reloadable', 'false') == 'true'
                }
                host_info['contexts'].append(context_info)
            
            # Get valves
            for valve in host.findall('.//Valve'):
                valve_info = {
                    'class_name': valve.get('className'),
                    'attributes': dict(valve.attrib)
                }
                host_info['valves'].append(valve_info)
            
            hosts.append(host_info)
        
        return hosts
    
    def _analyze_valves(self, root: ET.Element) -> List[Dict[str, Any]]:
        valves = []
        
        for valve in root.findall('.//Valve'):
            valve_info = {
                'class_name': valve.get('className'),
                'pattern': valve.get('pattern'),
                'directory': valve.get('directory'),
                'prefix': valve.get('prefix'),
                'suffix': valve.get('suffix')
            }
            
            # Identify valve type
            class_name = valve.get('className', '')
            if 'AccessLogValve' in class_name:
                valve_info['type'] = 'access_log'
            elif 'RemoteIpValve' in class_name:
                valve_info['type'] = 'remote_ip'
            elif 'ErrorReportValve' in class_name:
                valve_info['type'] = 'error_report'
            else:
                valve_info['type'] = 'custom'
            
            valves.append(valve_info)
        
        return valves
    
    def _analyze_realms(self, root: ET.Element) -> List[Dict[str, Any]]:
        realms = []
        
        for realm in root.findall('.//Realm'):
            realm_info = {
                'class_name': realm.get('className'),
                'attributes': dict(realm.attrib)
            }
            
            # Identify realm type
            class_name = realm.get('className', '')
            if 'UserDatabaseRealm' in class_name:
                realm_info['type'] = 'user_database'
            elif 'JDBCRealm' in class_name:
                realm_info['type'] = 'jdbc'
            elif 'DataSourceRealm' in class_name:
                realm_info['type'] = 'datasource'
            elif 'JNDIRealm' in class_name:
                realm_info['type'] = 'jndi'
            else:
                realm_info['type'] = 'custom'
            
            realms.append(realm_info)
        
        return realms
    
    def _analyze_global_resources(self, root: ET.Element) -> List[Dict[str, Any]]:
        resources = []
        
        global_resources = root.find('.//GlobalNamingResources')
        if global_resources is not None:
            for resource in global_resources.findall('.//Resource'):
                resource_info = {
                    'name': resource.get('name'),
                    'auth': resource.get('auth'),
                    'type': resource.get('type'),
                    'attributes': dict(resource.attrib)
                }
                resources.append(resource_info)
        
        return resources
    
    # JBoss/WildFly specific methods
    def _extract_profiles(self, root: ET.Element) -> List[Dict[str, Any]]:
        profiles = []
        
        for profile in root.findall('.//{urn:jboss:domain:*}profile'):
            profile_info = {
                'name': profile.get('name'),
                'subsystems': []
            }
            
            # Count subsystems
            for subsystem in profile.findall('.//{urn:jboss:domain:*}subsystem'):
                profile_info['subsystems'].append(subsystem.tag.split('}')[-1])
            
            profiles.append(profile_info)
        
        return profiles
    
    def _analyze_subsystems(self, root: ET.Element) -> Dict[str, Any]:
        subsystems = {}
        
        # Common subsystems to look for
        subsystem_patterns = [
            'datasources', 'security', 'web', 'ejb3', 'transactions',
            'messaging', 'logging', 'deployment-scanner'
        ]
        
        for pattern in subsystem_patterns:
            elements = root.findall(f'.//*[local-name()="{pattern}"]')
            if elements:
                subsystems[pattern] = len(elements)
        
        return subsystems
    
    def _extract_interfaces(self, root: ET.Element) -> List[Dict[str, str]]:
        interfaces = []
        
        for interface in root.findall('.//*[local-name()="interface"]'):
            interfaces.append({
                'name': interface.get('name'),
                'inet_address': interface.find('.//*[local-name()="inet-address"]').get('value', '')
                if interface.find('.//*[local-name()="inet-address"]') is not None else ''
            })
        
        return interfaces
    
    def _extract_socket_bindings(self, root: ET.Element) -> List[Dict[str, Any]]:
        bindings = []
        
        for binding in root.findall('.//*[local-name()="socket-binding"]'):
            bindings.append({
                'name': binding.get('name'),
                'port': binding.get('port'),
                'interface': binding.get('interface')
            })
        
        return bindings[:10]  # Limit to first 10
    
    def _extract_deployments(self, root: ET.Element) -> List[Dict[str, str]]:
        deployments = []
        
        for deployment in root.findall('.//*[local-name()="deployment"]'):
            deployments.append({
                'name': deployment.get('name'),
                'runtime_name': deployment.get('runtime-name', deployment.get('name'))
            })
        
        return deployments
    
    def _extract_jboss_datasources(self, root: ET.Element) -> List[Dict[str, Any]]:
        datasources = []
        
        for ds in root.findall('.//*[local-name()="datasource"]'):
            datasources.append({
                'jndi_name': ds.get('jndi-name'),
                'pool_name': ds.get('pool-name'),
                'enabled': ds.get('enabled', 'true') == 'true',
                'driver': ds.find('.//*[local-name()="driver"]').text if ds.find('.//*[local-name()="driver"]') is not None else None
            })
        
        return datasources
    
    # Generic analysis methods
    def _analyze_structure(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'root_element': root.tag,
            'total_elements': len(list(root.iter())),
            'max_depth': self._calculate_max_depth(root),
            'unique_tags': len(set(elem.tag for elem in root.iter()))
        }
    
    def _find_security_elements(self, root: ET.Element) -> List[Dict[str, Any]]:
        security_elements = []
        security_keywords = ['security', 'auth', 'role', 'permission', 'credential', 'password', 'realm']
        
        for elem in root.iter():
            tag_lower = elem.tag.lower()
            if any(keyword in tag_lower for keyword in security_keywords):
                security_elements.append({
                    'tag': elem.tag,
                    'attributes': dict(elem.attrib),
                    'has_sensitive_data': self._check_sensitive_data(elem)
                })
        
        return security_elements[:20]  # Limit results
    
    def _find_connection_settings(self, root: ET.Element) -> List[Dict[str, Any]]:
        connections = []
        connection_keywords = ['connection', 'datasource', 'pool', 'jdbc', 'url', 'host', 'port']
        
        for elem in root.iter():
            tag_lower = elem.tag.lower()
            if any(keyword in tag_lower for keyword in connection_keywords):
                connections.append({
                    'tag': elem.tag,
                    'attributes': dict(elem.attrib)
                })
        
        return connections[:15]
    
    def _find_resource_pools(self, root: ET.Element) -> List[Dict[str, Any]]:
        pools = []
        pool_keywords = ['pool', 'max-size', 'min-size', 'timeout']
        
        for elem in root.iter():
            if any(keyword in elem.tag.lower() or keyword in str(elem.attrib).lower() 
                  for keyword in pool_keywords):
                pools.append({
                    'tag': elem.tag,
                    'configuration': dict(elem.attrib)
                })
        
        return pools
    
    def _find_deployment_settings(self, root: ET.Element) -> Dict[str, Any]:
        deployment_info = {
            'contexts': [],
            'applications': [],
            'modules': []
        }
        
        # Look for context paths
        for elem in root.iter():
            if 'context' in elem.tag.lower() and elem.get('path'):
                deployment_info['contexts'].append(elem.get('path'))
            elif 'application' in elem.tag.lower() and elem.get('name'):
                deployment_info['applications'].append(elem.get('name'))
            elif 'module' in elem.tag.lower() and elem.get('name'):
                deployment_info['modules'].append(elem.get('name'))
        
        return deployment_info
    
    # Utility methods
    def _determine_config_type(self, root: ET.Element) -> str:
        """Determine the specific configuration type"""
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if root_tag == 'web-app':
            return "Java EE Web Application Deployment Descriptor"
        elif root_tag == 'Server':
            return "Tomcat Server Configuration"
        elif root_tag == 'Context':
            return "Tomcat Context Configuration"
        elif 'jboss' in root.tag:
            return "JBoss/WildFly Configuration"
        else:
            return "Enterprise Application Configuration"
    
    def _guess_file_type(self, root: ET.Element, namespaces: Dict[str, str]) -> str:
        """Guess the configuration file type"""
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if root_tag == 'web-app':
            return 'web.xml'
        elif root_tag == 'Server':
            return 'server.xml'
        elif root_tag == 'Context':
            return 'context.xml'
        elif 'jboss' in str(namespaces.values()):
            return 'standalone.xml'
        else:
            return 'config.xml'
    
    def _get_child_text(self, parent: ET.Element, child_name: str) -> Optional[str]:
        """Get text content of a child element"""
        child = parent.find(f'.//{child_name}')
        if child is None:
            # Try with common namespaces
            for ns in ['{http://java.sun.com/xml/ns/javaee}', '{http://xmlns.jcp.org/xml/ns/javaee}']:
                child = parent.find(f'.//{ns}{child_name}')
                if child is not None:
                    break
        
        return child.text if child is not None else None
    
    def _extract_init_params(self, parent: ET.Element) -> Dict[str, str]:
        """Extract init parameters"""
        params = {}
        
        for param in parent.findall('.//init-param'):
            name = self._get_child_text(param, 'param-name')
            value = self._get_child_text(param, 'param-value')
            if name and value:
                params[name] = value
        
        return params
    
    def _extract_context_params(self, root: ET.Element) -> Dict[str, str]:
        """Extract context parameters"""
        params = {}
        
        for param in root.findall('.//context-param'):
            name = self._get_child_text(param, 'param-name')
            value = self._get_child_text(param, 'param-value')
            if name and value:
                params[name] = value
        
        return params
    
    def _extract_error_pages(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract error page mappings"""
        error_pages = []
        
        for page in root.findall('.//error-page'):
            page_info = {}
            
            error_code = self._get_child_text(page, 'error-code')
            exception_type = self._get_child_text(page, 'exception-type')
            location = self._get_child_text(page, 'location')
            
            if error_code:
                page_info['error_code'] = error_code
            if exception_type:
                page_info['exception_type'] = exception_type
            if location:
                page_info['location'] = location
            
            if page_info:
                error_pages.append(page_info)
        
        return error_pages
    
    def _extract_welcome_files(self, root: ET.Element) -> List[str]:
        """Extract welcome file list"""
        welcome_files = []
        
        welcome_list = root.find('.//welcome-file-list')
        if welcome_list is not None:
            for file_elem in welcome_list.findall('.//welcome-file'):
                if file_elem.text:
                    welcome_files.append(file_elem.text)
        
        return welcome_files
    
    def _extract_session_config(self, root: ET.Element) -> Dict[str, Any]:
        """Extract session configuration"""
        session_config = {}
        
        config = root.find('.//session-config')
        if config is not None:
            timeout = self._get_child_text(config, 'session-timeout')
            if timeout:
                session_config['timeout_minutes'] = timeout
            
            # Cookie config
            cookie_config = config.find('.//cookie-config')
            if cookie_config is not None:
                session_config['cookie'] = {
                    'name': self._get_child_text(cookie_config, 'name'),
                    'domain': self._get_child_text(cookie_config, 'domain'),
                    'path': self._get_child_text(cookie_config, 'path'),
                    'secure': self._get_child_text(cookie_config, 'secure') == 'true',
                    'http_only': self._get_child_text(cookie_config, 'http-only') == 'true'
                }
        
        return session_config
    
    def _extract_form_config(self, login_config: ET.Element) -> Optional[Dict[str, str]]:
        """Extract form login configuration"""
        form_config = login_config.find('.//form-login-config')
        if form_config is not None:
            return {
                'login_page': self._get_child_text(form_config, 'form-login-page'),
                'error_page': self._get_child_text(form_config, 'form-error-page')
            }
        return None
    
    def _extract_server_info(self, root: ET.Element) -> Dict[str, Any]:
        """Extract server-level information"""
        server = root
        
        return {
            'port': server.get('port', '8005'),
            'shutdown': server.get('shutdown', 'SHUTDOWN'),
            'services': len(server.findall('.//Service'))
        }
    
    def _extract_security_settings(self, root: ET.Element) -> Dict[str, Any]:
        """Extract security-related settings"""
        return {
            'security_constraints': len(root.findall('.//security-constraint')),
            'security_roles': [r.text for r in root.findall('.//role-name') if r.text],
            'auth_methods': list(set(m.text for m in root.findall('.//auth-method') if m.text)),
            'ssl_enabled': any(c.get('SSLEnabled') == 'true' for c in root.findall('.//Connector'))
        }
    
    def _extract_resources(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract resource definitions"""
        resources = []
        
        # Standard resources
        for resource in root.findall('.//Resource'):
            resources.append({
                'name': resource.get('name'),
                'type': resource.get('type'),
                'auth': resource.get('auth')
            })
        
        # DataSources
        for ds in root.findall('.//DataSource'):
            resources.append({
                'name': ds.get('name'),
                'type': 'DataSource',
                'jndi_name': ds.get('jndiName')
            })
        
        return resources[:20]  # Limit
    
    def _extract_deployment_info(self, root: ET.Element) -> Dict[str, Any]:
        """Extract deployment-related information"""
        return {
            'contexts': [c.get('path') for c in root.findall('.//Context') if c.get('path')],
            'web_apps': [w.get('docBase') for w in root.findall('.//Context') if w.get('docBase')],
            'auto_deploy': any(h.get('autoDeploy') == 'true' for h in root.findall('.//Host'))
        }
    
    def _extract_servlets(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract servlet definitions"""
        servlets = []
        
        for servlet in root.findall('.//servlet'):
            servlets.append({
                'name': self._get_child_text(servlet, 'servlet-name'),
                'class': self._get_child_text(servlet, 'servlet-class'),
                'load_on_startup': self._get_child_text(servlet, 'load-on-startup')
            })
        
        return servlets[:20]
    
    def _extract_filters(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract filter definitions"""
        filters = []
        
        for filter_elem in root.findall('.//filter'):
            filters.append({
                'name': self._get_child_text(filter_elem, 'filter-name'),
                'class': self._get_child_text(filter_elem, 'filter-class')
            })
        
        return filters[:20]
    
    def _extract_connectors(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract connector configurations"""
        connectors = []
        
        for connector in root.findall('.//Connector'):
            connectors.append({
                'port': connector.get('port'),
                'protocol': connector.get('protocol'),
                'secure': connector.get('secure') == 'true',
                'ssl_enabled': connector.get('SSLEnabled') == 'true'
            })
        
        return connectors
    
    def _extract_hosts(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract host configurations"""
        hosts = []
        
        for host in root.findall('.//Host'):
            hosts.append({
                'name': host.get('name'),
                'app_base': host.get('appBase'),
                'auto_deploy': host.get('autoDeploy')
            })
        
        return hosts
    
    def _calculate_max_depth(self, elem: ET.Element, depth: int = 0) -> int:
        """Calculate maximum depth of XML tree"""
        if not list(elem):
            return depth
        return max(self._calculate_max_depth(child, depth + 1) for child in elem)
    
    def _check_sensitive_data(self, elem: ET.Element) -> bool:
        """Check if element might contain sensitive data"""
        sensitive_keywords = ['password', 'secret', 'key', 'token', 'credential']
        
        # Check tag name
        if any(keyword in elem.tag.lower() for keyword in sensitive_keywords):
            return True
        
        # Check attributes
        for attr_name, attr_value in elem.attrib.items():
            if any(keyword in attr_name.lower() for keyword in sensitive_keywords):
                return True
            # Check for non-empty password values
            if 'password' in attr_name.lower() and attr_value and attr_value != '*':
                return True
        
        return False
    
    def _create_inventory(self, findings: Dict[str, Any]) -> Dict[str, int]:
        """Create data inventory from findings"""
        inventory = {}
        
        # Count various elements based on findings structure
        if 'servlets' in findings:
            inventory['servlets'] = len(findings['servlets'])
        if 'filters' in findings:
            inventory['filters'] = len(findings['filters'])
        if 'connectors' in findings:
            inventory['connectors'] = len(findings['connectors'])
        if 'datasources' in findings:
            inventory['datasources'] = len(findings['datasources'])
        if 'security' in findings and 'constraints' in findings['security']:
            inventory['security_constraints'] = len(findings['security']['constraints'])
        
        return inventory
    
    def _assess_config_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess configuration quality"""
        scores = {}
        
        # Security score
        security_score = 0.0
        if 'security' in findings:
            if findings['security'].get('constraints'):
                security_score += 0.3
            if findings['security'].get('login_config'):
                security_score += 0.3
            if findings['security'].get('roles'):
                security_score += 0.2
            # Check for HTTPS
            if 'connectors' in findings:
                if any(c.get('secure') or c.get('ssl_enabled') for c in findings['connectors']):
                    security_score += 0.2
        scores['security'] = min(security_score, 1.0)
        
        # Configuration completeness
        completeness = 0.0
        expected_elements = ['servlets', 'security', 'session_config', 'error_pages']
        for element in expected_elements:
            if element in findings and findings[element]:
                completeness += 0.25
        scores['completeness'] = completeness
        
        # Best practices
        best_practices = 0.0
        
        # Check session timeout
        if 'session_config' in findings and findings['session_config'].get('timeout_minutes'):
            best_practices += 0.25
        
        # Check error handling
        if 'error_pages' in findings and len(findings['error_pages']) > 0:
            best_practices += 0.25
        
        # Check for secure cookies
        if 'session_config' in findings and 'cookie' in findings['session_config']:
            cookie = findings['session_config']['cookie']
            if cookie.get('secure') and cookie.get('http_only'):
                best_practices += 0.5
        
        scores['best_practices'] = best_practices
        
        # Overall quality
        scores['overall'] = sum(scores.values()) / len(scores)
        
        return scores
</file>

<file path="handlers/openapi_xml_handler.py">
#!/usr/bin/env python3
"""
OpenAPI XML Handler

Handles OpenAPI/Swagger specifications in XML format.
While JSON is more common, XML representations exist and are used
in some enterprise environments.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class OpenAPIXMLHandler(XMLHandler):
    """Handler for OpenAPI/Swagger specifications in XML format"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        # Check for OpenAPI root elements
        if root_tag in ['openapi', 'swagger']:
            return True, 1.0
        
        # Check for OpenAPI/Swagger indicators
        indicators = ['paths', 'components', 'info', 'servers', 'definitions']
        found_indicators = 0
        
        for indicator in indicators:
            if root.find(f'.//{indicator}') is not None:
                found_indicators += 1
        
        if found_indicators >= 2:
            return True, min(found_indicators * 0.3, 0.9)
        
        # Check for Swagger namespaces
        if any('swagger.io' in uri for uri in namespaces.values()):
            return True, 0.8
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Determine version
        version = "3.0.0"  # Default to OpenAPI 3.0
        
        # Check for version attribute or element
        if root.get('version'):
            version = root.get('version')
        elif root.find('.//openapi') is not None:
            version_elem = root.find('.//openapi')
            if version_elem.text:
                version = version_elem.text
        elif root.find('.//swagger') is not None:
            version_elem = root.find('.//swagger')
            if version_elem.text:
                version = version_elem.text
        
        # Determine if it's Swagger 2.0 or OpenAPI 3.x
        api_type = "OpenAPI" if version.startswith('3') else "Swagger"
        
        return DocumentTypeInfo(
            type_name=f"{api_type} Specification",
            confidence=0.9,
            version=version,
            metadata={
                "standard": api_type,
                "category": "api_specification",
                "format": "XML"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        # Determine version for proper parsing
        version = self._determine_version(root)
        is_openapi3 = version.startswith('3')
        
        findings = {
            'api_info': self._extract_api_info(root),
            'servers': self._extract_servers(root) if is_openapi3 else self._extract_host_info(root),
            'paths': self._analyze_paths(root),
            'operations': self._analyze_operations(root),
            'schemas': self._analyze_schemas(root, is_openapi3),
            'security': self._analyze_security(root),
            'tags': self._extract_tags(root),
            'external_docs': self._extract_external_docs(root)
        }
        
        recommendations = [
            "Generate API client libraries",
            "Create interactive API documentation",
            "Validate against OpenAPI specification",
            "Extract for API gateway configuration",
            "Monitor for breaking changes",
            "Generate API test suites"
        ]
        
        ai_use_cases = [
            "API documentation generation",
            "Client SDK generation",
            "API test case generation",
            "Breaking change detection",
            "API security analysis",
            "Usage pattern analysis",
            "API versioning recommendations",
            "Performance optimization suggestions"
        ]
        
        return SpecializedAnalysis(
            document_type=f"{findings['api_info'].get('title', 'API')} Specification",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'paths': len(findings['paths']),
                'operations': findings['operations']['total'],
                'schemas': len(findings['schemas']),
                'security_schemes': len(findings['security'].get('schemes', []))
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_api_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        version = self._determine_version(root)
        is_openapi3 = version.startswith('3')
        
        return {
            'api_metadata': self._extract_api_info(root),
            'endpoints': self._extract_all_endpoints(root),
            'request_schemas': self._extract_request_schemas(root, is_openapi3),
            'response_schemas': self._extract_response_schemas(root, is_openapi3),
            'authentication': self._extract_auth_methods(root)
        }
    
    def _determine_version(self, root: ET.Element) -> str:
        """Determine OpenAPI/Swagger version"""
        # Check for version in various places
        if root.get('version'):
            return root.get('version')
        
        openapi_elem = root.find('.//openapi')
        if openapi_elem is not None and openapi_elem.text:
            return openapi_elem.text
        
        swagger_elem = root.find('.//swagger')
        if swagger_elem is not None and swagger_elem.text:
            return swagger_elem.text
        
        # Default based on structure
        if root.find('.//components') is not None:
            return "3.0.0"
        elif root.find('.//definitions') is not None:
            return "2.0"
        
        return "3.0.0"
    
    def _extract_api_info(self, root: ET.Element) -> Dict[str, Any]:
        """Extract API information"""
        info = root.find('.//info')
        if info is None:
            return {}
        
        api_info = {
            'title': self._get_child_text(info, 'title'),
            'version': self._get_child_text(info, 'version'),
            'description': self._get_child_text(info, 'description'),
            'terms_of_service': self._get_child_text(info, 'termsOfService')
        }
        
        # Contact info
        contact = info.find('.//contact')
        if contact is not None:
            api_info['contact'] = {
                'name': self._get_child_text(contact, 'name'),
                'email': self._get_child_text(contact, 'email'),
                'url': self._get_child_text(contact, 'url')
            }
        
        # License info
        license_elem = info.find('.//license')
        if license_elem is not None:
            api_info['license'] = {
                'name': self._get_child_text(license_elem, 'name'),
                'url': self._get_child_text(license_elem, 'url')
            }
        
        return api_info
    
    def _extract_servers(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract server information (OpenAPI 3.x)"""
        servers = []
        
        for server in root.findall('.//servers/server'):
            server_info = {
                'url': self._get_child_text(server, 'url'),
                'description': self._get_child_text(server, 'description'),
                'variables': {}
            }
            
            # Extract server variables
            variables = server.find('.//variables')
            if variables is not None:
                for var in variables:
                    var_name = var.tag
                    server_info['variables'][var_name] = {
                        'default': self._get_child_text(var, 'default'),
                        'description': self._get_child_text(var, 'description'),
                        'enum': [e.text for e in var.findall('.//enum') if e.text]
                    }
            
            servers.append(server_info)
        
        return servers
    
    def _extract_host_info(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract host information (Swagger 2.0)"""
        host = self._get_child_text(root, 'host')
        base_path = self._get_child_text(root, 'basePath', '/')
        schemes = [s.text for s in root.findall('.//schemes/scheme') if s.text]
        
        if not host:
            return []
        
        servers = []
        if schemes:
            for scheme in schemes:
                servers.append({
                    'url': f"{scheme}://{host}{base_path}",
                    'description': f"{scheme.upper()} endpoint"
                })
        else:
            servers.append({
                'url': f"https://{host}{base_path}",
                'description': "Default HTTPS endpoint"
            })
        
        return servers
    
    def _analyze_paths(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Analyze API paths"""
        paths_elem = root.find('.//paths')
        if paths_elem is None:
            return []
        
        paths = []
        
        for path_elem in paths_elem:
            if path_elem.tag.startswith('{'):  # Skip namespace elements
                continue
                
            path = path_elem.tag
            path_info = {
                'path': path,
                'operations': [],
                'parameters': []
            }
            
            # Extract operations
            http_methods = ['get', 'post', 'put', 'delete', 'patch', 'options', 'head']
            for method in http_methods:
                op_elem = path_elem.find(f'.//{method}')
                if op_elem is not None:
                    path_info['operations'].append({
                        'method': method.upper(),
                        'operation_id': self._get_child_text(op_elem, 'operationId'),
                        'summary': self._get_child_text(op_elem, 'summary'),
                        'tags': [t.text for t in op_elem.findall('.//tags/tag') if t.text]
                    })
            
            # Extract path parameters
            for param in path_elem.findall('.//parameters/parameter'):
                if self._get_child_text(param, 'in') == 'path':
                    path_info['parameters'].append({
                        'name': self._get_child_text(param, 'name'),
                        'type': self._get_child_text(param, 'type'),
                        'required': self._get_child_text(param, 'required') == 'true'
                    })
            
            paths.append(path_info)
        
        return paths
    
    def _analyze_operations(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze all operations"""
        operations = {
            'total': 0,
            'by_method': {},
            'by_tag': {},
            'deprecated': []
        }
        
        paths_elem = root.find('.//paths')
        if paths_elem is None:
            return operations
        
        http_methods = ['get', 'post', 'put', 'delete', 'patch', 'options', 'head']
        
        for path_elem in paths_elem:
            if path_elem.tag.startswith('{'):
                continue
                
            path = path_elem.tag
            
            for method in http_methods:
                op_elem = path_elem.find(f'.//{method}')
                if op_elem is not None:
                    operations['total'] += 1
                    
                    # Count by method
                    operations['by_method'][method.upper()] = operations['by_method'].get(method.upper(), 0) + 1
                    
                    # Count by tag
                    tags = [t.text for t in op_elem.findall('.//tags/tag') if t.text]
                    for tag in tags:
                        operations['by_tag'][tag] = operations['by_tag'].get(tag, 0) + 1
                    
                    # Check if deprecated
                    if self._get_child_text(op_elem, 'deprecated') == 'true':
                        operations['deprecated'].append({
                            'path': path,
                            'method': method.upper(),
                            'operation_id': self._get_child_text(op_elem, 'operationId')
                        })
        
        return operations
    
    def _analyze_schemas(self, root: ET.Element, is_openapi3: bool) -> List[Dict[str, Any]]:
        """Analyze data schemas"""
        schemas = []
        
        if is_openapi3:
            # OpenAPI 3.x schemas in components
            schemas_elem = root.find('.//components/schemas')
            if schemas_elem is not None:
                for schema_elem in schemas_elem:
                    if schema_elem.tag.startswith('{'):
                        continue
                    
                    schemas.append(self._extract_schema_info(schema_elem.tag, schema_elem))
        else:
            # Swagger 2.0 definitions
            defs_elem = root.find('.//definitions')
            if defs_elem is not None:
                for def_elem in defs_elem:
                    if def_elem.tag.startswith('{'):
                        continue
                    
                    schemas.append(self._extract_schema_info(def_elem.tag, def_elem))
        
        return schemas
    
    def _extract_schema_info(self, name: str, schema_elem: ET.Element) -> Dict[str, Any]:
        """Extract schema information"""
        schema_info = {
            'name': name,
            'type': self._get_child_text(schema_elem, 'type', 'object'),
            'properties': [],
            'required': []
        }
        
        # Extract properties
        props_elem = schema_elem.find('.//properties')
        if props_elem is not None:
            for prop_elem in props_elem:
                if prop_elem.tag.startswith('{'):
                    continue
                
                prop_info = {
                    'name': prop_elem.tag,
                    'type': self._get_child_text(prop_elem, 'type'),
                    'format': self._get_child_text(prop_elem, 'format'),
                    'description': self._get_child_text(prop_elem, 'description')
                }
                schema_info['properties'].append(prop_info)
        
        # Extract required fields
        required_elem = schema_elem.find('.//required')
        if required_elem is not None:
            schema_info['required'] = [r.text for r in required_elem if r.text]
        
        # Check for inheritance
        all_of = schema_elem.find('.//allOf')
        if all_of is not None:
            schema_info['inheritance'] = 'allOf'
        
        return schema_info
    
    def _analyze_security(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze security definitions"""
        security = {
            'schemes': [],
            'requirements': []
        }
        
        # OpenAPI 3.x security schemes
        sec_schemes = root.find('.//components/securitySchemes')
        if sec_schemes is not None:
            for scheme_elem in sec_schemes:
                if scheme_elem.tag.startswith('{'):
                    continue
                
                scheme_info = {
                    'name': scheme_elem.tag,
                    'type': self._get_child_text(scheme_elem, 'type'),
                    'scheme': self._get_child_text(scheme_elem, 'scheme'),
                    'bearer_format': self._get_child_text(scheme_elem, 'bearerFormat'),
                    'flows': self._extract_oauth_flows(scheme_elem)
                }
                security['schemes'].append(scheme_info)
        else:
            # Swagger 2.0 security definitions
            sec_defs = root.find('.//securityDefinitions')
            if sec_defs is not None:
                for sec_def in sec_defs:
                    if sec_def.tag.startswith('{'):
                        continue
                    
                    scheme_info = {
                        'name': sec_def.tag,
                        'type': self._get_child_text(sec_def, 'type'),
                        'in': self._get_child_text(sec_def, 'in'),
                        'name_param': self._get_child_text(sec_def, 'name'),
                        'flow': self._get_child_text(sec_def, 'flow'),
                        'scopes': self._extract_scopes(sec_def)
                    }
                    security['schemes'].append(scheme_info)
        
        # Global security requirements
        for sec_req in root.findall('.//security'):
            req_info = {}
            for child in sec_req:
                if not child.tag.startswith('{'):
                    req_info[child.tag] = [s.text for s in child if s.text]
            if req_info:
                security['requirements'].append(req_info)
        
        return security
    
    def _extract_oauth_flows(self, scheme_elem: ET.Element) -> Dict[str, Any]:
        """Extract OAuth flows (OpenAPI 3.x)"""
        flows = {}
        flows_elem = scheme_elem.find('.//flows')
        
        if flows_elem is not None:
            for flow_type in ['implicit', 'password', 'clientCredentials', 'authorizationCode']:
                flow_elem = flows_elem.find(f'.//{flow_type}')
                if flow_elem is not None:
                    flows[flow_type] = {
                        'authorization_url': self._get_child_text(flow_elem, 'authorizationUrl'),
                        'token_url': self._get_child_text(flow_elem, 'tokenUrl'),
                        'refresh_url': self._get_child_text(flow_elem, 'refreshUrl'),
                        'scopes': self._extract_scopes(flow_elem)
                    }
        
        return flows
    
    def _extract_scopes(self, parent_elem: ET.Element) -> Dict[str, str]:
        """Extract OAuth scopes"""
        scopes = {}
        scopes_elem = parent_elem.find('.//scopes')
        
        if scopes_elem is not None:
            for scope_elem in scopes_elem:
                if not scope_elem.tag.startswith('{'):
                    scopes[scope_elem.tag] = scope_elem.text or ''
        
        return scopes
    
    def _extract_tags(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract tag definitions"""
        tags = []
        
        for tag_elem in root.findall('.//tags/tag'):
            tag_info = {
                'name': self._get_child_text(tag_elem, 'name'),
                'description': self._get_child_text(tag_elem, 'description')
            }
            
            # External docs for tag
            ext_docs = tag_elem.find('.//externalDocs')
            if ext_docs is not None:
                tag_info['external_docs'] = {
                    'description': self._get_child_text(ext_docs, 'description'),
                    'url': self._get_child_text(ext_docs, 'url')
                }
            
            tags.append(tag_info)
        
        return tags
    
    def _extract_external_docs(self, root: ET.Element) -> Optional[Dict[str, str]]:
        """Extract external documentation"""
        ext_docs = root.find('.//externalDocs')
        if ext_docs is not None:
            return {
                'description': self._get_child_text(ext_docs, 'description'),
                'url': self._get_child_text(ext_docs, 'url')
            }
        return None
    
    def _extract_all_endpoints(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract all API endpoints"""
        endpoints = []
        paths_elem = root.find('.//paths')
        
        if paths_elem is None:
            return endpoints
        
        http_methods = ['get', 'post', 'put', 'delete', 'patch', 'options', 'head']
        
        for path_elem in paths_elem:
            if path_elem.tag.startswith('{'):
                continue
                
            path = path_elem.tag
            
            for method in http_methods:
                op_elem = path_elem.find(f'.//{method}')
                if op_elem is not None:
                    endpoints.append({
                        'path': path,
                        'method': method.upper(),
                        'operation_id': self._get_child_text(op_elem, 'operationId'),
                        'summary': self._get_child_text(op_elem, 'summary'),
                        'deprecated': self._get_child_text(op_elem, 'deprecated') == 'true'
                    })
        
        return endpoints
    
    def _extract_request_schemas(self, root: ET.Element, is_openapi3: bool) -> List[Dict[str, Any]]:
        """Extract request body schemas"""
        request_schemas = []
        paths_elem = root.find('.//paths')
        
        if paths_elem is None:
            return request_schemas
        
        for path_elem in paths_elem:
            if path_elem.tag.startswith('{'):
                continue
                
            path = path_elem.tag
            
            for method in ['post', 'put', 'patch']:
                op_elem = path_elem.find(f'.//{method}')
                if op_elem is not None:
                    if is_openapi3:
                        # OpenAPI 3.x request body
                        req_body = op_elem.find('.//requestBody')
                        if req_body is not None:
                            content = req_body.find('.//content')
                            if content is not None:
                                for media_type in content:
                                    if not media_type.tag.startswith('{'):
                                        schema_ref = media_type.find('.//schema/$ref')
                                        if schema_ref is not None:
                                            request_schemas.append({
                                                'path': path,
                                                'method': method.upper(),
                                                'media_type': media_type.tag,
                                                'schema_ref': schema_ref.text
                                            })
                    else:
                        # Swagger 2.0 parameters
                        for param in op_elem.findall('.//parameters/parameter'):
                            if self._get_child_text(param, 'in') == 'body':
                                schema_elem = param.find('.//schema')
                                if schema_elem is not None:
                                    request_schemas.append({
                                        'path': path,
                                        'method': method.upper(),
                                        'name': self._get_child_text(param, 'name'),
                                        'required': self._get_child_text(param, 'required') == 'true'
                                    })
        
        return request_schemas[:20]  # Limit
    
    def _extract_response_schemas(self, root: ET.Element, is_openapi3: bool) -> List[Dict[str, Any]]:
        """Extract response schemas"""
        response_schemas = []
        paths_elem = root.find('.//paths')
        
        if paths_elem is None:
            return response_schemas
        
        for path_elem in paths_elem:
            if path_elem.tag.startswith('{'):
                continue
                
            path = path_elem.tag
            
            for op_elem in path_elem:
                if op_elem.tag.startswith('{') or op_elem.tag not in ['get', 'post', 'put', 'delete', 'patch']:
                    continue
                
                method = op_elem.tag
                responses = op_elem.find('.//responses')
                
                if responses is not None:
                    for response in responses:
                        if not response.tag.startswith('{'):
                            status_code = response.tag
                            
                            if is_openapi3:
                                content = response.find('.//content')
                                if content is not None:
                                    for media_type in content:
                                        if not media_type.tag.startswith('{'):
                                            response_schemas.append({
                                                'path': path,
                                                'method': method.upper(),
                                                'status_code': status_code,
                                                'media_type': media_type.tag
                                            })
                            else:
                                schema = response.find('.//schema')
                                if schema is not None:
                                    response_schemas.append({
                                        'path': path,
                                        'method': method.upper(),
                                        'status_code': status_code,
                                        'description': self._get_child_text(response, 'description')
                                    })
        
        return response_schemas[:20]  # Limit
    
    def _extract_auth_methods(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract authentication methods"""
        auth_methods = []
        
        # From security schemes
        security = self._analyze_security(root)
        for scheme in security['schemes']:
            auth_methods.append({
                'name': scheme['name'],
                'type': scheme['type'],
                'description': f"{scheme['type']} authentication"
            })
        
        return auth_methods
    
    def _get_child_text(self, parent: ET.Element, child_name: str, default: str = None) -> Optional[str]:
        """Get text content of a child element"""
        child = parent.find(f'.//{child_name}')
        return child.text if child is not None else default
    
    def _assess_api_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess API specification quality"""
        # Documentation completeness
        total_ops = findings['operations']['total']
        doc_score = 0.0
        
        if total_ops > 0:
            # Check if operations have summaries (would need more detailed analysis)
            doc_score = 0.7  # Placeholder
        
        # Security implementation
        security_score = 0.0
        if findings['security']['schemes']:
            security_score = min(len(findings['security']['schemes']) * 0.3, 1.0)
        
        # API organization (tags)
        org_score = 0.0
        if findings['tags']:
            org_score = min(len(findings['tags']) * 0.2, 1.0)
        
        # Deprecation management
        deprecation_score = 1.0
        if findings['operations']['deprecated']:
            deprecation_ratio = len(findings['operations']['deprecated']) / max(total_ops, 1)
            deprecation_score = max(0, 1.0 - deprecation_ratio * 2)  # Penalize heavily
        
        # Schema coverage
        schema_score = min(len(findings['schemas']) / 10, 1.0)  # Assume 10+ schemas is good
        
        return {
            "documentation": doc_score,
            "security": security_score,
            "organization": org_score,
            "deprecation_management": deprecation_score,
            "schema_coverage": schema_score,
            "overall": (doc_score + security_score + org_score + deprecation_score + schema_score) / 5
        }
</file>

<file path="handlers/properties_xml_handler.py">
#!/usr/bin/env python3
"""
Properties XML Handler

Handles Java Properties files in XML format.
These are commonly used for configuration in Java applications
and provide a structured alternative to traditional .properties files.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os
from datetime import datetime

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class PropertiesXMLHandler(XMLHandler):
    """Handler for Java Properties XML files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Java Properties XML files have specific DTD
        if root.tag == 'properties' or root.tag.endswith('}properties'):
            # Check for properties-specific structure
            if root.find('.//entry') is not None:
                return True, 1.0
            # Even without entries, if it has the comment element typical of properties
            if root.find('.//comment') is not None:
                return True, 0.8
            # Basic properties root
            return True, 0.6
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Properties XML files typically reference a specific DTD
        dtd_version = "1.0"  # Standard version
        
        # Check for comment that might indicate version or purpose
        comment = root.find('.//comment')
        comment_text = comment.text if comment is not None else None
        
        return DocumentTypeInfo(
            type_name="Java Properties XML",
            confidence=1.0,
            version=dtd_version,
            schema_uri="http://java.sun.com/dtd/properties.dtd",
            metadata={
                "standard": "Java Properties",
                "category": "configuration",
                "format": "XML",
                "comment": comment_text
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'properties': self._extract_all_properties(root),
            'property_groups': self._group_properties_by_prefix(root),
            'environment_configs': self._detect_environment_configs(root),
            'security_sensitive': self._find_sensitive_properties(root),
            'placeholders': self._find_placeholders(root),
            'duplicates': self._find_duplicate_keys(root),
            'statistics': self._calculate_statistics(root)
        }
        
        recommendations = [
            "Review for hardcoded sensitive values",
            "Check for environment-specific configurations",
            "Validate property naming conventions",
            "Extract for configuration management",
            "Monitor for configuration drift",
            "Consider encrypting sensitive properties"
        ]
        
        ai_use_cases = [
            "Configuration validation",
            "Security scanning for exposed credentials",
            "Environment configuration comparison",
            "Property dependency analysis",
            "Configuration migration assistance",
            "Default value recommendations",
            "Configuration documentation generation",
            "Property usage analysis"
        ]
        
        return SpecializedAnalysis(
            document_type="Java Properties Configuration",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'total_properties': len(findings['properties']),
                'property_groups': len(findings['property_groups']),
                'sensitive_properties': len(findings['security_sensitive']),
                'placeholders': len(findings['placeholders'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_property_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'all_properties': self._extract_properties_dict(root),
            'grouped_properties': self._extract_grouped_properties(root),
            'configuration_metadata': self._extract_metadata(root),
            'property_patterns': self._analyze_property_patterns(root)
        }
    
    def _extract_all_properties(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract all properties with their details"""
        properties = []
        
        for entry in root.findall('.//entry'):
            key = entry.get('key')
            if key:
                prop_info = {
                    'key': key,
                    'value': entry.text or '',
                    'type': self._infer_property_type(entry.text),
                    'category': self._categorize_property(key),
                    'is_empty': not entry.text or not entry.text.strip()
                }
                
                # Check for special patterns
                if prop_info['value']:
                    prop_info['has_placeholder'] = '${' in prop_info['value'] or '#{' in prop_info['value']
                    prop_info['is_reference'] = prop_info['value'].startswith('@') or prop_info['value'].startswith('$')
                else:
                    prop_info['has_placeholder'] = False
                    prop_info['is_reference'] = False
                
                properties.append(prop_info)
        
        # Sort by key for consistency
        properties.sort(key=lambda x: x['key'])
        
        return properties
    
    def _group_properties_by_prefix(self, root: ET.Element) -> Dict[str, List[Dict[str, str]]]:
        """Group properties by their prefix (e.g., 'database.', 'server.', etc.)"""
        groups = {}
        
        for entry in root.findall('.//entry'):
            key = entry.get('key')
            if key and '.' in key:
                prefix = key.split('.')[0]
                if prefix not in groups:
                    groups[prefix] = []
                
                groups[prefix].append({
                    'key': key,
                    'value': entry.text or '',
                    'suffix': '.'.join(key.split('.')[1:])
                })
        
        return groups
    
    def _detect_environment_configs(self, root: ET.Element) -> Dict[str, List[str]]:
        """Detect environment-specific configurations"""
        env_configs = {
            'development': [],
            'testing': [],
            'staging': [],
            'production': []
        }
        
        env_patterns = {
            'development': ['dev', 'development', 'local', 'debug'],
            'testing': ['test', 'testing', 'qa', 'quality'],
            'staging': ['stage', 'staging', 'uat', 'preprod'],
            'production': ['prod', 'production', 'live', 'release']
        }
        
        for entry in root.findall('.//entry'):
            key = entry.get('key', '').lower()
            value = (entry.text or '').lower()
            
            for env, patterns in env_patterns.items():
                if any(pattern in key or pattern in value for pattern in patterns):
                    env_configs[env].append(entry.get('key'))
        
        # Remove duplicates
        for env in env_configs:
            env_configs[env] = list(set(env_configs[env]))
        
        return env_configs
    
    def _find_sensitive_properties(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Find potentially sensitive properties"""
        sensitive_properties = []
        
        sensitive_patterns = [
            'password', 'passwd', 'pwd', 'secret', 'key', 'token',
            'credential', 'auth', 'private', 'certificate', 'cert',
            'api_key', 'apikey', 'access_key', 'encryption'
        ]
        
        for entry in root.findall('.//entry'):
            key = entry.get('key', '').lower()
            value = entry.text or ''
            
            # Check if key contains sensitive patterns
            is_sensitive = any(pattern in key for pattern in sensitive_patterns)
            
            if is_sensitive:
                sensitive_info = {
                    'key': entry.get('key'),
                    'value_length': len(value),
                    'is_encrypted': self._looks_encrypted(value),
                    'is_empty': not value,
                    'is_placeholder': '${' in value or value.startswith('ENC('),
                    'risk_level': self._assess_risk_level(key, value)
                }
                
                # Don't include actual value for security
                if not sensitive_info['is_empty'] and not sensitive_info['is_placeholder']:
                    sensitive_info['value_preview'] = value[:3] + '***' if len(value) > 3 else '***'
                
                sensitive_properties.append(sensitive_info)
        
        return sensitive_properties
    
    def _find_placeholders(self, root: ET.Element) -> List[Dict[str, str]]:
        """Find properties with placeholder values"""
        placeholders = []
        
        placeholder_patterns = [
            (r'\$\{([^}]+)\}', 'maven_style'),  # ${property}
            (r'#\{([^}]+)\}', 'spring_el'),     # #{expression}
            (r'@([^@]+)@', 'ant_style'),        # @property@
            (r'%\(([^)]+)\)s?', 'python_style') # %(property)s
        ]
        
        for entry in root.findall('.//entry'):
            key = entry.get('key')
            value = entry.text or ''
            
            for pattern, style in placeholder_patterns:
                matches = re.findall(pattern, value)
                if matches:
                    placeholders.append({
                        'key': key,
                        'value': value,
                        'placeholders': matches,
                        'style': style
                    })
                    break
        
        return placeholders
    
    def _find_duplicate_keys(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Find duplicate property keys"""
        key_occurrences = {}
        duplicates = []
        
        for i, entry in enumerate(root.findall('.//entry')):
            key = entry.get('key')
            if key:
                if key not in key_occurrences:
                    key_occurrences[key] = []
                key_occurrences[key].append({
                    'index': i,
                    'value': entry.text or ''
                })
        
        for key, occurrences in key_occurrences.items():
            if len(occurrences) > 1:
                duplicates.append({
                    'key': key,
                    'occurrences': len(occurrences),
                    'values': [occ['value'] for occ in occurrences],
                    'all_same': len(set(occ['value'] for occ in occurrences)) == 1
                })
        
        return duplicates
    
    def _calculate_statistics(self, root: ET.Element) -> Dict[str, Any]:
        """Calculate property statistics"""
        all_entries = root.findall('.//entry')
        
        stats = {
            'total_properties': len(all_entries),
            'empty_properties': 0,
            'property_types': {},
            'key_lengths': {'min': 0, 'max': 0, 'avg': 0},
            'value_lengths': {'min': 0, 'max': 0, 'avg': 0},
            'naming_patterns': {}
        }
        
        key_lengths = []
        value_lengths = []
        
        for entry in all_entries:
            key = entry.get('key', '')
            value = entry.text or ''
            
            # Count empty properties
            if not value.strip():
                stats['empty_properties'] += 1
            
            # Track lengths
            key_lengths.append(len(key))
            value_lengths.append(len(value))
            
            # Analyze property types
            prop_type = self._infer_property_type(value)
            stats['property_types'][prop_type] = stats['property_types'].get(prop_type, 0) + 1
            
            # Analyze naming patterns
            pattern = self._analyze_naming_pattern(key)
            stats['naming_patterns'][pattern] = stats['naming_patterns'].get(pattern, 0) + 1
        
        # Calculate length statistics
        if key_lengths:
            stats['key_lengths'] = {
                'min': min(key_lengths),
                'max': max(key_lengths),
                'avg': sum(key_lengths) / len(key_lengths)
            }
        
        if value_lengths:
            stats['value_lengths'] = {
                'min': min(value_lengths),
                'max': max(value_lengths),
                'avg': sum(value_lengths) / len(value_lengths)
            }
        
        return stats
    
    def _infer_property_type(self, value: str) -> str:
        """Infer the type of a property value"""
        if not value:
            return 'empty'
        
        value = value.strip()
        
        # Boolean
        if value.lower() in ['true', 'false', 'yes', 'no', 'on', 'off', '1', '0']:
            return 'boolean'
        
        # Numeric
        try:
            int(value)
            return 'integer'
        except ValueError:
            try:
                float(value)
                return 'float'
            except ValueError:
                pass
        
        # URL/URI
        if value.startswith(('http://', 'https://', 'ftp://', 'file://', 'jdbc:')):
            return 'url'
        
        # File path
        if '/' in value or '\\' in value or value.endswith(('.xml', '.properties', '.conf')):
            return 'path'
        
        # Email
        if '@' in value and '.' in value.split('@')[-1]:
            return 'email'
        
        # Class name (Java)
        if '.' in value and value[0].isupper() and not ' ' in value:
            return 'classname'
        
        # List/Array
        if ',' in value or ';' in value:
            return 'list'
        
        # Placeholder
        if '${' in value or '#{' in value:
            return 'placeholder'
        
        return 'string'
    
    def _categorize_property(self, key: str) -> str:
        """Categorize property based on its key"""
        key_lower = key.lower()
        
        categories = {
            'database': ['db', 'database', 'jdbc', 'datasource', 'sql'],
            'server': ['server', 'host', 'port', 'url', 'endpoint'],
            'security': ['password', 'secret', 'key', 'token', 'auth', 'credential'],
            'logging': ['log', 'logger', 'logging', 'debug'],
            'performance': ['cache', 'pool', 'timeout', 'max', 'min', 'size'],
            'feature': ['enable', 'disable', 'feature', 'flag'],
            'path': ['path', 'dir', 'directory', 'file', 'location'],
            'network': ['proxy', 'network', 'connection', 'socket']
        }
        
        for category, keywords in categories.items():
            if any(keyword in key_lower for keyword in keywords):
                return category
        
        return 'general'
    
    def _looks_encrypted(self, value: str) -> bool:
        """Check if a value looks like it might be encrypted"""
        if not value:
            return False
        
        # Common encryption markers
        if value.startswith(('ENC(', 'ENCRYPTED(', '{cipher}')):
            return True
        
        # Base64-like pattern (long string with specific characters)
        if len(value) > 20 and re.match(r'^[A-Za-z0-9+/=]+$', value):
            return True
        
        # Hex-like pattern
        if len(value) > 20 and re.match(r'^[0-9a-fA-F]+$', value):
            return True
        
        return False
    
    def _assess_risk_level(self, key: str, value: str) -> str:
        """Assess the risk level of a sensitive property"""
        if not value:
            return 'low'  # Empty is low risk
        
        if '${' in value or value.startswith('ENC('):
            return 'low'  # Placeholder or encrypted
        
        if self._looks_encrypted(value):
            return 'medium'  # Encrypted but still present
        
        # High risk patterns
        high_risk_keywords = ['password', 'secret', 'private', 'key']
        if any(keyword in key.lower() for keyword in high_risk_keywords) and len(value) > 3:
            return 'high'
        
        return 'medium'
    
    def _analyze_naming_pattern(self, key: str) -> str:
        """Analyze the naming pattern of a property key"""
        if not key:
            return 'empty'
        
        # Dot notation (most common in Java)
        if '.' in key:
            parts = key.split('.')
            if all(part.islower() for part in parts):
                return 'dot.lowercase'
            elif all(part[0].isupper() for part in parts if part):
                return 'dot.PascalCase'
            else:
                return 'dot.mixed'
        
        # Underscore notation
        if '_' in key:
            if key.isupper():
                return 'CONSTANT_CASE'
            elif key.islower():
                return 'snake_case'
            else:
                return 'mixed_underscore'
        
        # Hyphen notation
        if '-' in key:
            return 'kebab-case'
        
        # Camel case
        if key[0].islower() and any(c.isupper() for c in key[1:]):
            return 'camelCase'
        
        # Pascal case
        if key[0].isupper() and any(c.islower() for c in key):
            return 'PascalCase'
        
        # Simple lowercase
        if key.islower():
            return 'lowercase'
        
        # Simple uppercase
        if key.isupper():
            return 'UPPERCASE'
        
        return 'other'
    
    def _extract_properties_dict(self, root: ET.Element) -> Dict[str, str]:
        """Extract properties as a simple key-value dictionary"""
        properties = {}
        
        for entry in root.findall('.//entry'):
            key = entry.get('key')
            if key:
                properties[key] = entry.text or ''
        
        return properties
    
    def _extract_grouped_properties(self, root: ET.Element) -> Dict[str, Dict[str, str]]:
        """Extract properties grouped by prefix"""
        grouped = {}
        
        for entry in root.findall('.//entry'):
            key = entry.get('key')
            if key and '.' in key:
                prefix = key.split('.')[0]
                if prefix not in grouped:
                    grouped[prefix] = {}
                
                # Use the rest of the key as the property name
                prop_name = '.'.join(key.split('.')[1:])
                grouped[prefix][prop_name] = entry.text or ''
        
        return grouped
    
    def _extract_metadata(self, root: ET.Element) -> Dict[str, Any]:
        """Extract metadata about the properties file"""
        metadata = {}
        
        # Extract comment if present
        comment = root.find('.//comment')
        if comment is not None and comment.text:
            metadata['comment'] = comment.text.strip()
            
            # Try to extract metadata from comment
            # Look for common patterns like version, date, author
            comment_text = comment.text.lower()
            
            # Version
            version_match = re.search(r'version[:\s]+([0-9.]+)', comment_text)
            if version_match:
                metadata['version'] = version_match.group(1)
            
            # Date
            date_match = re.search(r'date[:\s]+([0-9/-]+)', comment_text)
            if date_match:
                metadata['date'] = date_match.group(1)
            
            # Author
            author_match = re.search(r'author[:\s]+([^\n]+)', comment_text)
            if author_match:
                metadata['author'] = author_match.group(1).strip()
        
        # Count total entries
        metadata['total_entries'] = len(root.findall('.//entry'))
        
        return metadata
    
    def _analyze_property_patterns(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze patterns in property definitions"""
        patterns = {
            'hierarchical_groups': {},
            'common_prefixes': {},
            'common_suffixes': {},
            'value_patterns': {}
        }
        
        all_keys = []
        
        for entry in root.findall('.//entry'):
            key = entry.get('key')
            if key:
                all_keys.append(key)
                
                # Analyze hierarchical structure
                if '.' in key:
                    parts = key.split('.')
                    for i in range(1, len(parts)):
                        prefix = '.'.join(parts[:i])
                        patterns['hierarchical_groups'][prefix] = patterns['hierarchical_groups'].get(prefix, 0) + 1
        
        # Find common prefixes (first part before delimiter)
        for key in all_keys:
            if '.' in key:
                prefix = key.split('.')[0]
                patterns['common_prefixes'][prefix] = patterns['common_prefixes'].get(prefix, 0) + 1
            elif '_' in key:
                prefix = key.split('_')[0]
                patterns['common_prefixes'][prefix] = patterns['common_prefixes'].get(prefix, 0) + 1
        
        # Find common suffixes (last part after delimiter)
        for key in all_keys:
            if '.' in key:
                suffix = key.split('.')[-1]
                patterns['common_suffixes'][suffix] = patterns['common_suffixes'].get(suffix, 0) + 1
            elif '_' in key:
                suffix = key.split('_')[-1]
                patterns['common_suffixes'][suffix] = patterns['common_suffixes'].get(suffix, 0) + 1
        
        # Limit results to most common
        patterns['common_prefixes'] = dict(sorted(patterns['common_prefixes'].items(), 
                                                key=lambda x: x[1], reverse=True)[:10])
        patterns['common_suffixes'] = dict(sorted(patterns['common_suffixes'].items(), 
                                                key=lambda x: x[1], reverse=True)[:10])
        
        return patterns
    
    def _assess_property_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess the quality of the properties configuration"""
        # Completeness (fewer empty properties is better)
        empty_ratio = findings['statistics']['empty_properties'] / max(findings['statistics']['total_properties'], 1)
        completeness = max(0, 1.0 - empty_ratio)
        
        # Security (fewer exposed sensitive values is better)
        security_score = 1.0
        high_risk_count = sum(1 for prop in findings['security_sensitive'] 
                            if prop.get('risk_level') == 'high')
        if high_risk_count > 0:
            security_score = max(0, 1.0 - (high_risk_count * 0.2))  # -0.2 per high risk
        
        # Organization (good use of hierarchical naming)
        organization = 0.0
        if findings['property_groups']:
            # More groups indicate better organization
            organization = min(len(findings['property_groups']) / 10, 1.0)
        
        # Consistency (no duplicates is better)
        consistency = 1.0 if not findings['duplicates'] else 0.5
        
        # Best practices (use of placeholders for environment-specific values)
        best_practices = 0.0
        if findings['placeholders']:
            # Having placeholders for configuration is good
            placeholder_ratio = len(findings['placeholders']) / max(findings['statistics']['total_properties'], 1)
            best_practices = min(placeholder_ratio * 5, 1.0)  # Up to 20% placeholders is good
        
        return {
            "completeness": completeness,
            "security": security_score,
            "organization": organization,
            "consistency": consistency,
            "best_practices": best_practices,
            "overall": (completeness + security_score + organization + consistency + best_practices) / 5
        }
</file>

<file path="handlers/test_report_handler.py">
#!/usr/bin/env python3
"""
JUnit/TestNG Test Report Handler

Analyzes JUnit and TestNG XML test report files to extract
test execution results, failure patterns, and test metrics.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os
from datetime import datetime

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class TestReportHandler(XMLHandler):
    """Handler for JUnit and TestNG test report XML files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        # JUnit report indicators
        if root_tag in ['testsuites', 'testsuite']:
            # Check for JUnit-specific attributes
            if root.get('tests') is not None or root.get('failures') is not None:
                return True, 1.0
        
        # TestNG report indicators
        if root_tag == 'testng-results':
            return True, 1.0
        
        # Check for test-related elements
        test_indicators = ['testcase', 'test-method', 'test', 'suite']
        found = sum(1 for ind in test_indicators if root.find(f'.//{ind}') is not None)
        
        if found >= 2:
            return True, min(found * 0.3, 0.9)
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        # Determine test framework
        if root_tag == 'testng-results':
            framework = "TestNG"
            version = root.get('version', 'unknown')
        elif root_tag in ['testsuites', 'testsuite']:
            framework = "JUnit"
            # Try to detect JUnit version
            if root.find('.//properties') is not None:
                version = "4.x"  # JUnit 4 typically has properties
            else:
                version = "5.x"  # Assume JUnit 5 for newer reports
        else:
            framework = "Generic Test Report"
            version = "unknown"
        
        return DocumentTypeInfo(
            type_name=f"{framework} Test Report",
            confidence=0.95,
            version=version,
            metadata={
                "framework": framework,
                "category": "test_results",
                "report_type": "execution_report"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        framework = self._determine_framework(root)
        
        if framework == "TestNG":
            findings = self._analyze_testng(root)
        else:
            findings = self._analyze_junit(root)
        
        recommendations = [
            "Analyze failure patterns for flaky tests",
            "Track test execution time trends",
            "Identify slow-running test suites",
            "Generate test coverage reports",
            "Monitor test stability over time",
            "Prioritize test maintenance efforts"
        ]
        
        ai_use_cases = [
            "Flaky test detection",
            "Test failure prediction",
            "Test execution optimization",
            "Root cause analysis for failures",
            "Test suite optimization",
            "Test quality metrics",
            "Regression test selection",
            "Test impact analysis"
        ]
        
        return SpecializedAnalysis(
            document_type=f"{framework} Test Report",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'total_tests': findings['summary']['total'],
                'passed_tests': findings['summary']['passed'],
                'failed_tests': findings['summary']['failed'],
                'skipped_tests': findings['summary']['skipped'],
                'test_suites': len(findings.get('suites', []))
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_test_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        framework = self._determine_framework(root)
        
        return {
            'test_summary': self._extract_test_summary(root, framework),
            'failed_tests': self._extract_failed_tests(root, framework),
            'slow_tests': self._extract_slow_tests(root, framework),
            'test_metrics': self._calculate_test_metrics(root, framework),
            'error_categories': self._categorize_errors(root, framework)
        }
    
    def _determine_framework(self, root: ET.Element) -> str:
        """Determine which test framework generated the report"""
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if root_tag == 'testng-results':
            return "TestNG"
        elif root_tag in ['testsuites', 'testsuite']:
            return "JUnit"
        else:
            return "Unknown"
    
    def _analyze_junit(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze JUnit test report"""
        findings = {
            'summary': {
                'total': 0,
                'passed': 0,
                'failed': 0,
                'skipped': 0,
                'error': 0,
                'time': 0.0
            },
            'suites': [],
            'failed_tests': [],
            'skipped_tests': [],
            'slow_tests': [],
            'execution_time': {}
        }
        
        # Handle both single testsuite and testsuites container
        if root.tag == 'testsuites' or root.tag.endswith('}testsuites'):
            testsuites = root.findall('.//testsuite')
        else:
            testsuites = [root]
        
        for suite in testsuites:
            suite_info = {
                'name': suite.get('name'),
                'tests': int(suite.get('tests', 0)),
                'failures': int(suite.get('failures', 0)),
                'errors': int(suite.get('errors', 0)),
                'skipped': int(suite.get('skipped', 0)),
                'time': float(suite.get('time', 0)),
                'timestamp': suite.get('timestamp'),
                'testcases': []
            }
            
            # Analyze test cases
            for testcase in suite.findall('.//testcase'):
                test_info = {
                    'name': testcase.get('name'),
                    'classname': testcase.get('classname'),
                    'time': float(testcase.get('time', 0)),
                    'status': 'passed'  # Default
                }
                
                # Check for failures
                failure = testcase.find('.//failure')
                if failure is not None:
                    test_info['status'] = 'failed'
                    test_info['failure'] = {
                        'message': failure.get('message'),
                        'type': failure.get('type'),
                        'text': failure.text[:500] if failure.text else None
                    }
                    findings['failed_tests'].append(test_info)
                
                # Check for errors
                error = testcase.find('.//error')
                if error is not None:
                    test_info['status'] = 'error'
                    test_info['error'] = {
                        'message': error.get('message'),
                        'type': error.get('type'),
                        'text': error.text[:500] if error.text else None
                    }
                    findings['failed_tests'].append(test_info)
                
                # Check for skipped
                skipped = testcase.find('.//skipped')
                if skipped is not None:
                    test_info['status'] = 'skipped'
                    test_info['skip_message'] = skipped.get('message')
                    findings['skipped_tests'].append(test_info)
                
                suite_info['testcases'].append(test_info)
                
                # Track slow tests
                if test_info['time'] > 1.0:  # Tests taking more than 1 second
                    findings['slow_tests'].append({
                        'name': test_info['name'],
                        'class': test_info['classname'],
                        'time': test_info['time']
                    })
            
            findings['suites'].append(suite_info)
            
            # Update summary
            findings['summary']['total'] += suite_info['tests']
            findings['summary']['failed'] += suite_info['failures']
            findings['summary']['error'] += suite_info['errors']
            findings['summary']['skipped'] += suite_info['skipped']
            findings['summary']['time'] += suite_info['time']
        
        findings['summary']['passed'] = (findings['summary']['total'] - 
                                        findings['summary']['failed'] - 
                                        findings['summary']['error'] - 
                                        findings['summary']['skipped'])
        
        # Sort slow tests by time
        findings['slow_tests'].sort(key=lambda x: x['time'], reverse=True)
        
        return findings
    
    def _analyze_testng(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze TestNG test report"""
        findings = {
            'summary': {
                'total': int(root.get('total', 0)),
                'passed': int(root.get('passed', 0)),
                'failed': int(root.get('failed', 0)),
                'skipped': int(root.get('skipped', 0)),
                'error': 0,  # TestNG doesn't separate errors
                'time': 0.0
            },
            'suites': [],
            'failed_tests': [],
            'skipped_tests': [],
            'slow_tests': [],
            'test_groups': {}
        }
        
        # Analyze suites
        for suite in root.findall('.//suite'):
            suite_info = {
                'name': suite.get('name'),
                'duration': float(suite.get('duration-ms', 0)) / 1000,  # Convert to seconds
                'started_at': suite.get('started-at'),
                'finished_at': suite.get('finished-at'),
                'tests': []
            }
            
            # Analyze tests within suite
            for test in suite.findall('.//test'):
                test_info = {
                    'name': test.get('name'),
                    'duration': float(test.get('duration-ms', 0)) / 1000,
                    'test_methods': []
                }
                
                # Analyze test methods
                for method in test.findall('.//test-method'):
                    method_info = {
                        'name': method.get('name'),
                        'signature': method.get('signature'),
                        'status': method.get('status'),
                        'duration': float(method.get('duration-ms', 0)) / 1000,
                        'started_at': method.get('started-at'),
                        'finished_at': method.get('finished-at')
                    }
                    
                    # Extract groups
                    groups = method.find('.//groups')
                    if groups is not None:
                        method_info['groups'] = [g.get('name') for g in groups.findall('.//group')]
                        # Track group statistics
                        for group_name in method_info['groups']:
                            if group_name not in findings['test_groups']:
                                findings['test_groups'][group_name] = {'total': 0, 'passed': 0, 'failed': 0}
                            findings['test_groups'][group_name]['total'] += 1
                            if method_info['status'] == 'PASS':
                                findings['test_groups'][group_name]['passed'] += 1
                            elif method_info['status'] == 'FAIL':
                                findings['test_groups'][group_name]['failed'] += 1
                    
                    # Track failures
                    if method_info['status'] == 'FAIL':
                        exception = method.find('.//exception')
                        if exception is not None:
                            method_info['exception'] = {
                                'class': exception.get('class'),
                                'message': self._get_child_text(exception, 'message'),
                                'stacktrace': self._get_child_text(exception, 'full-stacktrace', '')[:500]
                            }
                        findings['failed_tests'].append(method_info)
                    
                    # Track skipped
                    elif method_info['status'] == 'SKIP':
                        findings['skipped_tests'].append(method_info)
                    
                    # Track slow tests
                    if method_info['duration'] > 1.0:
                        findings['slow_tests'].append({
                            'name': method_info['name'],
                            'signature': method_info['signature'],
                            'time': method_info['duration']
                        })
                    
                    test_info['test_methods'].append(method_info)
                
                suite_info['tests'].append(test_info)
                findings['summary']['time'] += test_info['duration']
            
            findings['suites'].append(suite_info)
        
        # Sort slow tests
        findings['slow_tests'].sort(key=lambda x: x['time'], reverse=True)
        
        return findings
    
    def _extract_test_summary(self, root: ET.Element, framework: str) -> Dict[str, Any]:
        """Extract test execution summary"""
        if framework == "TestNG":
            return {
                'total': int(root.get('total', 0)),
                'passed': int(root.get('passed', 0)),
                'failed': int(root.get('failed', 0)),
                'skipped': int(root.get('skipped', 0)),
                'duration_ms': sum(float(s.get('duration-ms', 0)) for s in root.findall('.//suite'))
            }
        else:  # JUnit
            summary = {
                'total': 0,
                'passed': 0,
                'failed': 0,
                'errors': 0,
                'skipped': 0,
                'duration_seconds': 0.0
            }
            
            for suite in root.findall('.//testsuite'):
                summary['total'] += int(suite.get('tests', 0))
                summary['failed'] += int(suite.get('failures', 0))
                summary['errors'] += int(suite.get('errors', 0))
                summary['skipped'] += int(suite.get('skipped', 0))
                summary['duration_seconds'] += float(suite.get('time', 0))
            
            summary['passed'] = summary['total'] - summary['failed'] - summary['errors'] - summary['skipped']
            
            return summary
    
    def _extract_failed_tests(self, root: ET.Element, framework: str) -> List[Dict[str, Any]]:
        """Extract details of failed tests"""
        failed_tests = []
        
        if framework == "TestNG":
            for method in root.findall('.//test-method[@status="FAIL"]'):
                exception = method.find('.//exception')
                failed_tests.append({
                    'name': method.get('name'),
                    'signature': method.get('signature'),
                    'duration_ms': float(method.get('duration-ms', 0)),
                    'exception_class': exception.get('class') if exception is not None else None,
                    'message': self._get_child_text(exception, 'message') if exception is not None else None
                })
        else:  # JUnit
            for testcase in root.findall('.//testcase'):
                failure = testcase.find('.//failure')
                error = testcase.find('.//error')
                
                if failure is not None or error is not None:
                    fail_elem = failure if failure is not None else error
                    failed_tests.append({
                        'name': testcase.get('name'),
                        'classname': testcase.get('classname'),
                        'time': float(testcase.get('time', 0)),
                        'failure_type': fail_elem.get('type'),
                        'message': fail_elem.get('message'),
                        'text': fail_elem.text[:200] if fail_elem.text else None
                    })
        
        return failed_tests[:50]  # Limit to first 50
    
    def _extract_slow_tests(self, root: ET.Element, framework: str, threshold: float = 1.0) -> List[Dict[str, Any]]:
        """Extract slow-running tests"""
        slow_tests = []
        
        if framework == "TestNG":
            for method in root.findall('.//test-method'):
                duration = float(method.get('duration-ms', 0)) / 1000  # Convert to seconds
                if duration > threshold:
                    slow_tests.append({
                        'name': method.get('name'),
                        'signature': method.get('signature'),
                        'duration_seconds': duration,
                        'status': method.get('status')
                    })
        else:  # JUnit
            for testcase in root.findall('.//testcase'):
                time = float(testcase.get('time', 0))
                if time > threshold:
                    slow_tests.append({
                        'name': testcase.get('name'),
                        'classname': testcase.get('classname'),
                        'duration_seconds': time
                    })
        
        # Sort by duration descending
        slow_tests.sort(key=lambda x: x['duration_seconds'], reverse=True)
        
        return slow_tests[:20]  # Top 20 slowest
    
    def _calculate_test_metrics(self, root: ET.Element, framework: str) -> Dict[str, Any]:
        """Calculate various test metrics"""
        metrics = {
            'success_rate': 0.0,
            'average_test_time': 0.0,
            'total_execution_time': 0.0,
            'test_distribution': {},
            'failure_rate_by_suite': {}
        }
        
        # Get summary stats
        summary = self._extract_test_summary(root, framework)
        
        # Calculate success rate
        if summary['total'] > 0:
            metrics['success_rate'] = summary['passed'] / summary['total']
        
        # Calculate average test time
        if framework == "TestNG":
            test_count = 0
            total_time = 0
            for method in root.findall('.//test-method'):
                test_count += 1
                total_time += float(method.get('duration-ms', 0)) / 1000
            
            if test_count > 0:
                metrics['average_test_time'] = total_time / test_count
            metrics['total_execution_time'] = total_time
        else:  # JUnit
            test_count = summary['total']
            total_time = summary['duration_seconds']
            
            if test_count > 0:
                metrics['average_test_time'] = total_time / test_count
            metrics['total_execution_time'] = total_time
        
        # Test distribution by status
        metrics['test_distribution'] = {
            'passed': summary['passed'],
            'failed': summary.get('failed', 0),
            'skipped': summary.get('skipped', 0),
            'error': summary.get('errors', 0)
        }
        
        # Failure rate by suite
        if framework == "TestNG":
            for suite in root.findall('.//suite'):
                suite_name = suite.get('name')
                suite_stats = {
                    'total': 0,
                    'failed': 0
                }
                
                for method in suite.findall('.//test-method'):
                    suite_stats['total'] += 1
                    if method.get('status') == 'FAIL':
                        suite_stats['failed'] += 1
                
                if suite_stats['total'] > 0:
                    metrics['failure_rate_by_suite'][suite_name] = suite_stats['failed'] / suite_stats['total']
        else:  # JUnit
            for suite in root.findall('.//testsuite'):
                suite_name = suite.get('name')
                total = int(suite.get('tests', 0))
                failed = int(suite.get('failures', 0)) + int(suite.get('errors', 0))
                
                if total > 0:
                    metrics['failure_rate_by_suite'][suite_name] = failed / total
        
        return metrics
    
    def _categorize_errors(self, root: ET.Element, framework: str) -> Dict[str, List[Dict[str, Any]]]:
        """Categorize test failures by error type"""
        error_categories = {
            'assertion_errors': [],
            'null_pointer': [],
            'timeout': [],
            'setup_errors': [],
            'other': []
        }
        
        if framework == "TestNG":
            for method in root.findall('.//test-method[@status="FAIL"]'):
                exception = method.find('.//exception')
                if exception is not None:
                    error_info = {
                        'test': method.get('name'),
                        'exception_class': exception.get('class'),
                        'message': self._get_child_text(exception, 'message')
                    }
                    
                    self._categorize_single_error(error_info, error_categories)
        else:  # JUnit
            for testcase in root.findall('.//testcase'):
                failure = testcase.find('.//failure')
                error = testcase.find('.//error')
                
                if failure is not None or error is not None:
                    fail_elem = failure if failure is not None else error
                    error_info = {
                        'test': testcase.get('name'),
                        'class': testcase.get('classname'),
                        'exception_class': fail_elem.get('type'),
                        'message': fail_elem.get('message')
                    }
                    
                    self._categorize_single_error(error_info, error_categories)
        
        # Limit each category
        for category in error_categories:
            error_categories[category] = error_categories[category][:10]
        
        return error_categories
    
    def _categorize_single_error(self, error_info: Dict[str, Any], categories: Dict[str, List]):
        """Categorize a single error"""
        exception_class = error_info.get('exception_class', '').lower()
        message = (error_info.get('message') or '').lower()
        
        if 'assert' in exception_class or 'assert' in message:
            categories['assertion_errors'].append(error_info)
        elif 'nullpointer' in exception_class or 'null pointer' in message:
            categories['null_pointer'].append(error_info)
        elif 'timeout' in exception_class or 'timeout' in message:
            categories['timeout'].append(error_info)
        elif 'setup' in message or 'before' in exception_class or 'after' in exception_class:
            categories['setup_errors'].append(error_info)
        else:
            categories['other'].append(error_info)
    
    def _assess_test_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess test suite quality metrics"""
        # Test coverage (based on success rate)
        success_rate = 0.0
        if findings['summary']['total'] > 0:
            success_rate = findings['summary']['passed'] / findings['summary']['total']
        
        # Test stability (inverse of failure rate)
        stability = success_rate
        
        # Performance (based on slow tests)
        performance = 1.0
        if findings['summary']['total'] > 0:
            slow_test_ratio = len(findings['slow_tests']) / findings['summary']['total']
            performance = max(0, 1.0 - slow_test_ratio * 2)  # Penalize if >50% are slow
        
        # Test maintenance (based on skip rate)
        maintenance = 1.0
        if findings['summary']['total'] > 0:
            skip_ratio = findings['summary']['skipped'] / findings['summary']['total']
            maintenance = max(0, 1.0 - skip_ratio * 2)  # Penalize if >50% are skipped
        
        # Flakiness indicator (would need historical data for accurate measurement)
        # For now, use a simple heuristic based on error types
        flakiness_score = 0.8  # Default to good
        if 'failed_tests' in findings:
            timeout_failures = sum(1 for test in findings['failed_tests'] 
                                 if 'timeout' in str(test.get('failure', {}).get('type', '')).lower())
            if timeout_failures > 2:
                flakiness_score = 0.4
        
        return {
            "success_rate": success_rate,
            "stability": stability,
            "performance": performance,
            "maintenance": maintenance,
            "flakiness": flakiness_score,
            "overall": (success_rate + stability + performance + maintenance + flakiness_score) / 5
        }
    
    def _get_child_text(self, parent: ET.Element, child_name: str, default: str = None) -> Optional[str]:
        """Get text content of a child element"""
        if parent is None:
            return default
        
        child = parent.find(f'.//{child_name}')
        return child.text if child is not None and child.text else default
</file>

<file path="handlers/wsdl_handler.py">
#!/usr/bin/env python3
"""
WSDL (Web Services Description Language) Handler

Analyzes WSDL files to extract service definitions, operations,
message schemas, and binding information for SOAP web services.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class WSDLHandler(XMLHandler):
    """Handler for WSDL (Web Services Description Language) documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for WSDL root element
        if root.tag.endswith('definitions') or root.tag == 'definitions':
            # Check for WSDL namespace
            if any('schemas.xmlsoap.org/wsdl' in uri for uri in namespaces.values()):
                return True, 1.0
            return True, 0.7
        
        # WSDL 2.0 uses 'description' as root
        if root.tag.endswith('description') or root.tag == 'description':
            if any('w3.org/ns/wsdl' in uri for uri in namespaces.values()):
                return True, 0.9
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Determine WSDL version
        version = "1.1"  # Default
        if root.tag.endswith('description') or any('w3.org/ns/wsdl' in uri for uri in namespaces.values()):
            version = "2.0"
        
        target_namespace = root.get('targetNamespace', '')
        
        return DocumentTypeInfo(
            type_name="WSDL Service Definition",
            confidence=1.0,
            version=version,
            schema_uri=target_namespace,
            metadata={
                "standard": f"WSDL {version}",
                "category": "web_service_definition",
                "protocol": "SOAP"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        # Determine version for proper parsing
        is_wsdl2 = root.tag.endswith('description') or 'w3.org/ns/wsdl' in str(root.attrib.values())
        
        if is_wsdl2:
            findings = self._analyze_wsdl2(root)
        else:
            findings = self._analyze_wsdl1(root)
        
        recommendations = [
            "Generate client code from WSDL",
            "Extract operation documentation",
            "Analyze service dependencies",
            "Create API test cases from operations",
            "Map SOAP operations to REST endpoints",
            "Monitor deprecated operations"
        ]
        
        ai_use_cases = [
            "SOAP to REST API migration",
            "Service dependency mapping",
            "API documentation generation",
            "Test case generation",
            "Service compatibility checking",
            "Operation complexity analysis",
            "Security policy extraction",
            "Performance bottleneck identification"
        ]
        
        return SpecializedAnalysis(
            document_type="WSDL Service Definition",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'services': len(findings.get('services', [])),
                'operations': findings.get('total_operations', 0),
                'messages': len(findings.get('messages', [])),
                'types': len(findings.get('types', [])),
                'bindings': len(findings.get('bindings', []))
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_wsdl_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        is_wsdl2 = root.tag.endswith('description')
        
        if is_wsdl2:
            return self._extract_wsdl2_data(root)
        else:
            return self._extract_wsdl1_data(root)
    
    def _analyze_wsdl1(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze WSDL 1.1 document"""
        findings = {
            'services': self._extract_services(root),
            'port_types': self._extract_port_types(root),
            'operations': self._extract_operations(root),
            'messages': self._extract_messages(root),
            'types': self._extract_types(root),
            'bindings': self._extract_bindings(root),
            'imports': self._extract_imports(root),
            'total_operations': 0
        }
        
        # Count total operations
        for port_type in findings['port_types']:
            findings['total_operations'] += len(port_type.get('operations', []))
        
        return findings
    
    def _analyze_wsdl2(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze WSDL 2.0 document"""
        # WSDL 2.0 has different structure
        findings = {
            'services': self._extract_services_v2(root),
            'interfaces': self._extract_interfaces_v2(root),
            'operations': self._extract_operations_v2(root),
            'types': self._extract_types(root),  # Similar to 1.1
            'bindings': self._extract_bindings_v2(root),
            'imports': self._extract_imports(root),
            'total_operations': 0
        }
        
        # Count operations
        for interface in findings['interfaces']:
            findings['total_operations'] += len(interface.get('operations', []))
        
        return findings
    
    def _extract_services(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract service definitions from WSDL 1.1"""
        services = []
        
        for service in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}service'):
            service_info = {
                'name': service.get('name'),
                'documentation': self._get_documentation(service),
                'ports': []
            }
            
            # Extract ports
            for port in service.findall('.//{http://schemas.xmlsoap.org/wsdl/}port'):
                port_info = {
                    'name': port.get('name'),
                    'binding': port.get('binding'),
                    'address': None
                }
                
                # Get SOAP address
                soap_addr = port.find('.//{http://schemas.xmlsoap.org/wsdl/soap/}address')
                if soap_addr is not None:
                    port_info['address'] = soap_addr.get('location')
                
                # Check for SOAP 1.2
                soap12_addr = port.find('.//{http://schemas.xmlsoap.org/wsdl/soap12/}address')
                if soap12_addr is not None:
                    port_info['address'] = soap12_addr.get('location')
                    port_info['soap_version'] = '1.2'
                
                service_info['ports'].append(port_info)
            
            services.append(service_info)
        
        return services
    
    def _extract_port_types(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract port types (interfaces) from WSDL 1.1"""
        port_types = []
        
        for pt in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}portType'):
            pt_info = {
                'name': pt.get('name'),
                'operations': []
            }
            
            # Extract operations
            for op in pt.findall('.//{http://schemas.xmlsoap.org/wsdl/}operation'):
                op_info = {
                    'name': op.get('name'),
                    'documentation': self._get_documentation(op),
                    'input': None,
                    'output': None,
                    'faults': []
                }
                
                # Input message
                input_elem = op.find('.//{http://schemas.xmlsoap.org/wsdl/}input')
                if input_elem is not None:
                    op_info['input'] = input_elem.get('message')
                
                # Output message
                output_elem = op.find('.//{http://schemas.xmlsoap.org/wsdl/}output')
                if output_elem is not None:
                    op_info['output'] = output_elem.get('message')
                
                # Fault messages
                for fault in op.findall('.//{http://schemas.xmlsoap.org/wsdl/}fault'):
                    op_info['faults'].append({
                        'name': fault.get('name'),
                        'message': fault.get('message')
                    })
                
                pt_info['operations'].append(op_info)
            
            port_types.append(pt_info)
        
        return port_types
    
    def _extract_operations(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract all operations with details"""
        operations = []
        
        for pt in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}portType'):
            port_type_name = pt.get('name')
            
            for op in pt.findall('.//{http://schemas.xmlsoap.org/wsdl/}operation'):
                operations.append({
                    'name': op.get('name'),
                    'port_type': port_type_name,
                    'pattern': self._determine_mep(op),  # Message Exchange Pattern
                    'documentation': self._get_documentation(op)
                })
        
        return operations
    
    def _extract_messages(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract message definitions"""
        messages = []
        
        for msg in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}message'):
            msg_info = {
                'name': msg.get('name'),
                'parts': []
            }
            
            # Extract parts
            for part in msg.findall('.//{http://schemas.xmlsoap.org/wsdl/}part'):
                part_info = {
                    'name': part.get('name'),
                    'type': part.get('type'),
                    'element': part.get('element')
                }
                msg_info['parts'].append(part_info)
            
            messages.append(msg_info)
        
        return messages
    
    def _extract_types(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract type definitions from embedded schemas"""
        types = []
        
        types_section = root.find('.//{http://schemas.xmlsoap.org/wsdl/}types')
        if types_section is None:
            return types
        
        # Find all schemas
        for schema in types_section.findall('.//{http://www.w3.org/2001/XMLSchema}schema'):
            target_ns = schema.get('targetNamespace', 'default')
            
            # Extract complex types
            for ct in schema.findall('.//{http://www.w3.org/2001/XMLSchema}complexType'):
                types.append({
                    'name': ct.get('name'),
                    'namespace': target_ns,
                    'kind': 'complex'
                })
            
            # Extract simple types
            for st in schema.findall('.//{http://www.w3.org/2001/XMLSchema}simpleType'):
                types.append({
                    'name': st.get('name'),
                    'namespace': target_ns,
                    'kind': 'simple'
                })
            
            # Extract elements
            for elem in schema.findall('.//{http://www.w3.org/2001/XMLSchema}element'):
                if elem.get('name'):
                    types.append({
                        'name': elem.get('name'),
                        'namespace': target_ns,
                        'kind': 'element'
                    })
        
        return types
    
    def _extract_bindings(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract binding definitions"""
        bindings = []
        
        for binding in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}binding'):
            binding_info = {
                'name': binding.get('name'),
                'type': binding.get('type'),
                'protocol': 'unknown',
                'style': None,
                'transport': None,
                'operations': []
            }
            
            # Check for SOAP binding
            soap_binding = binding.find('.//{http://schemas.xmlsoap.org/wsdl/soap/}binding')
            if soap_binding is not None:
                binding_info['protocol'] = 'SOAP'
                binding_info['style'] = soap_binding.get('style', 'document')
                binding_info['transport'] = soap_binding.get('transport')
            
            # Extract operation bindings
            for op in binding.findall('.//{http://schemas.xmlsoap.org/wsdl/}operation'):
                op_binding = {
                    'name': op.get('name'),
                    'soap_action': None,
                    'style': None
                }
                
                soap_op = op.find('.//{http://schemas.xmlsoap.org/wsdl/soap/}operation')
                if soap_op is not None:
                    op_binding['soap_action'] = soap_op.get('soapAction')
                    op_binding['style'] = soap_op.get('style')
                
                binding_info['operations'].append(op_binding)
            
            bindings.append(binding_info)
        
        return bindings
    
    def _extract_imports(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract import statements"""
        imports = []
        
        for imp in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}import'):
            imports.append({
                'namespace': imp.get('namespace'),
                'location': imp.get('location')
            })
        
        return imports
    
    # WSDL 2.0 specific methods
    def _extract_services_v2(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract services from WSDL 2.0"""
        services = []
        
        for service in root.findall('.//{http://www.w3.org/ns/wsdl}service'):
            service_info = {
                'name': service.get('name'),
                'interface': service.get('interface'),
                'endpoints': []
            }
            
            for endpoint in service.findall('.//{http://www.w3.org/ns/wsdl}endpoint'):
                service_info['endpoints'].append({
                    'name': endpoint.get('name'),
                    'binding': endpoint.get('binding'),
                    'address': endpoint.get('address')
                })
            
            services.append(service_info)
        
        return services
    
    def _extract_interfaces_v2(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract interfaces from WSDL 2.0 (equivalent to portType in 1.1)"""
        interfaces = []
        
        for interface in root.findall('.//{http://www.w3.org/ns/wsdl}interface'):
            interface_info = {
                'name': interface.get('name'),
                'extends': interface.get('extends'),
                'operations': []
            }
            
            for op in interface.findall('.//{http://www.w3.org/ns/wsdl}operation'):
                interface_info['operations'].append({
                    'name': op.get('name'),
                    'pattern': op.get('pattern'),
                    'style': op.get('style')
                })
            
            interfaces.append(interface_info)
        
        return interfaces
    
    def _extract_operations_v2(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract operations from WSDL 2.0"""
        operations = []
        
        for interface in root.findall('.//{http://www.w3.org/ns/wsdl}interface'):
            interface_name = interface.get('name')
            
            for op in interface.findall('.//{http://www.w3.org/ns/wsdl}operation'):
                operations.append({
                    'name': op.get('name'),
                    'interface': interface_name,
                    'pattern': op.get('pattern', 'in-out'),
                    'safe': op.get('safe', 'false') == 'true'
                })
        
        return operations
    
    def _extract_bindings_v2(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract bindings from WSDL 2.0"""
        bindings = []
        
        for binding in root.findall('.//{http://www.w3.org/ns/wsdl}binding'):
            bindings.append({
                'name': binding.get('name'),
                'interface': binding.get('interface'),
                'type': binding.get('type'),
                'protocol': self._extract_protocol_v2(binding)
            })
        
        return bindings
    
    def _extract_protocol_v2(self, binding: ET.Element) -> str:
        """Determine protocol from WSDL 2.0 binding"""
        binding_type = binding.get('type', '')
        
        if 'soap' in binding_type.lower():
            return 'SOAP'
        elif 'http' in binding_type.lower():
            return 'HTTP'
        else:
            return 'unknown'
    
    def _determine_mep(self, operation: ET.Element) -> str:
        """Determine Message Exchange Pattern"""
        has_input = operation.find('.//{http://schemas.xmlsoap.org/wsdl/}input') is not None
        has_output = operation.find('.//{http://schemas.xmlsoap.org/wsdl/}output') is not None
        
        if has_input and has_output:
            return 'request-response'
        elif has_input and not has_output:
            return 'one-way'
        elif not has_input and has_output:
            return 'notification'
        else:
            return 'unknown'
    
    def _get_documentation(self, element: ET.Element) -> Optional[str]:
        """Extract documentation from element"""
        doc = element.find('.//{http://schemas.xmlsoap.org/wsdl/}documentation')
        if doc is not None and doc.text:
            return doc.text.strip()
        return None
    
    def _extract_wsdl1_data(self, root: ET.Element) -> Dict[str, Any]:
        """Extract key data for WSDL 1.1"""
        return {
            'service_endpoints': self._extract_all_endpoints(root),
            'operation_signatures': self._extract_operation_signatures(root),
            'soap_actions': self._extract_soap_actions(root),
            'message_schemas': self._extract_message_schemas(root)
        }
    
    def _extract_wsdl2_data(self, root: ET.Element) -> Dict[str, Any]:
        """Extract key data for WSDL 2.0"""
        return {
            'service_endpoints': self._extract_all_endpoints_v2(root),
            'interface_hierarchy': self._extract_interface_hierarchy(root),
            'operation_patterns': self._extract_operation_patterns_v2(root)
        }
    
    def _extract_all_endpoints(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract all service endpoints"""
        endpoints = []
        
        for service in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}service'):
            service_name = service.get('name')
            
            for port in service.findall('.//{http://schemas.xmlsoap.org/wsdl/}port'):
                # SOAP address
                addr = port.find('.//{http://schemas.xmlsoap.org/wsdl/soap/}address')
                if addr is not None:
                    endpoints.append({
                        'service': service_name,
                        'port': port.get('name'),
                        'url': addr.get('location'),
                        'protocol': 'SOAP'
                    })
        
        return endpoints
    
    def _extract_operation_signatures(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract operation signatures with input/output"""
        signatures = []
        
        for pt in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}portType'):
            for op in pt.findall('.//{http://schemas.xmlsoap.org/wsdl/}operation'):
                sig = {
                    'operation': op.get('name'),
                    'port_type': pt.get('name'),
                    'input': None,
                    'output': None
                }
                
                input_elem = op.find('.//{http://schemas.xmlsoap.org/wsdl/}input')
                if input_elem is not None:
                    sig['input'] = self._resolve_message_type(root, input_elem.get('message'))
                
                output_elem = op.find('.//{http://schemas.xmlsoap.org/wsdl/}output')
                if output_elem is not None:
                    sig['output'] = self._resolve_message_type(root, output_elem.get('message'))
                
                signatures.append(sig)
        
        return signatures[:20]  # Limit to first 20
    
    def _extract_soap_actions(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract SOAP actions for operations"""
        actions = []
        
        for binding in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}binding'):
            for op in binding.findall('.//{http://schemas.xmlsoap.org/wsdl/}operation'):
                soap_op = op.find('.//{http://schemas.xmlsoap.org/wsdl/soap/}operation')
                if soap_op is not None and soap_op.get('soapAction'):
                    actions.append({
                        'operation': op.get('name'),
                        'binding': binding.get('name'),
                        'action': soap_op.get('soapAction')
                    })
        
        return actions
    
    def _extract_message_schemas(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Extract message schemas"""
        schemas = []
        
        for msg in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}message')[:10]:
            schema = {
                'message': msg.get('name'),
                'parts': []
            }
            
            for part in msg.findall('.//{http://schemas.xmlsoap.org/wsdl/}part'):
                schema['parts'].append({
                    'name': part.get('name'),
                    'type': part.get('type') or part.get('element')
                })
            
            schemas.append(schema)
        
        return schemas
    
    def _resolve_message_type(self, root: ET.Element, message_ref: str) -> Optional[str]:
        """Resolve message reference to type"""
        if not message_ref:
            return None
        
        # Remove namespace prefix if present
        msg_name = message_ref.split(':')[-1]
        
        # Find message
        for msg in root.findall('.//{http://schemas.xmlsoap.org/wsdl/}message'):
            if msg.get('name') == msg_name:
                # Get first part's type
                part = msg.find('.//{http://schemas.xmlsoap.org/wsdl/}part')
                if part is not None:
                    return part.get('type') or part.get('element')
        
        return message_ref
    
    def _extract_all_endpoints_v2(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract endpoints from WSDL 2.0"""
        endpoints = []
        
        for service in root.findall('.//{http://www.w3.org/ns/wsdl}service'):
            for endpoint in service.findall('.//{http://www.w3.org/ns/wsdl}endpoint'):
                endpoints.append({
                    'service': service.get('name'),
                    'endpoint': endpoint.get('name'),
                    'address': endpoint.get('address'),
                    'binding': endpoint.get('binding')
                })
        
        return endpoints
    
    def _extract_interface_hierarchy(self, root: ET.Element) -> Dict[str, List[str]]:
        """Extract interface inheritance in WSDL 2.0"""
        hierarchy = {}
        
        for interface in root.findall('.//{http://www.w3.org/ns/wsdl}interface'):
            name = interface.get('name')
            extends = interface.get('extends')
            
            if extends:
                hierarchy[name] = [e.strip() for e in extends.split()]
            else:
                hierarchy[name] = []
        
        return hierarchy
    
    def _extract_operation_patterns_v2(self, root: ET.Element) -> List[Dict[str, str]]:
        """Extract operation patterns from WSDL 2.0"""
        patterns = []
        
        for interface in root.findall('.//{http://www.w3.org/ns/wsdl}interface'):
            for op in interface.findall('.//{http://www.w3.org/ns/wsdl}operation'):
                patterns.append({
                    'interface': interface.get('name'),
                    'operation': op.get('name'),
                    'pattern': op.get('pattern', 'in-out')
                })
        
        return patterns
    
    def _assess_wsdl_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        """Assess WSDL quality metrics"""
        # Documentation coverage
        doc_count = 0
        total_items = 0
        
        # Count documented operations
        for pt in findings.get('port_types', []):
            for op in pt.get('operations', []):
                total_items += 1
                if op.get('documentation'):
                    doc_count += 1
        
        doc_coverage = doc_count / max(total_items, 1)
        
        # Service completeness
        services = findings.get('services', [])
        endpoints_defined = sum(len(s.get('ports', [])) for s in services)
        service_completeness = min(endpoints_defined / max(len(services), 1), 1.0)
        
        # Type definition coverage
        types_defined = len(findings.get('types', []))
        type_coverage = min(types_defined / 20, 1.0)  # Assume 20+ types is good
        
        # Binding completeness
        bindings = findings.get('bindings', [])
        binding_completeness = 1.0 if bindings else 0.0
        
        return {
            "documentation": doc_coverage,
            "service_completeness": service_completeness,
            "type_coverage": type_coverage,
            "binding_completeness": binding_completeness,
            "overall_quality": (doc_coverage + service_completeness + type_coverage + binding_completeness) / 4
        }
</file>

<file path="handlers/xsd_handler.py">
#!/usr/bin/env python3
"""
XSD (XML Schema Definition) Handler

Analyzes XML Schema files to extract type definitions, validation rules,
and structural constraints for data quality and validation purposes.
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
import sys
import os

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis


class XSDSchemaHandler(XMLHandler):
    """Handler for XML Schema Definition files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for schema elements
        if root.tag.endswith('schema') or root.tag == 'schema':
            # Check for XSD namespace
            if any('XMLSchema' in uri for uri in namespaces.values()):
                return True, 1.0
            return True, 0.7
            
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        target_namespace = root.get('targetNamespace', 'none')
        version = root.get('version', '1.0')
        
        # Check if it's a specific schema type
        schema_type = "Generic XSD"
        if 'w3.org' in target_namespace:
            schema_type = "W3C Standard Schema"
        elif 'maven' in target_namespace.lower():
            schema_type = "Maven XSD"
        elif 'spring' in target_namespace.lower():
            schema_type = "Spring Framework XSD"
        
        return DocumentTypeInfo(
            type_name="XML Schema Definition",
            confidence=1.0,
            version=version,
            schema_uri=target_namespace,
            metadata={
                "standard": "W3C XSD",
                "category": "schema_definition",
                "target_namespace": target_namespace,
                "schema_type": schema_type
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'types': self._analyze_types(root),
            'elements': self._analyze_elements(root),
            'attributes': self._analyze_attributes(root),
            'validation_rules': self._extract_validation_rules(root),
            'namespaces': self._analyze_namespaces(root),
            'imports': self._find_imports(root),
            'complexity_metrics': self._calculate_complexity(root)
        }
        
        recommendations = [
            "Generate sample valid/invalid XML for testing",
            "Extract validation rules for data quality checks",
            "Create documentation from annotations",
            "Identify reusable type definitions",
            "Check for overly restrictive constraints",
            "Monitor schema evolution over time"
        ]
        
        ai_use_cases = [
            "Automated test data generation",
            "Schema evolution tracking",
            "Data quality rule extraction",
            "Documentation generation",
            "Schema compatibility checking",
            "Type system analysis",
            "Validation rule optimization",
            "Schema migration assistance"
        ]
        
        return SpecializedAnalysis(
            document_type="XML Schema Definition",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'complex_types': len(findings['types']['complex']),
                'simple_types': len(findings['types']['simple']),
                'global_elements': len(findings['elements']['global']),
                'global_attributes': len(findings['attributes']['global']),
                'validation_rules': len(findings['validation_rules'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_schema_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'type_definitions': self._extract_type_definitions(root),
            'element_definitions': self._extract_element_definitions(root),
            'validation_constraints': self._extract_constraints(root),
            'documentation': self._extract_documentation(root),
            'schema_metadata': {
                'target_namespace': root.get('targetNamespace'),
                'element_form_default': root.get('elementFormDefault', 'unqualified'),
                'attribute_form_default': root.get('attributeFormDefault', 'unqualified'),
                'version': root.get('version')
            }
        }
    
    def _analyze_types(self, root: ET.Element) -> Dict[str, List[Dict[str, Any]]]:
        complex_types = []
        simple_types = []
        
        # Extract complex types
        for ct in root.findall('.//{http://www.w3.org/2001/XMLSchema}complexType'):
            complex_types.append({
                'name': ct.get('name', 'anonymous'),
                'abstract': ct.get('abstract', 'false') == 'true',
                'mixed': ct.get('mixed', 'false') == 'true',
                'base': self._find_base_type(ct),
                'elements': len(ct.findall('.//{http://www.w3.org/2001/XMLSchema}element')),
                'attributes': len(ct.findall('.//{http://www.w3.org/2001/XMLSchema}attribute'))
            })
        
        # Extract simple types  
        for st in root.findall('.//{http://www.w3.org/2001/XMLSchema}simpleType'):
            simple_types.append({
                'name': st.get('name', 'anonymous'),
                'base': self._find_restriction_base(st),
                'constraints': self._extract_simple_constraints(st)
            })
            
        return {'complex': complex_types, 'simple': simple_types}
    
    def _analyze_elements(self, root: ET.Element) -> Dict[str, Any]:
        global_elements = []
        local_elements = []
        
        # Global elements (direct children of schema)
        for elem in root.findall('./{http://www.w3.org/2001/XMLSchema}element'):
            global_elements.append({
                'name': elem.get('name'),
                'type': elem.get('type'),
                'nillable': elem.get('nillable', 'false') == 'true',
                'abstract': elem.get('abstract', 'false') == 'true',
                'substitution_group': elem.get('substitutionGroup')
            })
        
        # Count local elements
        all_elements = root.findall('.//{http://www.w3.org/2001/XMLSchema}element')
        local_count = len(all_elements) - len(global_elements)
        
        return {
            'global': global_elements,
            'local_count': local_count,
            'total': len(all_elements)
        }
    
    def _analyze_attributes(self, root: ET.Element) -> Dict[str, Any]:
        global_attrs = []
        
        # Global attributes
        for attr in root.findall('./{http://www.w3.org/2001/XMLSchema}attribute'):
            global_attrs.append({
                'name': attr.get('name'),
                'type': attr.get('type'),
                'use': attr.get('use', 'optional'),
                'default': attr.get('default')
            })
        
        # Attribute groups
        attr_groups = root.findall('.//{http://www.w3.org/2001/XMLSchema}attributeGroup[@name]')
        
        return {
            'global': global_attrs,
            'groups': [{'name': ag.get('name')} for ag in attr_groups],
            'total': len(root.findall('.//{http://www.w3.org/2001/XMLSchema}attribute'))
        }
    
    def _extract_validation_rules(self, root: ET.Element) -> List[Dict[str, Any]]:
        rules = []
        
        # Extract all restrictions
        for restriction in root.findall('.//{http://www.w3.org/2001/XMLSchema}restriction'):
            base = restriction.get('base', 'unknown')
            constraints = {}
            
            # Common facets
            facets = ['minLength', 'maxLength', 'pattern', 'enumeration', 
                     'minInclusive', 'maxInclusive', 'minExclusive', 'maxExclusive',
                     'totalDigits', 'fractionDigits', 'whiteSpace']
            
            for facet in facets:
                elements = restriction.findall(f'.//{{http://www.w3.org/2001/XMLSchema}}{facet}')
                if elements:
                    if facet == 'enumeration':
                        constraints[facet] = [e.get('value') for e in elements]
                    elif len(elements) == 1:
                        constraints[facet] = elements[0].get('value')
                    else:
                        constraints[facet] = [e.get('value') for e in elements]
            
            if constraints:
                rules.append({
                    'base_type': base,
                    'constraints': constraints
                })
        
        # Extract key/keyref constraints
        for key in root.findall('.//{http://www.w3.org/2001/XMLSchema}key'):
            rules.append({
                'type': 'key',
                'name': key.get('name'),
                'selector': key.find('.//{http://www.w3.org/2001/XMLSchema}selector').get('xpath', ''),
                'fields': [f.get('xpath', '') for f in key.findall('.//{http://www.w3.org/2001/XMLSchema}field')]
            })
        
        return rules
    
    def _analyze_namespaces(self, root: ET.Element) -> Dict[str, Any]:
        imports = []
        includes = []
        
        for imp in root.findall('./{http://www.w3.org/2001/XMLSchema}import'):
            imports.append({
                'namespace': imp.get('namespace'),
                'location': imp.get('schemaLocation')
            })
        
        for inc in root.findall('./{http://www.w3.org/2001/XMLSchema}include'):
            includes.append({
                'location': inc.get('schemaLocation')
            })
        
        return {
            'target': root.get('targetNamespace'),
            'imports': imports,
            'includes': includes
        }
    
    def _find_imports(self, root: ET.Element) -> List[Dict[str, str]]:
        imports = []
        
        for imp in root.findall('.//{http://www.w3.org/2001/XMLSchema}import'):
            imports.append({
                'namespace': imp.get('namespace', ''),
                'location': imp.get('schemaLocation', '')
            })
        
        return imports
    
    def _calculate_complexity(self, root: ET.Element) -> Dict[str, Any]:
        # Count various elements to assess complexity
        metrics = {
            'total_types': len(root.findall('.//{http://www.w3.org/2001/XMLSchema}complexType')) + 
                          len(root.findall('.//{http://www.w3.org/2001/XMLSchema}simpleType')),
            'total_elements': len(root.findall('.//{http://www.w3.org/2001/XMLSchema}element')),
            'total_attributes': len(root.findall('.//{http://www.w3.org/2001/XMLSchema}attribute')),
            'max_nesting': self._calculate_max_nesting(root),
            'has_recursion': self._check_recursion(root),
            'uses_substitution_groups': len(root.findall('.//*[@substitutionGroup]')) > 0,
            'uses_abstract_types': len(root.findall('.//*[@abstract="true"]')) > 0
        }
        
        # Calculate complexity score
        complexity_score = (
            min(metrics['total_types'] / 50, 1.0) * 0.3 +
            min(metrics['total_elements'] / 100, 1.0) * 0.3 +
            min(metrics['max_nesting'] / 10, 1.0) * 0.2 +
            (0.2 if metrics['has_recursion'] else 0.0)
        )
        
        metrics['complexity_score'] = round(complexity_score, 2)
        
        return metrics
    
    def _find_base_type(self, complex_type: ET.Element) -> Optional[str]:
        # Check for extension
        extension = complex_type.find('.//{http://www.w3.org/2001/XMLSchema}extension')
        if extension is not None:
            return extension.get('base')
        
        # Check for restriction
        restriction = complex_type.find('.//{http://www.w3.org/2001/XMLSchema}restriction')
        if restriction is not None:
            return restriction.get('base')
        
        return None
    
    def _find_restriction_base(self, simple_type: ET.Element) -> Optional[str]:
        restriction = simple_type.find('./{http://www.w3.org/2001/XMLSchema}restriction')
        if restriction is not None:
            return restriction.get('base')
        
        # Check for list
        list_elem = simple_type.find('./{http://www.w3.org/2001/XMLSchema}list')
        if list_elem is not None:
            return f"list of {list_elem.get('itemType', 'unknown')}"
        
        # Check for union
        union_elem = simple_type.find('./{http://www.w3.org/2001/XMLSchema}union')
        if union_elem is not None:
            return f"union of {union_elem.get('memberTypes', 'multiple types')}"
        
        return None
    
    def _extract_simple_constraints(self, simple_type: ET.Element) -> Dict[str, Any]:
        constraints = {}
        restriction = simple_type.find('./{http://www.w3.org/2001/XMLSchema}restriction')
        
        if restriction is not None:
            # Extract enumeration values
            enums = restriction.findall('./{http://www.w3.org/2001/XMLSchema}enumeration')
            if enums:
                constraints['enumeration'] = [e.get('value') for e in enums]
            
            # Extract pattern
            pattern = restriction.find('./{http://www.w3.org/2001/XMLSchema}pattern')
            if pattern is not None:
                constraints['pattern'] = pattern.get('value')
            
            # Extract length constraints
            for constraint in ['minLength', 'maxLength', 'length']:
                elem = restriction.find(f'.//{{http://www.w3.org/2001/XMLSchema}}{constraint}')
                if elem is not None:
                    constraints[constraint] = elem.get('value')
        
        return constraints
    
    def _extract_type_definitions(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Return first 20 type definitions with details
        types = []
        
        for ct in root.findall('.//{http://www.w3.org/2001/XMLSchema}complexType[@name]')[:10]:
            types.append({
                'name': ct.get('name'),
                'kind': 'complex',
                'abstract': ct.get('abstract', 'false') == 'true',
                'documentation': self._get_documentation(ct)
            })
        
        for st in root.findall('.//{http://www.w3.org/2001/XMLSchema}simpleType[@name]')[:10]:
            types.append({
                'name': st.get('name'),
                'kind': 'simple',
                'base': self._find_restriction_base(st),
                'documentation': self._get_documentation(st)
            })
        
        return types
    
    def _extract_element_definitions(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Return global element definitions
        elements = []
        
        for elem in root.findall('./{http://www.w3.org/2001/XMLSchema}element')[:20]:
            elements.append({
                'name': elem.get('name'),
                'type': elem.get('type'),
                'min_occurs': elem.get('minOccurs', '1'),
                'max_occurs': elem.get('maxOccurs', '1'),
                'documentation': self._get_documentation(elem)
            })
        
        return elements
    
    def _extract_constraints(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Extract unique/key/keyref constraints
        constraints = []
        
        for constraint_type in ['unique', 'key', 'keyref']:
            for elem in root.findall(f'.//{{http://www.w3.org/2001/XMLSchema}}{constraint_type}'):
                constraints.append({
                    'type': constraint_type,
                    'name': elem.get('name'),
                    'refer': elem.get('refer')  # for keyref
                })
        
        return constraints
    
    def _extract_documentation(self, root: ET.Element) -> Dict[str, List[str]]:
        docs = {}
        
        for elem in root.findall('.//*[@name]'):
            doc = self._get_documentation(elem)
            if doc:
                elem_name = elem.get('name')
                elem_type = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
                key = f"{elem_type}:{elem_name}"
                docs[key] = doc
        
        return docs
    
    def _get_documentation(self, element: ET.Element) -> Optional[str]:
        annotation = element.find('./{http://www.w3.org/2001/XMLSchema}annotation')
        if annotation is not None:
            doc = annotation.find('./{http://www.w3.org/2001/XMLSchema}documentation')
            if doc is not None and doc.text:
                return doc.text.strip()
        return None
    
    def _calculate_max_nesting(self, root: ET.Element) -> int:
        # Simplified calculation of nesting depth
        max_depth = 0
        
        def check_depth(elem, depth=0):
            nonlocal max_depth
            max_depth = max(max_depth, depth)
            
            # Check sequences and choices
            for container in elem.findall('.//{http://www.w3.org/2001/XMLSchema}sequence') + \
                           elem.findall('.//{http://www.w3.org/2001/XMLSchema}choice'):
                check_depth(container, depth + 1)
        
        for ct in root.findall('.//{http://www.w3.org/2001/XMLSchema}complexType'):
            check_depth(ct)
        
        return max_depth
    
    def _check_recursion(self, root: ET.Element) -> bool:
        # Simplified check for recursive type definitions
        type_refs = {}
        
        # Build reference map
        for elem in root.findall('.//{http://www.w3.org/2001/XMLSchema}element[@type]'):
            elem_type = elem.get('type')
            parent = elem
            while parent is not None:
                parent = parent.find('..')
                if parent is not None and parent.get('name'):
                    parent_name = parent.get('name')
                    if parent_name not in type_refs:
                        type_refs[parent_name] = set()
                    type_refs[parent_name].add(elem_type)
                    break
        
        # Check for cycles (simplified)
        for type_name, refs in type_refs.items():
            if type_name in refs:
                return True
        
        return False
    
    def _assess_schema_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        # Assess various quality aspects
        metrics = findings['complexity_metrics']
        
        # Documentation coverage
        doc_count = len(findings.get('documentation', {}))
        total_named = len(findings['types']['complex']) + len(findings['types']['simple']) + \
                     len(findings['elements']['global'])
        doc_coverage = doc_count / max(total_named, 1)
        
        # Reusability (global vs local definitions)
        global_count = len(findings['types']['complex']) + len(findings['types']['simple'])
        reusability = min(global_count / 20, 1.0)  # Assume 20+ global types is good
        
        # Constraint usage
        constraint_score = min(len(findings['validation_rules']) / 10, 1.0)
        
        return {
            "documentation": doc_coverage,
            "reusability": reusability,
            "validation_completeness": constraint_score,
            "complexity": 1.0 - metrics['complexity_score'],  # Lower complexity is better
            "maintainability": (doc_coverage + reusability + (1.0 - metrics['complexity_score'])) / 3
        }
</file>

<file path="additional_xml_handlers.py">
#!/usr/bin/env python3
"""
Additional XML Handlers for Common Document Types

This module extends the specialized handler system with more document types
commonly found in enterprise environments.
"""

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis
import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
from datetime import datetime

class MavenPOMHandler(XMLHandler):
    """Handler for Maven Project Object Model (POM) files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check if root is 'project' and has Maven namespace
        if root.tag == 'project' or root.tag.endswith('}project'):
            if 'maven.apache.org' in str(namespaces.values()):
                return True, 1.0
            # Even without namespace, if it has Maven-like structure
            if root.find('.//groupId') is not None and root.find('.//artifactId') is not None:
                return True, 0.8
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        pom_version = root.find('.//modelVersion')
        version = pom_version.text if pom_version is not None else "4.0.0"
        
        return DocumentTypeInfo(
            type_name="Maven POM",
            confidence=1.0,
            version=version,
            schema_uri="http://maven.apache.org/POM/4.0.0",
            metadata={
                "build_tool": "Maven",
                "category": "build_configuration"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'project_info': self._extract_project_info(root),
            'dependencies': self._analyze_dependencies(root),
            'plugins': self._analyze_plugins(root),
            'repositories': self._extract_repositories(root),
            'properties': self._extract_properties(root)
        }
        
        recommendations = [
            "Analyze dependency tree for security vulnerabilities",
            "Check for outdated dependencies",
            "Extract for software composition analysis",
            "Monitor for license compliance"
        ]
        
        ai_use_cases = [
            "Dependency vulnerability detection",
            "License compliance checking",
            "Technical debt analysis",
            "Build optimization recommendations",
            "Dependency update suggestions"
        ]
        
        data_inventory = {
            'dependencies': len(findings['dependencies']['all']),
            'plugins': len(findings['plugins']),
            'properties': len(findings['properties'])
        }
        
        return SpecializedAnalysis(
            document_type="Maven POM",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=data_inventory,
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_pom_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'coordinates': {
                'groupId': getattr(root.find('.//groupId'), 'text', None),
                'artifactId': getattr(root.find('.//artifactId'), 'text', None),
                'version': getattr(root.find('.//version'), 'text', None),
                'packaging': getattr(root.find('.//packaging'), 'text', 'jar')
            },
            'dependencies': self._extract_dependency_list(root),
            'build_config': self._extract_build_config(root)
        }
    
    def _extract_project_info(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'name': getattr(root.find('.//name'), 'text', None),
            'description': getattr(root.find('.//description'), 'text', None),
            'url': getattr(root.find('.//url'), 'text', None),
            'parent': self._extract_parent_info(root)
        }
    
    def _analyze_dependencies(self, root: ET.Element) -> Dict[str, Any]:
        deps = root.findall('.//dependency')
        
        scopes = {}
        for dep in deps:
            scope = getattr(dep.find('.//scope'), 'text', 'compile')
            scopes[scope] = scopes.get(scope, 0) + 1
        
        return {
            'all': [self._extract_dependency(d) for d in deps],
            'count': len(deps),
            'by_scope': scopes,
            'management': len(root.findall('.//dependencyManagement//dependency'))
        }
    
    def _analyze_plugins(self, root: ET.Element) -> List[Dict[str, str]]:
        plugins = []
        for plugin in root.findall('.//plugin'):
            plugins.append({
                'groupId': getattr(plugin.find('.//groupId'), 'text', 'org.apache.maven.plugins'),
                'artifactId': getattr(plugin.find('.//artifactId'), 'text', None),
                'version': getattr(plugin.find('.//version'), 'text', None)
            })
        return plugins
    
    def _extract_repositories(self, root: ET.Element) -> List[Dict[str, str]]:
        repos = []
        for repo in root.findall('.//repository'):
            repos.append({
                'id': getattr(repo.find('.//id'), 'text', None),
                'url': getattr(repo.find('.//url'), 'text', None)
            })
        return repos
    
    def _extract_properties(self, root: ET.Element) -> Dict[str, str]:
        props = {}
        properties = root.find('.//properties')
        if properties is not None:
            for prop in properties:
                props[prop.tag] = prop.text
        return props
    
    def _extract_parent_info(self, root: ET.Element) -> Optional[Dict[str, str]]:
        parent = root.find('.//parent')
        if parent is None:
            return None
        return {
            'groupId': getattr(parent.find('.//groupId'), 'text', None),
            'artifactId': getattr(parent.find('.//artifactId'), 'text', None),
            'version': getattr(parent.find('.//version'), 'text', None)
        }
    
    def _extract_dependency(self, dep: ET.Element) -> Dict[str, str]:
        return {
            'groupId': getattr(dep.find('.//groupId'), 'text', None),
            'artifactId': getattr(dep.find('.//artifactId'), 'text', None),
            'version': getattr(dep.find('.//version'), 'text', None),
            'scope': getattr(dep.find('.//scope'), 'text', 'compile')
        }
    
    def _extract_dependency_list(self, root: ET.Element) -> List[Dict[str, str]]:
        return [self._extract_dependency(d) for d in root.findall('.//dependency')[:20]]
    
    def _extract_build_config(self, root: ET.Element) -> Dict[str, Any]:
        build = root.find('.//build')
        if build is None:
            return {}
        
        return {
            'sourceDirectory': getattr(build.find('.//sourceDirectory'), 'text', None),
            'outputDirectory': getattr(build.find('.//outputDirectory'), 'text', None),
            'finalName': getattr(build.find('.//finalName'), 'text', None)
        }
    
    def _calculate_pom_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        has_description = 1.0 if findings['project_info']['description'] else 0.0
        has_url = 1.0 if findings['project_info']['url'] else 0.0
        deps_with_version = sum(1 for d in findings['dependencies']['all'] if d.get('version'))
        total_deps = len(findings['dependencies']['all'])
        
        return {
            "completeness": (has_description + has_url) / 2,
            "dependency_management": deps_with_version / total_deps if total_deps > 0 else 1.0,
            "best_practices": 0.8 if findings['dependencies']['management'] > 0 else 0.4
        }

class Log4jConfigHandler(XMLHandler):
    """Handler for Log4j XML configuration files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Log4j 1.x uses 'log4j:configuration'
        if root.tag == 'log4j:configuration' or root.tag.endswith('}configuration'):
            if 'log4j' in root.tag:
                return True, 1.0
        # Log4j 2.x uses 'Configuration'
        if root.tag == 'Configuration':
            if root.find('.//Appenders') is not None or root.find('.//Loggers') is not None:
                return True, 0.9
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        version = "2.x" if root.tag == 'Configuration' else "1.x"
        
        return DocumentTypeInfo(
            type_name="Log4j Configuration",
            confidence=1.0,
            version=version,
            metadata={
                "framework": "Apache Log4j",
                "category": "logging_configuration"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        is_v2 = root.tag == 'Configuration'
        
        findings = {
            'version': "2.x" if is_v2 else "1.x",
            'appenders': self._analyze_appenders(root, is_v2),
            'loggers': self._analyze_loggers(root, is_v2),
            'log_levels': self._extract_log_levels(root, is_v2),
            'security_concerns': self._check_security_issues(root)
        }
        
        recommendations = [
            "Review log levels for production appropriateness",
            "Check for sensitive data in log patterns",
            "Ensure file appenders have proper rotation",
            "Validate external appender destinations"
        ]
        
        ai_use_cases = [
            "Log level optimization",
            "Security configuration analysis",
            "Performance impact assessment",
            "Compliance checking for log retention",
            "Sensitive data detection in patterns"
        ]
        
        return SpecializedAnalysis(
            document_type="Log4j Configuration",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'appenders': len(findings['appenders']),
                'loggers': len(findings['loggers'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_logging_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        is_v2 = root.tag == 'Configuration'
        
        return {
            'appender_configs': self._extract_appender_details(root, is_v2),
            'logger_configs': self._extract_logger_details(root, is_v2),
            'global_settings': self._extract_global_settings(root, is_v2)
        }
    
    def _analyze_appenders(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        appenders = []
        
        if is_v2:
            for appender in root.findall('.//Appenders/*'):
                appenders.append({
                    'name': appender.get('name'),
                    'type': appender.tag,
                    'target': self._extract_v2_target(appender)
                })
        else:
            for appender in root.findall('.//appender'):
                appenders.append({
                    'name': appender.get('name'),
                    'class': appender.get('class'),
                    'type': self._extract_v1_type(appender.get('class', ''))
                })
        
        return appenders
    
    def _analyze_loggers(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        loggers = []
        
        if is_v2:
            for logger in root.findall('.//Loggers/*'):
                loggers.append({
                    'name': logger.get('name', 'ROOT' if logger.tag == 'Root' else ''),
                    'level': logger.get('level'),
                    'additivity': logger.get('additivity', 'true')
                })
        else:
            for logger in root.findall('.//logger'):
                loggers.append({
                    'name': logger.get('name'),
                    'level': logger.find('.//level').get('value') if logger.find('.//level') is not None else None
                })
        
        return loggers
    
    def _extract_log_levels(self, root: ET.Element, is_v2: bool) -> Dict[str, int]:
        levels = {}
        
        if is_v2:
            for elem in root.findall('.//*[@level]'):
                level = elem.get('level').upper()
                levels[level] = levels.get(level, 0) + 1
        else:
            for level_elem in root.findall('.//level'):
                level = level_elem.get('value', '').upper()
                if level:
                    levels[level] = levels.get(level, 0) + 1
        
        return levels
    
    def _check_security_issues(self, root: ET.Element) -> List[str]:
        issues = []
        
        # Check for JNDI lookup patterns (Log4Shell vulnerability)
        for elem in root.iter():
            if elem.text and '${jndi:' in elem.text:
                issues.append("Potential JNDI lookup pattern detected")
        
        # Check for external socket appenders
        for appender in root.findall('.//appender'):
            if 'SocketAppender' in appender.get('class', ''):
                issues.append("External socket appender detected")
        
        return issues
    
    def _extract_v2_target(self, appender: ET.Element) -> Optional[str]:
        if appender.tag == 'File':
            return appender.get('fileName')
        elif appender.tag == 'Console':
            return appender.get('target', 'SYSTEM_OUT')
        return None
    
    def _extract_v1_type(self, class_name: str) -> str:
        if 'ConsoleAppender' in class_name:
            return 'Console'
        elif 'FileAppender' in class_name:
            return 'File'
        elif 'RollingFileAppender' in class_name:
            return 'RollingFile'
        return 'Other'
    
    def _extract_appender_details(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        # Detailed implementation would extract full appender configurations
        return self._analyze_appenders(root, is_v2)
    
    def _extract_logger_details(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        # Detailed implementation would extract full logger configurations
        return self._analyze_loggers(root, is_v2)
    
    def _extract_global_settings(self, root: ET.Element, is_v2: bool) -> Dict[str, Any]:
        settings = {}
        
        if is_v2:
            settings['status'] = root.get('status', 'ERROR')
            settings['monitorInterval'] = root.get('monitorInterval')
        else:
            settings['threshold'] = root.get('threshold')
            settings['debug'] = root.get('debug', 'false')
        
        return settings
    
    def _assess_logging_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        # Check for good practices
        has_file_rotation = any('Rolling' in a.get('type', '') for a in findings['appenders'])
        has_appropriate_levels = 'DEBUG' not in findings['log_levels'] or findings['log_levels'].get('DEBUG', 0) < 5
        no_security_issues = len(findings['security_concerns']) == 0
        
        return {
            "security": 1.0 if no_security_issues else 0.3,
            "production_ready": 1.0 if has_appropriate_levels else 0.5,
            "reliability": 1.0 if has_file_rotation else 0.6
        }

class SpringConfigHandler(XMLHandler):
    """Handler for Spring Framework XML configuration files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for Spring namespaces
        spring_indicators = [
            'springframework.org/schema/beans',
            'springframework.org/schema/context',
            'springframework.org/schema/mvc'
        ]
        
        if any(ind in str(namespaces.values()) for ind in spring_indicators):
            return True, 1.0
        
        # Check for beans root element
        if root.tag == 'beans' or root.tag.endswith('}beans'):
            return True, 0.7
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Detect Spring version from schema
        version = "5.x"  # Default
        for uri in namespaces.values():
            if 'springframework.org/schema' in uri:
                version_match = re.search(r'/(\d+\.\d+)\.xsd', uri)
                if version_match:
                    version = version_match.group(1)
                    break
        
        return DocumentTypeInfo(
            type_name="Spring Configuration",
            confidence=1.0,
            version=version,
            metadata={
                "framework": "Spring Framework",
                "category": "dependency_injection"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'beans': self._analyze_beans(root),
            'profiles': self._extract_profiles(root),
            'imports': self._extract_imports(root),
            'property_sources': self._extract_property_sources(root),
            'aop_config': self._check_aop_usage(root),
            'security_config': self._check_security_config(root)
        }
        
        recommendations = [
            "Review bean dependencies for circular references",
            "Check for hardcoded values that should be externalized",
            "Validate security configurations",
            "Consider migrating to annotation-based config"
        ]
        
        ai_use_cases = [
            "Dependency graph visualization",
            "Security misconfiguration detection",
            "Migration to modern Spring Boot",
            "Configuration optimization",
            "Circular dependency detection"
        ]
        
        return SpecializedAnalysis(
            document_type="Spring Configuration",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'beans': len(findings['beans']['all']),
                'profiles': len(findings['profiles']),
                'property_sources': len(findings['property_sources'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_spring_config_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'bean_definitions': self._extract_bean_definitions(root),
            'component_scans': self._extract_component_scans(root),
            'configurations': self._extract_configurations(root)
        }
    
    def _analyze_beans(self, root: ET.Element, namespaces: Dict[str, str] = None) -> Dict[str, Any]:
        beans = []
        bean_classes = {}
        
        for bean in root.findall('.//*[@id]'):
            if bean.tag.endswith('bean') or bean.tag == 'bean':
                bean_info = {
                    'id': bean.get('id'),
                    'class': bean.get('class'),
                    'scope': bean.get('scope', 'singleton'),
                    'lazy': bean.get('lazy-init', 'false'),
                    'parent': bean.get('parent')
                }
                beans.append(bean_info)
                
                # Count bean classes
                if bean_info['class']:
                    bean_classes[bean_info['class']] = bean_classes.get(bean_info['class'], 0) + 1
        
        return {
            'all': beans,
            'count': len(beans),
            'by_scope': self._count_by_attribute(beans, 'scope'),
            'lazy_count': sum(1 for b in beans if b['lazy'] == 'true'),
            'common_classes': {k: v for k, v in bean_classes.items() if v > 1}
        }
    
    def _extract_profiles(self, root: ET.Element) -> List[str]:
        profiles = set()
        
        for elem in root.findall('.//*[@profile]'):
            profile = elem.get('profile')
            if profile:
                # Handle multiple profiles
                for p in profile.split(','):
                    profiles.add(p.strip())
        
        return list(profiles)
    
    def _extract_imports(self, root: ET.Element) -> List[str]:
        imports = []
        
        for imp in root.findall('.//import'):
            resource = imp.get('resource')
            if resource:
                imports.append(resource)
        
        return imports
    
    def _extract_property_sources(self, root: ET.Element) -> List[Dict[str, str]]:
        sources = []
        
        # Look for property placeholder configurers
        for elem in root.findall('.//*'):
            if 'PropertyPlaceholderConfigurer' in elem.get('class', ''):
                location = elem.find('.//property[@name="location"]')
                if location is not None:
                    sources.append({
                        'type': 'properties',
                        'location': location.get('value')
                    })
        
        return sources
    
    def _check_aop_usage(self, root: ET.Element) -> bool:
        # Check for AOP namespace or AOP-related beans
        for elem in root.iter():
            if 'aop' in elem.tag or 'aspectj' in elem.tag.lower():
                return True
        return False
    
    def _check_security_config(self, root: ET.Element) -> Dict[str, Any]:
        security = {
            'present': False,
            'authentication': False,
            'authorization': False
        }
        
        for elem in root.iter():
            if 'security' in elem.tag:
                security['present'] = True
            if 'authentication' in elem.tag:
                security['authentication'] = True
            if 'authorization' in elem.tag or 'access' in elem.tag:
                security['authorization'] = True
        
        return security
    
    def _extract_bean_definitions(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Simplified version - full implementation would extract all properties
        return self._analyze_beans(root)['all'][:20]  # First 20 beans
    
    def _extract_component_scans(self, root: ET.Element) -> List[str]:
        scans = []
        
        for scan in root.findall('.//*component-scan'):
            base_package = scan.get('base-package')
            if base_package:
                scans.append(base_package)
        
        return scans
    
    def _extract_configurations(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'transaction_management': self._check_transaction_config(root),
            'caching': self._check_cache_config(root),
            'scheduling': self._check_scheduling_config(root)
        }
    
    def _check_transaction_config(self, root: ET.Element) -> bool:
        return any('transaction' in elem.tag for elem in root.iter())
    
    def _check_cache_config(self, root: ET.Element) -> bool:
        return any('cache' in elem.tag for elem in root.iter())
    
    def _check_scheduling_config(self, root: ET.Element) -> bool:
        return any('task' in elem.tag or 'scheduling' in elem.tag for elem in root.iter())
    
    def _count_by_attribute(self, items: List[Dict], attr: str) -> Dict[str, int]:
        counts = {}
        for item in items:
            value = item.get(attr)
            if value:
                counts[value] = counts.get(value, 0) + 1
        return counts
    
    def _assess_spring_config_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        # Assess configuration quality
        beans = findings['beans']
        
        # Check for good practices
        uses_profiles = len(findings['profiles']) > 0
        externalizes_config = len(findings['property_sources']) > 0
        reasonable_bean_count = beans['count'] < 100  # Large XML configs are hard to maintain
        
        return {
            "maintainability": 0.8 if reasonable_bean_count else 0.3,
            "flexibility": 1.0 if uses_profiles else 0.5,
            "configuration_management": 1.0 if externalizes_config else 0.4
        }

class DocBookHandler(XMLHandler):
    """Handler for DocBook XML documentation files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for DocBook elements
        docbook_roots = ['book', 'article', 'chapter', 'section', 'para']
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if tag in docbook_roots:
            return True, 0.8
        
        # Check for DocBook namespace
        if any('docbook.org' in uri for uri in namespaces.values()):
            return True, 1.0
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        return DocumentTypeInfo(
            type_name="DocBook Documentation",
            confidence=0.9,
            version="5.0",  # Assume 5.0 unless specified
            metadata={
                "document_type": tag,
                "category": "technical_documentation"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'structure': self._analyze_document_structure(root),
            'metadata': self._extract_metadata(root),
            'content_stats': self._analyze_content(root),
            'media': self._find_media_references(root),
            'cross_references': self._find_cross_references(root)
        }
        
        recommendations = [
            "Extract for documentation search system",
            "Generate multiple output formats (HTML, PDF)",
            "Check for broken cross-references",
            "Analyze readability and completeness"
        ]
        
        ai_use_cases = [
            "Documentation quality analysis",
            "Automatic summary generation",
            "Technical content extraction",
            "Glossary and index generation",
            "Documentation translation"
        ]
        
        return SpecializedAnalysis(
            document_type="DocBook Documentation",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'chapters': len(findings['structure']['chapters']),
                'sections': findings['structure']['total_sections'],
                'media_items': len(findings['media'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_documentation_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'title': self._extract_title(root),
            'table_of_contents': self._generate_toc(root),
            'glossary': self._extract_glossary(root),
            'code_examples': self._extract_code_examples(root)
        }
    
    def _analyze_document_structure(self, root: ET.Element) -> Dict[str, Any]:
        structure = {
            'type': root.tag.split('}')[-1] if '}' in root.tag else root.tag,
            'chapters': [],
            'total_sections': 0,
            'max_depth': 0
        }
        
        # Find chapters
        for chapter in root.findall('.//chapter'):
            title_elem = chapter.find('.//title')
            structure['chapters'].append({
                'title': title_elem.text if title_elem is not None else 'Untitled',
                'sections': len(chapter.findall('.//section'))
            })
        
        # Count all sections
        structure['total_sections'] = len(root.findall('.//section'))
        
        return structure
    
    def _extract_metadata(self, root: ET.Element) -> Dict[str, Any]:
        info = root.find('.//info') or root.find('.//bookinfo') or root.find('.//articleinfo')
        
        if info is None:
            return {}
        
        return {
            'title': getattr(info.find('.//title'), 'text', None),
            'author': self._extract_author(info),
            'date': getattr(info.find('.//date'), 'text', None),
            'abstract': getattr(info.find('.//abstract'), 'text', None)
        }
    
    def _analyze_content(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'paragraphs': len(root.findall('.//para')),
            'lists': len(root.findall('.//itemizedlist')) + len(root.findall('.//orderedlist')),
            'tables': len(root.findall('.//table')),
            'examples': len(root.findall('.//example')),
            'notes': len(root.findall('.//note')),
            'warnings': len(root.findall('.//warning'))
        }
    
    def _find_media_references(self, root: ET.Element) -> List[Dict[str, str]]:
        media = []
        
        for elem in root.findall('.//imagedata'):
            media.append({
                'type': 'image',
                'fileref': elem.get('fileref'),
                'format': elem.get('format')
            })
        
        return media
    
    def _find_cross_references(self, root: ET.Element) -> List[str]:
        xrefs = []
        
        for xref in root.findall('.//xref'):
            linkend = xref.get('linkend')
            if linkend:
                xrefs.append(linkend)
        
        return xrefs
    
    def _extract_title(self, root: ET.Element) -> str:
        title = root.find('.//title')
        return title.text if title is not None else 'Untitled'
    
    def _generate_toc(self, root: ET.Element) -> List[Dict[str, Any]]:
        toc = []
        
        for chapter in root.findall('.//chapter'):
            chapter_title = chapter.find('.//title')
            if chapter_title is not None:
                toc_entry = {
                    'title': chapter_title.text,
                    'sections': []
                }
                
                for section in chapter.findall('.//section'):
                    section_title = section.find('.//title')
                    if section_title is not None:
                        toc_entry['sections'].append(section_title.text)
                
                toc.append(toc_entry)
        
        return toc
    
    def _extract_glossary(self, root: ET.Element) -> List[Dict[str, str]]:
        glossary = []
        
        for entry in root.findall('.//glossentry'):
            term = entry.find('.//glossterm')
            definition = entry.find('.//glossdef')
            
            if term is not None and definition is not None:
                glossary.append({
                    'term': term.text,
                    'definition': definition.text
                })
        
        return glossary
    
    def _extract_code_examples(self, root: ET.Element) -> List[Dict[str, str]]:
        examples = []
        
        for example in root.findall('.//programlisting'):
            examples.append({
                'language': example.get('language', 'unknown'),
                'code': example.text if example.text else ''
            })
        
        return examples[:10]  # First 10 examples
    
    def _extract_author(self, info: ET.Element) -> Optional[str]:
        author = info.find('.//author')
        if author is not None:
            firstname = author.find('.//firstname')
            surname = author.find('.//surname')
            if firstname is not None and surname is not None:
                return f"{firstname.text} {surname.text}"
        return None
    
    def _assess_documentation_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        content = findings['content_stats']
        
        # Check for good documentation practices
        has_examples = content['examples'] > 0
        has_structure = findings['structure']['total_sections'] > 0
        has_metadata = bool(findings['metadata'])
        
        # Calculate ratios
        example_ratio = min(content['examples'] / max(findings['structure']['total_sections'], 1), 1.0)
        warning_ratio = content['warnings'] / max(content['paragraphs'], 1)
        
        return {
            "completeness": (has_examples + has_structure + has_metadata) / 3,
            "example_coverage": example_ratio,
            "safety_documentation": min(warning_ratio * 10, 1.0)  # Good to have some warnings
        }

class SitemapHandler(XMLHandler):
    """Handler for XML sitemap files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for sitemap namespace
        if 'sitemaps.org/schemas/sitemap' in str(namespaces.values()):
            return True, 1.0
        
        # Check for urlset or sitemapindex root
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        if tag in ['urlset', 'sitemapindex']:
            return True, 0.8
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        is_index = tag == 'sitemapindex'
        
        return DocumentTypeInfo(
            type_name="XML Sitemap" + (" Index" if is_index else ""),
            confidence=1.0,
            version="0.9",
            schema_uri="http://www.sitemaps.org/schemas/sitemap/0.9",
            metadata={
                "type": "sitemap_index" if is_index else "url_sitemap",
                "category": "seo"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        is_index = tag == 'sitemapindex'
        
        if is_index:
            findings = self._analyze_sitemap_index(root)
        else:
            findings = self._analyze_url_sitemap(root)
        
        recommendations = [
            "Validate URLs for accessibility",
            "Check for outdated or broken links",
            "Analyze URL patterns for SEO optimization",
            "Monitor change frequencies vs actual updates"
        ]
        
        ai_use_cases = [
            "SEO health monitoring",
            "Content update pattern analysis",
            "Website structure visualization",
            "Broken link detection",
            "Content priority optimization"
        ]
        
        return SpecializedAnalysis(
            document_type="XML Sitemap",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={'urls': findings.get('url_count', 0)},
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_sitemap_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if tag == 'sitemapindex':
            return {
                'sitemaps': self._extract_sitemaps(root)
            }
        else:
            return {
                'urls': self._extract_urls(root)[:100]  # First 100 URLs
            }
    
    def _analyze_url_sitemap(self, root: ET.Element) -> Dict[str, Any]:
        urls = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url')
        
        findings = {
            'url_count': len(urls),
            'priorities': self._analyze_priorities(urls),
            'change_frequencies': self._analyze_changefreqs(urls),
            'last_modified': self._analyze_lastmod(urls),
            'url_patterns': self._analyze_url_patterns(urls)
        }
        
        return findings
    
    def _analyze_sitemap_index(self, root: ET.Element) -> Dict[str, Any]:
        sitemaps = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap')
        
        findings = {
            'sitemap_count': len(sitemaps),
            'last_modified': self._analyze_sitemap_dates(sitemaps)
        }
        
        return findings
    
    def _analyze_priorities(self, urls: List[ET.Element]) -> Dict[str, int]:
        priorities = {}
        
        for url in urls:
            priority = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}priority')
            if priority is not None and priority.text:
                p_value = priority.text
                priorities[p_value] = priorities.get(p_value, 0) + 1
        
        return priorities
    
    def _analyze_changefreqs(self, urls: List[ET.Element]) -> Dict[str, int]:
        frequencies = {}
        
        for url in urls:
            changefreq = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}changefreq')
            if changefreq is not None and changefreq.text:
                freq = changefreq.text
                frequencies[freq] = frequencies.get(freq, 0) + 1
        
        return frequencies
    
    def _analyze_lastmod(self, urls: List[ET.Element]) -> Dict[str, Any]:
        dates = []
        
        for url in urls:
            lastmod = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod')
            if lastmod is not None and lastmod.text:
                dates.append(lastmod.text)
        
        if dates:
            return {
                'count': len(dates),
                'latest': max(dates),
                'oldest': min(dates)
            }
        
        return {'count': 0}
    
    def _analyze_url_patterns(self, urls: List[ET.Element]) -> Dict[str, Any]:
        patterns = {
            'domains': set(),
            'extensions': {},
            'depth_levels': {}
        }
        
        for url in urls[:1000]:  # Analyze first 1000 URLs
            loc = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
            if loc is not None and loc.text:
                url_text = loc.text
                
                # Extract domain
                if '://' in url_text:
                    domain = url_text.split('://')[1].split('/')[0]
                    patterns['domains'].add(domain)
                
                # Count depth
                depth = url_text.count('/') - 2  # Subtract protocol slashes
                patterns['depth_levels'][depth] = patterns['depth_levels'].get(depth, 0) + 1
        
        patterns['domains'] = list(patterns['domains'])
        return patterns
    
    def _analyze_sitemap_dates(self, sitemaps: List[ET.Element]) -> Dict[str, Any]:
        dates = []
        
        for sitemap in sitemaps:
            lastmod = sitemap.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod')
            if lastmod is not None and lastmod.text:
                dates.append(lastmod.text)
        
        if dates:
            return {
                'latest': max(dates),
                'oldest': min(dates)
            }
        
        return {}
    
    def _extract_urls(self, root: ET.Element) -> List[Dict[str, str]]:
        urls = []
        
        for url in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'):
            url_data = {}
            
            loc = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
            if loc is not None:
                url_data['loc'] = loc.text
            
            lastmod = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod')
            if lastmod is not None:
                url_data['lastmod'] = lastmod.text
            
            changefreq = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}changefreq')
            if changefreq is not None:
                url_data['changefreq'] = changefreq.text
            
            priority = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}priority')
            if priority is not None:
                url_data['priority'] = priority.text
            
            urls.append(url_data)
        
        return urls
    
    def _extract_sitemaps(self, root: ET.Element) -> List[Dict[str, str]]:
        sitemaps = []
        
        for sitemap in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap'):
            sitemap_data = {}
            
            loc = sitemap.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
            if loc is not None:
                sitemap_data['loc'] = loc.text
            
            lastmod = sitemap.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod')
            if lastmod is not None:
                sitemap_data['lastmod'] = lastmod.text
            
            sitemaps.append(sitemap_data)
        
        return sitemaps
    
    def _assess_sitemap_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        quality = {}
        
        # Check if URLs have recommended attributes
        url_count = findings.get('url_count', 0)
        if url_count > 0:
            has_priority = sum(findings.get('priorities', {}).values())
            has_changefreq = sum(findings.get('change_frequencies', {}).values())
            has_lastmod = findings.get('last_modified', {}).get('count', 0)
            
            quality['completeness'] = (
                (has_priority / url_count * 0.3) +
                (has_changefreq / url_count * 0.3) +
                (has_lastmod / url_count * 0.4)
            )
        else:
            quality['completeness'] = 0.0
        
        # Check URL diversity
        if 'url_patterns' in findings:
            depth_distribution = len(findings['url_patterns'].get('depth_levels', {}))
            quality['structure_diversity'] = min(depth_distribution / 5.0, 1.0)
        
        return quality

# Additional handler classes can be added here:
# - WSDLHandler (for web service definitions)
# - XSLTHandler (for transformation stylesheets)
# - KMLHandler (for geographic data)
# - XMLSchemaHandler (for XSD files)
# - Ant/NantBuildHandler (for build scripts)
# - NuGetHandler (for .nuspec files)
# - etc.
</file>

<file path="additional-xml-handlers_OLD.py">
#!/usr/bin/env python3
"""
Additional XML Handlers for Common Document Types

This module extends the specialized handler system with more document types
commonly found in enterprise environments.
"""

from xml_specialized_handlers import XMLHandler, DocumentTypeInfo, SpecializedAnalysis
import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Any, Tuple
import re
from datetime import datetime

class MavenPOMHandler(XMLHandler):
    """Handler for Maven Project Object Model (POM) files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check if root is 'project' and has Maven namespace
        if root.tag == 'project' or root.tag.endswith('}project'):
            if 'maven.apache.org' in str(namespaces.values()):
                return True, 1.0
            # Even without namespace, if it has Maven-like structure
            if root.find('.//groupId') is not None and root.find('.//artifactId') is not None:
                return True, 0.8
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        pom_version = root.find('.//modelVersion')
        version = pom_version.text if pom_version is not None else "4.0.0"
        
        return DocumentTypeInfo(
            type_name="Maven POM",
            confidence=1.0,
            version=version,
            schema_uri="http://maven.apache.org/POM/4.0.0",
            metadata={
                "build_tool": "Maven",
                "category": "build_configuration"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'project_info': self._extract_project_info(root),
            'dependencies': self._analyze_dependencies(root),
            'plugins': self._analyze_plugins(root),
            'repositories': self._extract_repositories(root),
            'properties': self._extract_properties(root)
        }
        
        recommendations = [
            "Analyze dependency tree for security vulnerabilities",
            "Check for outdated dependencies",
            "Extract for software composition analysis",
            "Monitor for license compliance"
        ]
        
        ai_use_cases = [
            "Dependency vulnerability detection",
            "License compliance checking",
            "Technical debt analysis",
            "Build optimization recommendations",
            "Dependency update suggestions"
        ]
        
        data_inventory = {
            'dependencies': len(findings['dependencies']['all']),
            'plugins': len(findings['plugins']),
            'properties': len(findings['properties'])
        }
        
        return SpecializedAnalysis(
            document_type="Maven POM",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=data_inventory,
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_pom_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'coordinates': {
                'groupId': getattr(root.find('.//groupId'), 'text', None),
                'artifactId': getattr(root.find('.//artifactId'), 'text', None),
                'version': getattr(root.find('.//version'), 'text', None),
                'packaging': getattr(root.find('.//packaging'), 'text', 'jar')
            },
            'dependencies': self._extract_dependency_list(root),
            'build_config': self._extract_build_config(root)
        }
    
    def _extract_project_info(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'name': getattr(root.find('.//name'), 'text', None),
            'description': getattr(root.find('.//description'), 'text', None),
            'url': getattr(root.find('.//url'), 'text', None),
            'parent': self._extract_parent_info(root)
        }
    
    def _analyze_dependencies(self, root: ET.Element) -> Dict[str, Any]:
        deps = root.findall('.//dependency')
        
        scopes = {}
        for dep in deps:
            scope = getattr(dep.find('.//scope'), 'text', 'compile')
            scopes[scope] = scopes.get(scope, 0) + 1
        
        return {
            'all': [self._extract_dependency(d) for d in deps],
            'count': len(deps),
            'by_scope': scopes,
            'management': len(root.findall('.//dependencyManagement//dependency'))
        }
    
    def _analyze_plugins(self, root: ET.Element) -> List[Dict[str, str]]:
        plugins = []
        for plugin in root.findall('.//plugin'):
            plugins.append({
                'groupId': getattr(plugin.find('.//groupId'), 'text', 'org.apache.maven.plugins'),
                'artifactId': getattr(plugin.find('.//artifactId'), 'text', None),
                'version': getattr(plugin.find('.//version'), 'text', None)
            })
        return plugins
    
    def _extract_repositories(self, root: ET.Element) -> List[Dict[str, str]]:
        repos = []
        for repo in root.findall('.//repository'):
            repos.append({
                'id': getattr(repo.find('.//id'), 'text', None),
                'url': getattr(repo.find('.//url'), 'text', None)
            })
        return repos
    
    def _extract_properties(self, root: ET.Element) -> Dict[str, str]:
        props = {}
        properties = root.find('.//properties')
        if properties is not None:
            for prop in properties:
                props[prop.tag] = prop.text
        return props
    
    def _extract_parent_info(self, root: ET.Element) -> Optional[Dict[str, str]]:
        parent = root.find('.//parent')
        if parent is None:
            return None
        return {
            'groupId': getattr(parent.find('.//groupId'), 'text', None),
            'artifactId': getattr(parent.find('.//artifactId'), 'text', None),
            'version': getattr(parent.find('.//version'), 'text', None)
        }
    
    def _extract_dependency(self, dep: ET.Element) -> Dict[str, str]:
        return {
            'groupId': getattr(dep.find('.//groupId'), 'text', None),
            'artifactId': getattr(dep.find('.//artifactId'), 'text', None),
            'version': getattr(dep.find('.//version'), 'text', None),
            'scope': getattr(dep.find('.//scope'), 'text', 'compile')
        }
    
    def _extract_dependency_list(self, root: ET.Element) -> List[Dict[str, str]]:
        return [self._extract_dependency(d) for d in root.findall('.//dependency')[:20]]
    
    def _extract_build_config(self, root: ET.Element) -> Dict[str, Any]:
        build = root.find('.//build')
        if build is None:
            return {}
        
        return {
            'sourceDirectory': getattr(build.find('.//sourceDirectory'), 'text', None),
            'outputDirectory': getattr(build.find('.//outputDirectory'), 'text', None),
            'finalName': getattr(build.find('.//finalName'), 'text', None)
        }
    
    def _calculate_pom_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        has_description = 1.0 if findings['project_info']['description'] else 0.0
        has_url = 1.0 if findings['project_info']['url'] else 0.0
        deps_with_version = sum(1 for d in findings['dependencies']['all'] if d.get('version'))
        total_deps = len(findings['dependencies']['all'])
        
        return {
            "completeness": (has_description + has_url) / 2,
            "dependency_management": deps_with_version / total_deps if total_deps > 0 else 1.0,
            "best_practices": 0.8 if findings['dependencies']['management'] > 0 else 0.4
        }

class Log4jConfigHandler(XMLHandler):
    """Handler for Log4j XML configuration files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Log4j 1.x uses 'log4j:configuration'
        if root.tag == 'log4j:configuration' or root.tag.endswith('}configuration'):
            if 'log4j' in root.tag:
                return True, 1.0
        # Log4j 2.x uses 'Configuration'
        if root.tag == 'Configuration':
            if root.find('.//Appenders') is not None or root.find('.//Loggers') is not None:
                return True, 0.9
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        version = "2.x" if root.tag == 'Configuration' else "1.x"
        
        return DocumentTypeInfo(
            type_name="Log4j Configuration",
            confidence=1.0,
            version=version,
            metadata={
                "framework": "Apache Log4j",
                "category": "logging_configuration"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        is_v2 = root.tag == 'Configuration'
        
        findings = {
            'version': "2.x" if is_v2 else "1.x",
            'appenders': self._analyze_appenders(root, is_v2),
            'loggers': self._analyze_loggers(root, is_v2),
            'log_levels': self._extract_log_levels(root, is_v2),
            'security_concerns': self._check_security_issues(root)
        }
        
        recommendations = [
            "Review log levels for production appropriateness",
            "Check for sensitive data in log patterns",
            "Ensure file appenders have proper rotation",
            "Validate external appender destinations"
        ]
        
        ai_use_cases = [
            "Log level optimization",
            "Security configuration analysis",
            "Performance impact assessment",
            "Compliance checking for log retention",
            "Sensitive data detection in patterns"
        ]
        
        return SpecializedAnalysis(
            document_type="Log4j Configuration",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'appenders': len(findings['appenders']),
                'loggers': len(findings['loggers'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_logging_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        is_v2 = root.tag == 'Configuration'
        
        return {
            'appender_configs': self._extract_appender_details(root, is_v2),
            'logger_configs': self._extract_logger_details(root, is_v2),
            'global_settings': self._extract_global_settings(root, is_v2)
        }
    
    def _analyze_appenders(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        appenders = []
        
        if is_v2:
            for appender in root.findall('.//Appenders/*'):
                appenders.append({
                    'name': appender.get('name'),
                    'type': appender.tag,
                    'target': self._extract_v2_target(appender)
                })
        else:
            for appender in root.findall('.//appender'):
                appenders.append({
                    'name': appender.get('name'),
                    'class': appender.get('class'),
                    'type': self._extract_v1_type(appender.get('class', ''))
                })
        
        return appenders
    
    def _analyze_loggers(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        loggers = []
        
        if is_v2:
            for logger in root.findall('.//Loggers/*'):
                loggers.append({
                    'name': logger.get('name', 'ROOT' if logger.tag == 'Root' else ''),
                    'level': logger.get('level'),
                    'additivity': logger.get('additivity', 'true')
                })
        else:
            for logger in root.findall('.//logger'):
                loggers.append({
                    'name': logger.get('name'),
                    'level': logger.find('.//level').get('value') if logger.find('.//level') is not None else None
                })
        
        return loggers
    
    def _extract_log_levels(self, root: ET.Element, is_v2: bool) -> Dict[str, int]:
        levels = {}
        
        if is_v2:
            for elem in root.findall('.//*[@level]'):
                level = elem.get('level').upper()
                levels[level] = levels.get(level, 0) + 1
        else:
            for level_elem in root.findall('.//level'):
                level = level_elem.get('value', '').upper()
                if level:
                    levels[level] = levels.get(level, 0) + 1
        
        return levels
    
    def _check_security_issues(self, root: ET.Element) -> List[str]:
        issues = []
        
        # Check for JNDI lookup patterns (Log4Shell vulnerability)
        for elem in root.iter():
            if elem.text and '${jndi:' in elem.text:
                issues.append("Potential JNDI lookup pattern detected")
        
        # Check for external socket appenders
        for appender in root.findall('.//appender'):
            if 'SocketAppender' in appender.get('class', ''):
                issues.append("External socket appender detected")
        
        return issues
    
    def _extract_v2_target(self, appender: ET.Element) -> Optional[str]:
        if appender.tag == 'File':
            return appender.get('fileName')
        elif appender.tag == 'Console':
            return appender.get('target', 'SYSTEM_OUT')
        return None
    
    def _extract_v1_type(self, class_name: str) -> str:
        if 'ConsoleAppender' in class_name:
            return 'Console'
        elif 'FileAppender' in class_name:
            return 'File'
        elif 'RollingFileAppender' in class_name:
            return 'RollingFile'
        return 'Other'
    
    def _extract_appender_details(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        # Detailed implementation would extract full appender configurations
        return self._analyze_appenders(root, is_v2)
    
    def _extract_logger_details(self, root: ET.Element, is_v2: bool) -> List[Dict[str, Any]]:
        # Detailed implementation would extract full logger configurations
        return self._analyze_loggers(root, is_v2)
    
    def _extract_global_settings(self, root: ET.Element, is_v2: bool) -> Dict[str, Any]:
        settings = {}
        
        if is_v2:
            settings['status'] = root.get('status', 'ERROR')
            settings['monitorInterval'] = root.get('monitorInterval')
        else:
            settings['threshold'] = root.get('threshold')
            settings['debug'] = root.get('debug', 'false')
        
        return settings
    
    def _assess_logging_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        # Check for good practices
        has_file_rotation = any('Rolling' in a.get('type', '') for a in findings['appenders'])
        has_appropriate_levels = 'DEBUG' not in findings['log_levels'] or findings['log_levels'].get('DEBUG', 0) < 5
        no_security_issues = len(findings['security_concerns']) == 0
        
        return {
            "security": 1.0 if no_security_issues else 0.3,
            "production_ready": 1.0 if has_appropriate_levels else 0.5,
            "reliability": 1.0 if has_file_rotation else 0.6
        }

class SpringConfigHandler(XMLHandler):
    """Handler for Spring Framework XML configuration files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for Spring namespaces
        spring_indicators = [
            'springframework.org/schema/beans',
            'springframework.org/schema/context',
            'springframework.org/schema/mvc'
        ]
        
        if any(ind in str(namespaces.values()) for ind in spring_indicators):
            return True, 1.0
        
        # Check for beans root element
        if root.tag == 'beans' or root.tag.endswith('}beans'):
            return True, 0.7
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Detect Spring version from schema
        version = "5.x"  # Default
        for uri in namespaces.values():
            if 'springframework.org/schema' in uri:
                version_match = re.search(r'/(\d+\.\d+)\.xsd', uri)
                if version_match:
                    version = version_match.group(1)
                    break
        
        return DocumentTypeInfo(
            type_name="Spring Configuration",
            confidence=1.0,
            version=version,
            metadata={
                "framework": "Spring Framework",
                "category": "dependency_injection"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'beans': self._analyze_beans(root),
            'profiles': self._extract_profiles(root),
            'imports': self._extract_imports(root),
            'property_sources': self._extract_property_sources(root),
            'aop_config': self._check_aop_usage(root),
            'security_config': self._check_security_config(root)
        }
        
        recommendations = [
            "Review bean dependencies for circular references",
            "Check for hardcoded values that should be externalized",
            "Validate security configurations",
            "Consider migrating to annotation-based config"
        ]
        
        ai_use_cases = [
            "Dependency graph visualization",
            "Security misconfiguration detection",
            "Migration to modern Spring Boot",
            "Configuration optimization",
            "Circular dependency detection"
        ]
        
        return SpecializedAnalysis(
            document_type="Spring Configuration",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'beans': len(findings['beans']['all']),
                'profiles': len(findings['profiles']),
                'property_sources': len(findings['property_sources'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_spring_config_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'bean_definitions': self._extract_bean_definitions(root),
            'component_scans': self._extract_component_scans(root),
            'configurations': self._extract_configurations(root)
        }
    
    def _analyze_beans(self, root: ET.Element, namespaces: Dict[str, str] = None) -> Dict[str, Any]:
        beans = []
        bean_classes = {}
        
        for bean in root.findall('.//*[@id]'):
            if bean.tag.endswith('bean') or bean.tag == 'bean':
                bean_info = {
                    'id': bean.get('id'),
                    'class': bean.get('class'),
                    'scope': bean.get('scope', 'singleton'),
                    'lazy': bean.get('lazy-init', 'false'),
                    'parent': bean.get('parent')
                }
                beans.append(bean_info)
                
                # Count bean classes
                if bean_info['class']:
                    bean_classes[bean_info['class']] = bean_classes.get(bean_info['class'], 0) + 1
        
        return {
            'all': beans,
            'count': len(beans),
            'by_scope': self._count_by_attribute(beans, 'scope'),
            'lazy_count': sum(1 for b in beans if b['lazy'] == 'true'),
            'common_classes': {k: v for k, v in bean_classes.items() if v > 1}
        }
    
    def _extract_profiles(self, root: ET.Element) -> List[str]:
        profiles = set()
        
        for elem in root.findall('.//*[@profile]'):
            profile = elem.get('profile')
            if profile:
                # Handle multiple profiles
                for p in profile.split(','):
                    profiles.add(p.strip())
        
        return list(profiles)
    
    def _extract_imports(self, root: ET.Element) -> List[str]:
        imports = []
        
        for imp in root.findall('.//import'):
            resource = imp.get('resource')
            if resource:
                imports.append(resource)
        
        return imports
    
    def _extract_property_sources(self, root: ET.Element) -> List[Dict[str, str]]:
        sources = []
        
        # Look for property placeholder configurers
        for elem in root.findall('.//*'):
            if 'PropertyPlaceholderConfigurer' in elem.get('class', ''):
                location = elem.find('.//property[@name="location"]')
                if location is not None:
                    sources.append({
                        'type': 'properties',
                        'location': location.get('value')
                    })
        
        return sources
    
    def _check_aop_usage(self, root: ET.Element) -> bool:
        # Check for AOP namespace or AOP-related beans
        for elem in root.iter():
            if 'aop' in elem.tag or 'aspectj' in elem.tag.lower():
                return True
        return False
    
    def _check_security_config(self, root: ET.Element) -> Dict[str, Any]:
        security = {
            'present': False,
            'authentication': False,
            'authorization': False
        }
        
        for elem in root.iter():
            if 'security' in elem.tag:
                security['present'] = True
            if 'authentication' in elem.tag:
                security['authentication'] = True
            if 'authorization' in elem.tag or 'access' in elem.tag:
                security['authorization'] = True
        
        return security
    
    def _extract_bean_definitions(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Simplified version - full implementation would extract all properties
        return self._analyze_beans(root)['all'][:20]  # First 20 beans
    
    def _extract_component_scans(self, root: ET.Element) -> List[str]:
        scans = []
        
        for scan in root.findall('.//*component-scan'):
            base_package = scan.get('base-package')
            if base_package:
                scans.append(base_package)
        
        return scans
    
    def _extract_configurations(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'transaction_management': self._check_transaction_config(root),
            'caching': self._check_cache_config(root),
            'scheduling': self._check_scheduling_config(root)
        }
    
    def _check_transaction_config(self, root: ET.Element) -> bool:
        return any('transaction' in elem.tag for elem in root.iter())
    
    def _check_cache_config(self, root: ET.Element) -> bool:
        return any('cache' in elem.tag for elem in root.iter())
    
    def _check_scheduling_config(self, root: ET.Element) -> bool:
        return any('task' in elem.tag or 'scheduling' in elem.tag for elem in root.iter())
    
    def _count_by_attribute(self, items: List[Dict], attr: str) -> Dict[str, int]:
        counts = {}
        for item in items:
            value = item.get(attr)
            if value:
                counts[value] = counts.get(value, 0) + 1
        return counts
    
    def _assess_spring_config_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        # Assess configuration quality
        beans = findings['beans']
        
        # Check for good practices
        uses_profiles = len(findings['profiles']) > 0
        externalizes_config = len(findings['property_sources']) > 0
        reasonable_bean_count = beans['count'] < 100  # Large XML configs are hard to maintain
        
        return {
            "maintainability": 0.8 if reasonable_bean_count else 0.3,
            "flexibility": 1.0 if uses_profiles else 0.5,
            "configuration_management": 1.0 if externalizes_config else 0.4
        }

class DocBookHandler(XMLHandler):
    """Handler for DocBook XML documentation files"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for DocBook elements
        docbook_roots = ['book', 'article', 'chapter', 'section', 'para']
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        if tag in docbook_roots:
            return True, 0.8
        
        # Check for DocBook namespace
        if any('docbook.org' in uri for uri in namespaces.values()):
            return True, 1.0
        
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        return DocumentTypeInfo(
            type_name="DocBook Documentation",
            confidence=0.9,
            version="5.0",  # Assume 5.0 unless specified
            metadata={
                "document_type": tag,
                "category": "technical_documentation"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'structure': self._analyze_document_structure(root),
            'metadata': self._extract_metadata(root),
            'content_stats': self._analyze_content(root),
            'media': self._find_media_references(root),
            'cross_references': self._find_cross_references(root)
        }
        
        recommendations = [
            "Extract for documentation search system",
            "Generate multiple output formats (HTML, PDF)",
            "Check for broken cross-references",
            "Analyze readability and completeness"
        ]
        
        ai_use_cases = [
            "Documentation quality analysis",
            "Automatic summary generation",
            "Technical content extraction",
            "Glossary and index generation",
            "Documentation translation"
        ]
        
        return SpecializedAnalysis(
            document_type="DocBook Documentation",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={
                'chapters': len(findings['structure']['chapters']),
                'sections': findings['structure']['total_sections'],
                'media_items': len(findings['media'])
            },
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._assess_documentation_quality(findings)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'title': self._extract_title(root),
            'table_of_contents': self._generate_toc(root),
            'glossary': self._extract_glossary(root),
            'code_examples': self._extract_code_examples(root)
        }
    
    def _analyze_document_structure(self, root: ET.Element) -> Dict[str, Any]:
        structure = {
            'type': root.tag.split('}')[-1] if '}' in root.tag else root.tag,
            'chapters': [],
            'total_sections': 0,
            'max_depth': 0
        }
        
        # Find chapters
        for chapter in root.findall('.//chapter'):
            title_elem = chapter.find('.//title')
            structure['chapters'].append({
                'title': title_elem.text if title_elem is not None else 'Untitled',
                'sections': len(chapter.findall('.//section'))
            })
        
        # Count all sections
        structure['total_sections'] = len(root.findall('.//section'))
        
        return structure
    
    def _extract_metadata(self, root: ET.Element) -> Dict[str, Any]:
        info = root.find('.//info') or root.find('.//bookinfo') or root.find('.//articleinfo')
        
        if info is None:
            return {}
        
        return {
            'title': getattr(info.find('.//title'), 'text', None),
            'author': self._extract_author(info),
            'date': getattr(info.find('.//date'), 'text', None),
            'abstract': getattr(info.find('.//abstract'), 'text', None)
        }
    
    def _analyze_content(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'paragraphs': len(root.findall('.//para')),
            'lists': len(root.findall('.//itemizedlist')) + len(root.findall('.//orderedlist')),
            'tables': len(root.findall('.//table')),
            'examples': len(root.findall('.//example')),
            'notes': len(root.findall('.//note')),
            'warnings': len(root.findall('.//warning'))
        }
    
    def _find_media_references(self, root: ET.Element) -> List[Dict[str, str]]:
        media = []
        
        for elem in root.findall('.//imagedata'):
            media.append({
                'type': 'image',
                'fileref': elem.get('fileref'),
                'format': elem.get('format')
            })
        
        return media
    
    def _find_cross_references(self, root: ET.Element) -> List[str]:
        xrefs = []
        
        for xref in root.findall('.//xref'):
            linkend = xref.get('linkend')
            if linkend:
                xrefs.append(linkend)
        
        return xrefs
    
    def _extract_title(self, root: ET.Element) -> str:
        title = root.find('.//title')
        return title.text if title is not None else 'Untitled'
    
    def _generate_toc(self, root: ET.Element) -> List[Dict[str, Any]]:
        toc = []
        
        for chapter in root.findall('.//chapter'):
            chapter_title = chapter.find('.//title')
            if chapter_title is not None:
                toc_entry = {
                    'title': chapter_title.text,
                    'sections': []
                }
                
                for section in chapter.findall('.//section'):
                    section_title = section.find('.//title')
                    if section_title is not None:
                        toc_entry['sections'].append(section_title.text)
                
                toc.append(toc_entry)
        
        return toc
    
    def _extract_glossary(self, root: ET.Element) -> List[Dict[str, str]]:
        glossary = []
        
        for entry in root.findall('.//glossentry'):
            term = entry.find('.//glossterm')
            definition = entry.find('.//glossdef')
            
            if term is not None and definition is not None:
                glossary.append({
                    'term': term.text,
                    'definition': definition.text
                })
        
        return glossary
    
    def _extract_code_examples(self, root: ET.Element) -> List[Dict[str, str]]:
        examples = []
        
        for example in root.findall('.//programlisting'):
            examples.append({
                'language': example.get('language', 'unknown'),
                'code': example.text if example.text else ''
            })
        
        return examples[:10]  # First 10 examples
    
    def _extract_author(self, info: ET.Element) -> Optional[str]:
        author = info.find('.//author')
        if author is not None:
            firstname = author.find('.//firstname')
            surname = author.find('.//surname')
            if firstname is not None and surname is not None:
                return f"{firstname.text} {surname.text}"
        return None
    
    def _assess_documentation_quality(self, findings: Dict[str, Any]) -> Dict[str, float]:
        content = findings['content_stats']
        
        # Check for good documentation practices
        has_examples = content['examples'] > 0
        has_structure = findings['structure']['total_sections'] > 0
        has_metadata = bool(findings['metadata'])
        
        # Calculate ratios
        example_ratio = min(content['examples'] / max(findings['structure']['total_sections'], 1), 1.0)
        warning_ratio = content['warnings'] / max(content['paragraphs'], 1)
        
        return {
            "completeness": (has_examples + has_structure + has_metadata) / 3,
            "example_coverage": example_ratio,
            "safety_documentation": min(warning_ratio * 10, 1.0)  # Good to have some warnings
        }

# Additional handler classes can be added here:
# - SitemapHandler (for sitemap.xml)
# - WSDLHandler (for web service definitions)
# - XSLTHandler (for transformation stylesheets)
# - KMLHandler (for geographic data)
# - XMLSchemaHandler (for XSD files)
# - Ant/NantBuildHandler (for build scripts)
# - NuGetHandler (for .nuspec files)
# - etc.
</file>

<file path="agent_integration.py">
#!/usr/bin/env python3
"""
LLM Agent Integration for XML Processing

This module provides integration with various LLM providers for XML document analysis.
It includes:

1. Multi-provider LLM integration (OpenAI, Anthropic, local models)
2. Intelligent prompt routing and optimization
3. Result aggregation and synthesis
4. Error handling and retry logic
5. Cost optimization strategies

Usage:
    agent = XMLLLMAgent(provider='openai', model='gpt-4')
    result = agent.process_document('/path/to/file.xml')
"""

import json
import asyncio
import time
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import pickle
from pathlib import Path

# Import our XML framework
from xml_framework import XMLAgentFramework, DocumentSchema, DocumentChunk

@dataclass
class LLMResponse:
    """Response from LLM processing"""
    prompt_id: str
    content: str
    tokens_used: int
    cost_estimate: float
    processing_time: float
    metadata: Dict[str, Any]

@dataclass
class ProcessingResult:
    """Final processing result"""
    document_path: str
    document_type: str
    schema_analysis: str
    chunk_results: List[Dict[str, Any]]
    extracted_data: Dict[str, Any]
    processing_summary: Dict[str, Any]
    total_cost: float
    total_time: float

class LLMProvider(Enum):
    """Supported LLM providers"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    LOCAL = "local"
    AZURE = "azure"

class XMLLLMAgent:
    """Main agent class for XML document processing with LLMs"""
    
    def __init__(self, 
                 provider: str = "openai",
                 model: str = "gpt-4",
                 api_key: Optional[str] = None,
                 max_parallel: int = 3,
                 cache_enabled: bool = True,
                 cost_limit: float = 10.0):
        
        self.provider = LLMProvider(provider)
        self.model = model
        self.api_key = api_key
        self.max_parallel = max_parallel
        self.cache_enabled = cache_enabled
        self.cost_limit = cost_limit
        
        self.xml_framework = XMLAgentFramework()
        self.cache_dir = Path(".xml_agent_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        
        # Initialize LLM client
        self._init_llm_client()
        
        # Cost tracking
        self.total_cost = 0.0
        self.processing_stats = {
            'prompts_sent': 0,
            'tokens_used': 0,
            'cache_hits': 0,
            'errors': 0
        }
    
    def _init_llm_client(self):
        """Initialize the appropriate LLM client"""
        try:
            if self.provider == LLMProvider.OPENAI:
                import openai
                self.client = openai.OpenAI(api_key=self.api_key)
                
            elif self.provider == LLMProvider.ANTHROPIC:
                import anthropic
                self.client = anthropic.Anthropic(api_key=self.api_key)
                
            elif self.provider == LLMProvider.LOCAL:
                # For local models (e.g., via Ollama)
                import requests
                self.client = requests.Session()
                self.base_url = "http://localhost:11434"  # Default Ollama URL
                
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
                
            self.logger.info(f"Initialized {self.provider.value} client with model {self.model}")
            
        except ImportError as e:
            self.logger.error(f"Failed to import {self.provider.value} client: {e}")
            raise
    
    def _get_cache_key(self, prompt: str) -> str:
        """Generate cache key for prompt"""
        return hashlib.md5(f"{self.provider.value}:{self.model}:{prompt}".encode()).hexdigest()
    
    def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """Retrieve cached response if available"""
        if not self.cache_enabled:
            return None
            
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    response = pickle.load(f)
                self.processing_stats['cache_hits'] += 1
                self.logger.debug(f"Cache hit for {cache_key}")
                return response
            except Exception as e:
                self.logger.warning(f"Failed to load cache {cache_key}: {e}")
        
        return None
    
    def _cache_response(self, cache_key: str, response: LLMResponse):
        """Cache LLM response"""
        if not self.cache_enabled:
            return
            
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(response, f)
            self.logger.debug(f"Cached response for {cache_key}")
        except Exception as e:
            self.logger.warning(f"Failed to cache response {cache_key}: {e}")
    
    def _estimate_cost(self, prompt: str, response: str) -> float:
        """Estimate cost based on token usage"""
        # Rough token estimation (actual costs vary by provider)
        input_tokens = len(prompt) // 4  # Rough approximation
        output_tokens = len(response) // 4
        
        cost_per_1k_tokens = {
            LLMProvider.OPENAI: {'input': 0.03, 'output': 0.06},  # GPT-4 pricing
            LLMProvider.ANTHROPIC: {'input': 0.015, 'output': 0.075},  # Claude pricing
            LLMProvider.LOCAL: {'input': 0.0, 'output': 0.0},  # Local is free
            LLMProvider.AZURE: {'input': 0.03, 'output': 0.06}  # Similar to OpenAI
        }
        
        rates = cost_per_1k_tokens.get(self.provider, {'input': 0.02, 'output': 0.04})
        
        cost = (input_tokens / 1000 * rates['input'] + 
                output_tokens / 1000 * rates['output'])
        
        return cost
    
    async def _call_llm(self, prompt: str, prompt_id: str) -> LLMResponse:
        """Call LLM with prompt and return response"""
        
        # Check cache first
        cache_key = self._get_cache_key(prompt)
        cached_response = self._get_cached_response(cache_key)
        if cached_response:
            return cached_response
        
        # Check cost limit
        if self.total_cost >= self.cost_limit:
            raise RuntimeError(f"Cost limit exceeded: ${self.total_cost:.2f}")
        
        start_time = time.time()
        
        try:
            if self.provider == LLMProvider.OPENAI:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=4000,
                    temperature=0.1
                )
                content = response.choices[0].message.content
                tokens_used = response.usage.total_tokens
                
            elif self.provider == LLMProvider.ANTHROPIC:
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=4000,
                    temperature=0.1,
                    messages=[{"role": "user", "content": prompt}]
                )
                content = response.content[0].text
                tokens_used = response.usage.input_tokens + response.usage.output_tokens
                
            elif self.provider == LLMProvider.LOCAL:
                # Ollama API call
                response = self.client.post(f"{self.base_url}/api/generate", json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False
                })
                response.raise_for_status()
                content = response.json()["response"]
                tokens_used = len(prompt) // 4 + len(content) // 4  # Estimate
                
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
            
            processing_time = time.time() - start_time
            cost_estimate = self._estimate_cost(prompt, content)
            
            llm_response = LLMResponse(
                prompt_id=prompt_id,
                content=content,
                tokens_used=tokens_used,
                cost_estimate=cost_estimate,
                processing_time=processing_time,
                metadata={"provider": self.provider.value, "model": self.model}
            )
            
            # Update stats
            self.processing_stats['prompts_sent'] += 1
            self.processing_stats['tokens_used'] += tokens_used
            self.total_cost += cost_estimate
            
            # Cache response
            self._cache_response(cache_key, llm_response)
            
            self.logger.info(f"LLM call completed: {prompt_id}, "
                           f"tokens: {tokens_used}, cost: ${cost_estimate:.4f}")
            
            return llm_response
            
        except Exception as e:
            self.processing_stats['errors'] += 1
            self.logger.error(f"LLM call failed for {prompt_id}: {e}")
            raise
    
    def _extract_structured_data(self, schema_response: str, chunk_responses: List[LLMResponse]) -> Dict[str, Any]:
        """Extract and structure data from LLM responses"""
        
        extracted_data = {
            'document_summary': {},
            'security_findings': [],
            'compliance_data': [],
            'configuration_items': [],
            'relationships': [],
            'metadata': {}
        }
        
        # Parse schema analysis
        try:
            # Look for structured sections in schema response
            lines = schema_response.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('1.') or 'primary purpose' in line.lower():
                    current_section = 'purpose'
                elif line.startswith('2.') or 'key data entities' in line.lower():
                    current_section = 'entities'
                elif line.startswith('3.') or 'processing patterns' in line.lower():
                    current_section = 'patterns'
                elif line.startswith('4.') or 'critical elements' in line.lower():
                    current_section = 'critical_elements'
                elif line.startswith('5.') or 'compliance' in line.lower():
                    current_section = 'compliance'
                
                if current_section and line and not line.startswith(('1.', '2.', '3.', '4.', '5.')):
                    if current_section not in extracted_data['document_summary']:
                        extracted_data['document_summary'][current_section] = []
                    extracted_data['document_summary'][current_section].append(line)
        
        except Exception as e:
            self.logger.warning(f"Failed to parse schema response: {e}")
        
        # Parse chunk responses
        for response in chunk_responses:
            try:
                content = response.content
                
                # Look for structured data patterns
                if 'rule' in content.lower() or 'check' in content.lower():
                    # Security/compliance finding
                    finding = {
                        'chunk_id': response.prompt_id,
                        'content': content[:500],  # Truncate for summary
                        'type': 'security_check'
                    }
                    extracted_data['security_findings'].append(finding)
                
                elif 'configuration' in content.lower() or 'setting' in content.lower():
                    # Configuration item
                    config = {
                        'chunk_id': response.prompt_id,
                        'content': content[:500],
                        'type': 'configuration'
                    }
                    extracted_data['configuration_items'].append(config)
                
                # Extract key-value pairs (simple pattern matching)
                kv_pattern = r'(\w+):\s*([^\n]+)'
                import re
                matches = re.findall(kv_pattern, content)
                for key, value in matches[:5]:  # Limit to avoid noise
                    extracted_data['metadata'][key] = value
                    
            except Exception as e:
                self.logger.warning(f"Failed to parse chunk response {response.prompt_id}: {e}")
        
        return extracted_data
    
    async def process_document(self, file_path: str) -> ProcessingResult:
        """Process XML document with LLM analysis"""
        
        self.logger.info(f"Starting document processing: {file_path}")
        start_time = time.time()
        
        try:
            # Step 1: Analyze document structure
            self.logger.info("Analyzing document structure...")
            schema = self.xml_framework.analyze_document(file_path)
            
            # Step 2: Create chunks
            self.logger.info("Creating document chunks...")
            chunks = self.xml_framework.chunk_document(file_path, schema)
            
            # Step 3: Generate prompts
            self.logger.info("Generating LLM prompts...")
            prompts = self.xml_framework.generate_llm_prompts(schema, chunks, file_path)
            
            # Step 4: Process schema analysis with LLM
            self.logger.info("Sending schema analysis to LLM...")
            schema_response = await self._call_llm(
                prompts['schema_analysis'], 
                'schema_analysis'
            )
            
            # Step 5: Process chunks in parallel (with concurrency limit)
            self.logger.info(f"Processing {len(chunks)} chunks with LLM...")
            chunk_prompts = [(k, v) for k, v in prompts.items() if k.startswith('chunk_')]
            
            # Limit concurrent requests
            chunk_responses = []
            chunk_semaphore = asyncio.Semaphore(self.max_parallel)
            
            async def process_chunk(prompt_id, prompt_content):
                async with chunk_semaphore:
                    return await self._call_llm(prompt_content, prompt_id)
            
            # Process chunks concurrently
            tasks = [process_chunk(prompt_id, prompt) for prompt_id, prompt in chunk_prompts]
            chunk_responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out exceptions
            valid_responses = [r for r in chunk_responses if isinstance(r, LLMResponse)]
            
            # Step 6: Extract structured data
            self.logger.info("Extracting structured data...")
            extracted_data = self._extract_structured_data(
                schema_response.content, 
                valid_responses
            )
            
            # Step 7: Create processing summary
            processing_time = time.time() - start_time
            
            chunk_results = []
            for response in valid_responses:
                chunk_results.append({
                    'chunk_id': response.prompt_id,
                    'processing_time': response.processing_time,
                    'tokens_used': response.tokens_used,
                    'cost': response.cost_estimate,
                    'summary': response.content[:200] + "..." if len(response.content) > 200 else response.content
                })
            
            processing_summary = {
                'total_chunks': len(chunks),
                'successful_chunks': len(valid_responses),
                'failed_chunks': len(chunks) - len(valid_responses),
                'total_tokens': sum(r.tokens_used for r in [schema_response] + valid_responses),
                'total_cost': self.total_cost,
                'processing_time': processing_time,
                'cache_hits': self.processing_stats['cache_hits'],
                'provider': self.provider.value,
                'model': self.model
            }
            
            result = ProcessingResult(
                document_path=file_path,
                document_type=schema.document_type,
                schema_analysis=schema_response.content,
                chunk_results=chunk_results,
                extracted_data=extracted_data,
                processing_summary=processing_summary,
                total_cost=self.total_cost,
                total_time=processing_time
            )
            
            self.logger.info(f"Document processing completed successfully")
            self.logger.info(f"Total cost: ${self.total_cost:.4f}, Time: {processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Document processing failed: {e}")
            raise
    
    def save_results(self, result: ProcessingResult, output_path: str):
        """Save processing results to file"""
        
        output_data = {
            'document_path': result.document_path,
            'document_type': result.document_type,
            'processing_summary': result.processing_summary,
            'schema_analysis': result.schema_analysis,
            'extracted_data': result.extracted_data,
            'chunk_results': result.chunk_results
        }
        
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2)
        
        self.logger.info(f"Results saved to: {output_path}")
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get current processing statistics"""
        return {
            **self.processing_stats,
            'total_cost': self.total_cost,
            'provider': self.provider.value,
            'model': self.model
        }

# Helper functions for easy usage

async def analyze_xml_with_llm(file_path: str, 
                              provider: str = "openai",
                              model: str = "gpt-4",
                              api_key: Optional[str] = None,
                              output_file: Optional[str] = None) -> ProcessingResult:
    """Convenience function to analyze XML file with LLM"""
    
    agent = XMLLLMAgent(
        provider=provider,
        model=model,
        api_key=api_key,
        max_parallel=3,
        cache_enabled=True
    )
    
    result = await agent.process_document(file_path)
    
    if output_file:
        agent.save_results(result, output_file)
    
    return result

def analyze_xml_sync(file_path: str, **kwargs) -> ProcessingResult:
    """Synchronous wrapper for async analysis"""
    return asyncio.run(analyze_xml_with_llm(file_path, **kwargs))

# Example usage and testing
if __name__ == "__main__":
    import sys
    import os
    
    if len(sys.argv) < 2:
        print("Usage: python llm_agent.py <xml_file> [provider] [model]")
        print("\nExample:")
        print("  python llm_agent.py stig-file.xml openai gpt-4")
        print("  python llm_agent.py stig-file.xml anthropic claude-3-sonnet-20240229")
        print("  python llm_agent.py stig-file.xml local llama2")
        sys.exit(1)
    
    file_path = sys.argv[1]
    provider = sys.argv[2] if len(sys.argv) > 2 else "openai"
    model = sys.argv[3] if len(sys.argv) > 3 else "gpt-4"
    
    # Get API key from environment
    api_key = None
    if provider == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
    elif provider == "anthropic":
        api_key = os.getenv("ANTHROPIC_API_KEY")
    
    if not api_key and provider != "local":
        print(f"Please set the appropriate API key environment variable for {provider}")
        sys.exit(1)
    
    # Configure logging
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        print(f" Processing {file_path} with {provider}:{model}")
        
        result = analyze_xml_sync(
            file_path=file_path,
            provider=provider,
            model=model,
            api_key=api_key,
            output_file=f"{Path(file_path).stem}_llm_analysis.json"
        )
        
        print(f"\n Processing completed successfully!")
        print(f" Summary:")
        print(f"   Document Type: {result.document_type}")
        print(f"   Chunks Processed: {result.processing_summary['successful_chunks']}")
        print(f"   Total Cost: ${result.total_cost:.4f}")
        print(f"   Processing Time: {result.total_time:.2f}s")
        print(f"   Tokens Used: {result.processing_summary['total_tokens']:,}")
        
        print(f"\n Key Findings:")
        if 'security_findings' in result.extracted_data:
            print(f"   Security Findings: {len(result.extracted_data['security_findings'])}")
        if 'configuration_items' in result.extracted_data:
            print(f"   Configuration Items: {len(result.extracted_data['configuration_items'])}")
        
        print(f"\n Results saved to: {Path(file_path).stem}_llm_analysis.json")
        
    except Exception as e:
        print(f" Processing failed: {e}")
        sys.exit(1)
</file>

<file path="xml_document_analysis_framework.py">
#!/usr/bin/env python3
"""
XML Document Analysis Framework for LLM Agents

This framework provides a complete solution for analyzing XML documents and preparing
them for LLM-based processing. It includes:

1. Deterministic schema extraction
2. Chunking strategies for large files
3. LLM prompt templates for semantic understanding
4. Document type detection and specialized handlers
5. Incremental processing for memory-efficient analysis

Usage:
    framework = XMLAgentFramework()
    analysis = framework.analyze_document("path/to/file.xml")
    chunks = framework.chunk_document("path/to/file.xml", analysis)
    llm_prompts = framework.generate_llm_prompts(analysis, chunks)
"""

import xml.etree.ElementTree as ET
from xml.sax import make_parser, ContentHandler
from collections import defaultdict, Counter
import json
import re
import hashlib
from typing import Dict, List, Set, Any, Optional, Generator, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

@dataclass
class XMLElement:
    """Represents an XML element with all its metadata"""
    tag: str
    namespace: Optional[str]
    count: int
    depths: Set[int]
    attributes: Dict[str, Set[str]]
    text_samples: List[str]
    parent_elements: Set[str]
    child_elements: Set[str]
    paths: Set[str]
    line_numbers: List[int]

@dataclass
class DocumentSchema:
    """Complete document schema analysis"""
    document_type: str
    root_element: str
    namespaces: Dict[str, str]
    elements: Dict[str, XMLElement]
    structure_tree: Dict[str, Any]
    statistics: Dict[str, Any]
    specialized_info: Dict[str, Any]

@dataclass
class DocumentChunk:
    """A chunk of XML document for LLM processing"""
    chunk_id: str
    content: str
    element_path: str
    line_range: Tuple[int, int]
    size_bytes: int
    elements_contained: List[str]
    summary: str

class XMLStreamHandler(ContentHandler):
    """SAX handler for memory-efficient XML analysis"""
    
    def __init__(self, max_samples=5, max_text_length=200):
        super().__init__()
        self.elements = defaultdict(lambda: XMLElement(
            tag="", namespace=None, count=0, depths=set(), attributes=defaultdict(set),
            text_samples=[], parent_elements=set(), child_elements=set(),
            paths=set(), line_numbers=[]
        ))
        self.namespaces = {}
        self.element_stack = []
        self.path_stack = []
        self.current_text = ""
        self.max_samples = max_samples
        self.max_text_length = max_text_length
        self.line_number = 1
        
    def startNamespace(self, prefix, uri):
        if prefix:
            self.namespaces[prefix] = uri
        else:
            self.namespaces['default'] = uri
    
    def startElement(self, name, attrs):
        # Parse namespace and local name
        if ':' in name:
            namespace, local_name = name.split(':', 1)
        else:
            namespace, local_name = None, name
            
        # Update element info
        element_info = self.elements[local_name]
        element_info.tag = local_name
        element_info.namespace = namespace
        element_info.count += 1
        element_info.depths.add(len(self.element_stack))
        element_info.line_numbers.append(self.line_number)
        
        # Track path
        current_path = '/'.join(self.path_stack + [local_name])
        element_info.paths.add(current_path)
        
        # Parent-child relationships
        if self.element_stack:
            parent_tag = self.element_stack[-1]
            element_info.parent_elements.add(parent_tag)
            self.elements[parent_tag].child_elements.add(local_name)
        
        # Store attributes
        for attr_name in attrs.getNames():
            attr_value = attrs.getValue(attr_name)
            clean_attr = attr_name.split(':')[-1]  # Remove namespace prefix
            element_info.attributes[clean_attr].add(attr_value[:50])  # Limit length
        
        self.element_stack.append(local_name)
        self.path_stack.append(local_name)
        self.current_text = ""
    
    def endElement(self, name):
        local_name = name.split(':', 1)[-1] if ':' in name else name
        
        # Store text content if any
        if self.current_text.strip() and local_name in self.elements:
            element_info = self.elements[local_name]
            if len(element_info.text_samples) < self.max_samples:
                text = self.current_text.strip()[:self.max_text_length]
                element_info.text_samples.append(text)
        
        if self.element_stack:
            self.element_stack.pop()
        if self.path_stack:
            self.path_stack.pop()
        self.current_text = ""
    
    def characters(self, content):
        self.current_text += content
        # Count line numbers
        self.line_number += content.count('\n')

class DocumentTypeDetector:
    """Detects and classifies XML document types"""
    
    DOCUMENT_PATTERNS = {
        'SCAP': {
            'root_elements': ['asset-report-collection', 'data-stream-collection'],
            'namespaces': ['http://scap.nist.gov', 'http://checklists.nist.gov/xccdf'],
            'description': 'Security Content Automation Protocol document'
        },
        'XCCDF': {
            'root_elements': ['Benchmark'],
            'namespaces': ['http://checklists.nist.gov/xccdf'],
            'description': 'Extensible Configuration Checklist Description Format'
        },
        'OVAL': {
            'root_elements': ['oval_definitions', 'oval_results'],
            'namespaces': ['http://oval.mitre.org/XMLSchema'],
            'description': 'Open Vulnerability and Assessment Language'
        },
        'SOAP': {
            'root_elements': ['Envelope'],
            'namespaces': ['http://schemas.xmlsoap.org/soap'],
            'description': 'SOAP web service message'
        },
        'XML_SCHEMA': {
            'root_elements': ['schema'],
            'namespaces': ['http://www.w3.org/2001/XMLSchema'],
            'description': 'XML Schema Definition'
        },
        'RSS': {
            'root_elements': ['rss', 'feed'],
            'namespaces': ['http://www.w3.org/2005/Atom'],
            'description': 'RSS/Atom feed'
        },
        'SVG': {
            'root_elements': ['svg'],
            'namespaces': ['http://www.w3.org/2000/svg'],
            'description': 'Scalable Vector Graphics'
        },
        'WSDL': {
            'root_elements': ['definitions'],
            'namespaces': ['http://schemas.xmlsoap.org/wsdl'],
            'description': 'Web Services Description Language'
        }
    }
    
    @classmethod
    def detect_type(cls, schema: DocumentSchema) -> str:
        """Detect document type based on schema analysis"""
        for doc_type, patterns in cls.DOCUMENT_PATTERNS.items():
            # Check root element
            if schema.root_element in patterns['root_elements']:
                return doc_type
            
            # Check namespaces
            for pattern_ns in patterns['namespaces']:
                for actual_ns in schema.namespaces.values():
                    if pattern_ns in actual_ns:
                        return doc_type
        
        return 'GENERIC_XML'

class XMLChunker:
    """Chunks XML documents for LLM processing"""
    
    def __init__(self, max_chunk_size=8000, overlap_size=200):
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size
    
    def chunk_by_elements(self, file_path: str, schema: DocumentSchema) -> List[DocumentChunk]:
        """Chunk document by major structural elements"""
        chunks = []
        
        # Identify major structural elements (low frequency, high child count)
        major_elements = []
        for tag, element in schema.elements.items():
            if (element.count < 100 and len(element.child_elements) > 3 and 
                min(element.depths) <= 3):
                major_elements.append(tag)
        
        if not major_elements:
            # Fallback to size-based chunking
            return self.chunk_by_size(file_path)
        
        # Parse and chunk by major elements
        current_chunk = ""
        chunk_elements = []
        line_start = 1
        
        context = ET.iterparse(file_path, events=('start', 'end'))
        
        for event, elem in context:
            tag_name = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            
            if event == 'start' and tag_name in major_elements:
                if current_chunk and len(current_chunk) > 1000:  # Minimum chunk size
                    chunk_id = hashlib.md5(current_chunk.encode()).hexdigest()[:8]
                    chunks.append(DocumentChunk(
                        chunk_id=chunk_id,
                        content=current_chunk,
                        element_path=f"/{'/'.join(chunk_elements)}",
                        line_range=(line_start, line_start + current_chunk.count('\n')),
                        size_bytes=len(current_chunk.encode()),
                        elements_contained=chunk_elements.copy(),
                        summary=f"Contains {len(chunk_elements)} elements including {tag_name}"
                    ))
                    current_chunk = ""
                    chunk_elements = []
                    line_start += current_chunk.count('\n')
                
                chunk_elements.append(tag_name)
            
            if len(current_chunk) < self.max_chunk_size:
                current_chunk += ET.tostring(elem, encoding='unicode') if event == 'end' else ""
        
        # Add final chunk
        if current_chunk:
            chunk_id = hashlib.md5(current_chunk.encode()).hexdigest()[:8]
            chunks.append(DocumentChunk(
                chunk_id=chunk_id,
                content=current_chunk,
                element_path=f"/{'/'.join(chunk_elements)}",
                line_range=(line_start, line_start + current_chunk.count('\n')),
                size_bytes=len(current_chunk.encode()),
                elements_contained=chunk_elements,
                summary=f"Final chunk with {len(chunk_elements)} elements"
            ))
        
        return chunks
    
    def chunk_by_size(self, file_path: str) -> List[DocumentChunk]:
        """Fallback chunking by size with XML awareness"""
        chunks = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Simple size-based chunking with element boundary awareness
        start = 0
        chunk_num = 0
        
        while start < len(content):
            end = min(start + self.max_chunk_size, len(content))
            
            # Try to end at element boundary
            if end < len(content):
                # Find last complete element
                last_element_end = content.rfind('</', start, end)
                if last_element_end > start + self.max_chunk_size // 2:
                    element_end = content.find('>', last_element_end)
                    if element_end != -1:
                        end = element_end + 1
            
            chunk_content = content[start:end]
            chunk_id = f"chunk_{chunk_num:03d}"
            
            chunks.append(DocumentChunk(
                chunk_id=chunk_id,
                content=chunk_content,
                element_path="size_based",
                line_range=(content[:start].count('\n') + 1, content[:end].count('\n') + 1),
                size_bytes=len(chunk_content.encode()),
                elements_contained=[],
                summary=f"Size-based chunk {chunk_num}"
            ))
            
            start = end - self.overlap_size if end < len(content) else end
            chunk_num += 1
        
        return chunks

class LLMPromptGenerator:
    """Generates prompts for LLM analysis of XML documents"""
    
    SCHEMA_ANALYSIS_TEMPLATE = """
    Analyze this XML document schema and provide semantic understanding:

    DOCUMENT TYPE: {document_type}
    {description}

    SCHEMA OVERVIEW:
    - Root Element: {root_element}
    - Total Elements: {total_elements:,}
    - Maximum Depth: {max_depth}
    - File Size: {file_size_mb:.2f} MB

    NAMESPACES:
    {namespaces}

    KEY ELEMENTS:
    {elements_summary}

    STRUCTURAL HIERARCHY:
    {structure_tree}

    QUESTIONS FOR ANALYSIS:
    1. What is the primary purpose of this document?
    2. What are the key data entities and their relationships?
    3. What processing patterns would be most effective?
    4. What are the critical elements for data extraction?
    5. Are there any compliance or security considerations?

    Provide a comprehensive analysis addressing these questions and suggest 
    optimal strategies for automated processing of this document type.
    """
    
    CHUNK_ANALYSIS_TEMPLATE = """
    Analyze this XML document chunk in context:

    CHUNK INFO:
    - ID: {chunk_id}
    - Path: {element_path}
    - Size: {size_bytes:,} bytes
    - Line Range: {line_start}-{line_end}
    - Elements: {elements_contained}

    DOCUMENT CONTEXT:
    {document_context}

    CHUNK CONTENT:
    ```xml
    {content}
    ```

    ANALYSIS TASKS:
    1. Identify the main data entities in this chunk
    2. Extract key-value pairs and relationships
    3. Note any validation rules or constraints
    4. Identify dependencies on other document parts
    5. Suggest data extraction patterns

    Provide structured output suitable for automated processing.
    """
    
    @classmethod
    def generate_schema_prompt(cls, schema: DocumentSchema, file_size: int) -> str:
        """Generate prompt for overall schema analysis"""
        
        # Format namespaces
        ns_text = "\n".join([f"  {prefix}: {uri}" for prefix, uri in schema.namespaces.items()])
        
        # Format top elements
        sorted_elements = sorted(schema.elements.items(), 
                               key=lambda x: x[1].count, reverse=True)[:10]
        
        elements_text = ""
        for tag, element in sorted_elements:
            attrs = f"[{len(element.attributes)} attrs]" if element.attributes else "[no attrs]"
            children = f"[{len(element.child_elements)} children]" if element.child_elements else "[leaf]"
            text_info = "[has text]" if element.text_samples else "[no text]"
            
            elements_text += f"  {tag}: {element.count:,} occurrences {attrs} {children} {text_info}\n"
            
            if element.text_samples:
                sample = element.text_samples[0][:50]
                elements_text += f"    Sample: \"{sample}...\"\n"
        
        doc_type = DocumentTypeDetector.detect_type(schema)
        description = DocumentTypeDetector.DOCUMENT_PATTERNS.get(doc_type, {}).get('description', 'Unknown XML document type')
        
        return cls.SCHEMA_ANALYSIS_TEMPLATE.format(
            document_type=doc_type,
            description=description,
            root_element=schema.root_element,
            total_elements=sum(e.count for e in schema.elements.values()),
            max_depth=max(max(e.depths) for e in schema.elements.values() if e.depths),
            file_size_mb=file_size / (1024 * 1024),
            namespaces=ns_text,
            elements_summary=elements_text,
            structure_tree=json.dumps(schema.structure_tree, indent=2)
        )
    
    @classmethod
    def generate_chunk_prompt(cls, chunk: DocumentChunk, schema: DocumentSchema) -> str:
        """Generate prompt for chunk analysis"""
        
        # Create document context
        doc_type = DocumentTypeDetector.detect_type(schema)
        context = f"Document Type: {doc_type}\nRoot: {schema.root_element}\n"
        context += f"Total Elements: {len(schema.elements)}\n"
        
        return cls.CHUNK_ANALYSIS_TEMPLATE.format(
            chunk_id=chunk.chunk_id,
            element_path=chunk.element_path,
            size_bytes=chunk.size_bytes,
            line_start=chunk.line_range[0],
            line_end=chunk.line_range[1],
            elements_contained=", ".join(chunk.elements_contained),
            document_context=context,
            content=chunk.content[:6000]  # Limit content size
        )

class XMLAgentFramework:
    """Main framework class for XML document analysis"""
    
    def __init__(self, max_chunk_size=8000, max_samples=5):
        self.chunker = XMLChunker(max_chunk_size=max_chunk_size)
        self.max_samples = max_samples
        self.logger = logging.getLogger(__name__)
    
    def analyze_document(self, file_path: str) -> DocumentSchema:
        """Perform complete document analysis"""
        self.logger.info(f"Analyzing XML document: {file_path}")
        
        # Use SAX parser for memory efficiency
        parser = make_parser()
        handler = XMLStreamHandler(max_samples=self.max_samples)
        parser.setContentHandler(handler)
        parser.setFeature("http://xml.org/sax/features/namespaces", True)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            parser.parse(f)
        
        # Build structure tree
        structure_tree = self._build_structure_tree(handler.elements)
        
        # Get root element
        root_element = ""
        for tag, element in handler.elements.items():
            if not element.parent_elements:
                root_element = tag
                break
        
        # Gather statistics
        file_size = Path(file_path).stat().st_size
        total_elements = sum(e.count for e in handler.elements.values())
        
        statistics = {
            'file_size_bytes': file_size,
            'total_elements': total_elements,
            'unique_elements': len(handler.elements),
            'max_depth': max(max(e.depths) for e in handler.elements.values() if e.depths),
            'namespace_count': len(handler.namespaces)
        }
        
        # Convert defaultdicts for serialization
        elements_dict = {}
        for tag, element in handler.elements.items():
            elements_dict[tag] = XMLElement(
                tag=element.tag,
                namespace=element.namespace,
                count=element.count,
                depths=element.depths,
                attributes={k: v for k, v in element.attributes.items()},
                text_samples=element.text_samples,
                parent_elements=element.parent_elements,
                child_elements=element.child_elements,
                paths=element.paths,
                line_numbers=element.line_numbers
            )
        
        schema = DocumentSchema(
            document_type=DocumentTypeDetector.detect_type(None),  # Will be set after creation
            root_element=root_element,
            namespaces=handler.namespaces,
            elements=elements_dict,
            structure_tree=structure_tree,
            statistics=statistics,
            specialized_info={}
        )
        
        # Set document type
        schema.document_type = DocumentTypeDetector.detect_type(schema)
        
        return schema
    
    def chunk_document(self, file_path: str, schema: DocumentSchema) -> List[DocumentChunk]:
        """Chunk document for LLM processing"""
        return self.chunker.chunk_by_elements(file_path, schema)
    
    def generate_llm_prompts(self, schema: DocumentSchema, chunks: List[DocumentChunk], 
                           file_path: str) -> Dict[str, str]:
        """Generate all prompts for LLM analysis"""
        file_size = Path(file_path).stat().st_size
        
        prompts = {
            'schema_analysis': LLMPromptGenerator.generate_schema_prompt(schema, file_size)
        }
        
        # Generate chunk prompts
        for i, chunk in enumerate(chunks[:5]):  # Limit to first 5 chunks for demo
            prompts[f'chunk_{i:03d}'] = LLMPromptGenerator.generate_chunk_prompt(chunk, schema)
        
        return prompts
    
    def _build_structure_tree(self, elements: Dict[str, XMLElement]) -> Dict[str, Any]:
        """Build hierarchical structure representation"""
        tree = {}
        
        # Find root elements
        for tag, element in elements.items():
            if not element.parent_elements:
                tree[tag] = self._build_subtree(tag, elements)
        
        return tree
    
    def _build_subtree(self, tag: str, elements: Dict[str, XMLElement]) -> Dict[str, Any]:
        """Recursively build structure subtree"""
        element = elements[tag]
        
        node = {
            "count": element.count,
            "namespace": element.namespace,
            "attributes": list(element.attributes.keys()),
            "has_text": len(element.text_samples) > 0,
            "children": {}
        }
        
        for child_tag in element.child_elements:
            if child_tag in elements:
                node["children"][child_tag] = self._build_subtree(child_tag, elements)
        
        return node
    
    def process_document(self, file_path: str) -> Dict[str, Any]:
        """Complete document processing pipeline"""
        
        # Step 1: Analyze schema
        schema = self.analyze_document(file_path)
        self.logger.info(f"Document type detected: {schema.document_type}")
        
        # Step 2: Chunk document
        chunks = self.chunk_document(file_path, schema)
        self.logger.info(f"Created {len(chunks)} chunks")
        
        # Step 3: Generate LLM prompts
        prompts = self.generate_llm_prompts(schema, chunks, file_path)
        
        # Step 4: Return complete analysis package
        return {
            'schema': schema,
            'chunks': chunks,
            'prompts': prompts,
            'processing_strategy': self._suggest_processing_strategy(schema)
        }
    
    def _suggest_processing_strategy(self, schema: DocumentSchema) -> Dict[str, str]:
        """Suggest optimal processing strategy based on document type"""
        
        strategies = {
            'SCAP': {
                'approach': 'Security-focused analysis',
                'key_elements': ['rule', 'check', 'test', 'result'],
                'extraction_pattern': 'Extract compliance status and security findings'
            },
            'XCCDF': {
                'approach': 'Checklist processing',
                'key_elements': ['rule', 'select', 'check'],
                'extraction_pattern': 'Extract security configuration requirements'
            },
            'OVAL': {
                'approach': 'Vulnerability assessment',
                'key_elements': ['definition', 'test', 'object', 'state'],
                'extraction_pattern': 'Extract vulnerability definitions and test criteria'
            },
            'GENERIC_XML': {
                'approach': 'General XML processing',
                'key_elements': list(schema.elements.keys())[:5],
                'extraction_pattern': 'Extract structured data based on element hierarchy'
            }
        }
        
        return strategies.get(schema.document_type, strategies['GENERIC_XML'])

# Example usage and testing
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python xml_framework.py <xml_file>")
        sys.exit(1)
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    # Initialize framework
    framework = XMLAgentFramework(max_chunk_size=6000)
    
    # Process document
    try:
        result = framework.process_document(sys.argv[1])
        
        print(f"Document Type: {result['schema'].document_type}")
        print(f"Total Elements: {result['schema'].statistics['total_elements']:,}")
        print(f"Chunks Created: {len(result['chunks'])}")
        print(f"Processing Strategy: {result['processing_strategy']['approach']}")
        
        # Save analysis results
        output_file = Path(sys.argv[1]).stem + "_analysis.json"
        
        # Convert for JSON serialization
        serializable_result = {
            'schema': {
                'document_type': result['schema'].document_type,
                'root_element': result['schema'].root_element,
                'statistics': result['schema'].statistics,
                'namespaces': result['schema'].namespaces
            },
            'chunks_summary': [
                {
                    'chunk_id': chunk.chunk_id,
                    'size_bytes': chunk.size_bytes,
                    'element_path': chunk.element_path,
                    'summary': chunk.summary
                }
                for chunk in result['chunks']
            ],
            'processing_strategy': result['processing_strategy']
        }
        
        with open(output_file, 'w') as f:
            json.dump(serializable_result, f, indent=2)
        
        print(f"Analysis saved to: {output_file}")
        
        # Display first prompt
        print("\n=== SAMPLE SCHEMA ANALYSIS PROMPT ===")
        print(result['prompts']['schema_analysis'][:1000] + "...")
        
    except Exception as e:
        logging.error(f"Error processing document: {e}")
        sys.exit(1)
</file>

<file path="xml_framework_demo_script.py">
#!/usr/bin/env python3
"""
Demo script showing how to use the XML Agent Framework with STIG files

This script demonstrates:
1. Basic document analysis
2. Schema extraction and summarization
3. Chunking strategies
4. LLM prompt generation
5. Integration patterns for agentic systems

Run with: python demo.py /path/to/your/stig/file.xml
"""

import sys
import json
from pathlib import Path
import time

# Import our framework (assumes the framework code is in xml_framework.py)
try:
    from xml_framework import XMLAgentFramework, DocumentTypeDetector
except ImportError:
    print("Error: xml_framework.py not found. Please ensure the framework file is in the same directory.")
    sys.exit(1)

def analyze_stig_file(file_path: str):
    """Demonstrate STIG file analysis"""
    print(f" Analyzing STIG file: {file_path}")
    print("=" * 60)
    
    # Initialize framework
    framework = XMLAgentFramework(max_chunk_size=8000, max_samples=3)
    
    start_time = time.time()
    
    # Step 1: Quick file info
    file_size = Path(file_path).stat().st_size
    print(f" File size: {file_size / (1024*1024):.2f} MB")
    
    # Step 2: Analyze document schema
    print("\n Extracting document schema...")
    schema = framework.analyze_document(file_path)
    
    print(f" Schema analysis complete!")
    print(f"   Document Type: {schema.document_type}")
    print(f"   Root Element: {schema.root_element}")
    print(f"   Total Elements: {schema.statistics['total_elements']:,}")
    print(f"   Unique Elements: {schema.statistics['unique_elements']}")
    print(f"   Max Depth: {schema.statistics['max_depth']}")
    print(f"   Namespaces: {schema.statistics['namespace_count']}")
    
    # Step 3: Show key elements
    print(f"\n Top 10 Most Frequent Elements:")
    sorted_elements = sorted(schema.elements.items(), 
                           key=lambda x: x[1].count, reverse=True)[:10]
    
    for i, (tag, element) in enumerate(sorted_elements, 1):
        ns_info = f" ({element.namespace}:)" if element.namespace else ""
        attr_count = len(element.attributes)
        child_count = len(element.child_elements)
        
        print(f"   {i:2d}. {tag}{ns_info}: {element.count:,} occurrences")
        print(f"       Attributes: {attr_count}, Children: {child_count}")
        
        if element.text_samples:
            sample = element.text_samples[0][:60].replace('\n', ' ')
            print(f"       Sample text: \"{sample}...\"")
    
    # Step 4: Namespace analysis
    print(f"\n Namespace Analysis:")
    for prefix, uri in schema.namespaces.items():
        print(f"   {prefix:12s}: {uri}")
    
    # Step 5: Create chunks
    print(f"\n Creating document chunks...")
    chunks = framework.chunk_document(file_path, schema)
    
    total_chunk_size = sum(chunk.size_bytes for chunk in chunks)
    print(f" Created {len(chunks)} chunks")
    print(f"   Total chunk size: {total_chunk_size / (1024*1024):.2f} MB")
    print(f"   Average chunk size: {total_chunk_size / len(chunks) / 1024:.1f} KB")
    
    # Show chunk details
    print(f"\n Chunk Breakdown:")
    for i, chunk in enumerate(chunks[:5]):  # Show first 5
        print(f"   Chunk {i+1}: {chunk.chunk_id}")
        print(f"     Size: {chunk.size_bytes / 1024:.1f} KB")
        print(f"     Lines: {chunk.line_range[0]}-{chunk.line_range[1]}")
        print(f"     Path: {chunk.element_path}")
        print(f"     Elements: {', '.join(chunk.elements_contained[:3])}...")
    
    if len(chunks) > 5:
        print(f"   ... and {len(chunks) - 5} more chunks")
    
    # Step 6: Generate LLM prompts
    print(f"\n Generating LLM prompts...")
    prompts = framework.generate_llm_prompts(schema, chunks, file_path)
    
    print(f" Generated {len(prompts)} prompts:")
    for prompt_name in prompts.keys():
        prompt_size = len(prompts[prompt_name])
        print(f"   - {prompt_name}: {prompt_size:,} characters")
    
    # Step 7: Processing strategy
    strategy = framework._suggest_processing_strategy(schema)
    print(f"\n Suggested Processing Strategy:")
    print(f"   Approach: {strategy['approach']}")
    print(f"   Key Elements: {', '.join(strategy['key_elements'])}")
    print(f"   Pattern: {strategy['extraction_pattern']}")
    
    analysis_time = time.time() - start_time
    print(f"\n  Analysis completed in {analysis_time:.2f} seconds")
    
    return schema, chunks, prompts, strategy

def demonstrate_llm_integration(prompts: dict, schema, chunks):
    """Show how to integrate with LLM for semantic understanding"""
    
    print(f"\n LLM Integration Demonstration")
    print("=" * 60)
    
    # Show what you'd send to the LLM for schema analysis
    print(f"\n1  SCHEMA ANALYSIS PROMPT:")
    print("-" * 40)
    schema_prompt = prompts['schema_analysis']
    print(f"Prompt length: {len(schema_prompt):,} characters")
    print(f"First 500 characters:")
    print(schema_prompt[:500] + "...")
    
    print(f"\n This prompt would help the LLM understand:")
    print("    Document purpose and structure")
    print("    Key data entities and relationships") 
    print("    Optimal processing strategies")
    print("    Security/compliance considerations")
    
    # Show chunk analysis approach
    if any(key.startswith('chunk_') for key in prompts.keys()):
        print(f"\n2  CHUNK ANALYSIS APPROACH:")
        print("-" * 40)
        
        chunk_keys = [k for k in prompts.keys() if k.startswith('chunk_')]
        print(f"Total chunks for LLM processing: {len(chunk_keys)}")
        
        if chunk_keys:
            first_chunk_prompt = prompts[chunk_keys[0]]
            print(f"Sample chunk prompt length: {len(first_chunk_prompt):,} characters")
            print(f"First 300 characters of chunk prompt:")
            print(first_chunk_prompt[:300] + "...")
    
    # Show integration patterns
    print(f"\n3  INTEGRATION PATTERNS:")
    print("-" * 40)
    print("   Sequential Processing:")
    print("     1. Send schema prompt  Get document understanding")
    print("     2. Send chunk prompts  Extract structured data")
    print("     3. Aggregate results  Build complete picture")
    print()
    print("   Parallel Processing:")
    print("     1. Send schema prompt to one LLM instance")
    print("     2. Send chunk prompts to multiple instances")
    print("     3. Combine results using schema context")
    print()
    print("   Adaptive Processing:")
    print("     1. Use schema analysis to identify key sections")
    print("     2. Prioritize chunks with important elements")
    print("     3. Apply document-specific extraction rules")

def save_analysis_results(file_path: str, schema, chunks, prompts, strategy):
    """Save analysis results for later use"""
    
    output_dir = Path(file_path).parent / "xml_analysis"
    output_dir.mkdir(exist_ok=True)
    
    base_name = Path(file_path).stem
    
    # Save schema summary
    schema_file = output_dir / f"{base_name}_schema.json"
    schema_data = {
        'document_type': schema.document_type,
        'root_element': schema.root_element,
        'statistics': schema.statistics,
        'namespaces': schema.namespaces,
        'top_elements': [
            {
                'tag': tag,
                'count': element.count,
                'namespace': element.namespace,
                'attributes': list(element.attributes.keys()),
                'has_text': len(element.text_samples) > 0
            }
            for tag, element in sorted(schema.elements.items(), 
                                     key=lambda x: x[1].count, reverse=True)[:20]
        ],
        'processing_strategy': strategy
    }
    
    with open(schema_file, 'w') as f:
        json.dump(schema_data, f, indent=2)
    
    # Save chunk metadata
    chunks_file = output_dir / f"{base_name}_chunks.json"
    chunks_data = [
        {
            'chunk_id': chunk.chunk_id,
            'element_path': chunk.element_path,
            'line_range': chunk.line_range,
            'size_bytes': chunk.size_bytes,
            'elements_contained': chunk.elements_contained,
            'summary': chunk.summary
        }
        for chunk in chunks
    ]
    
    with open(chunks_file, 'w') as f:
        json.dump(chunks_data, f, indent=2)
    
    # Save prompts
    prompts_dir = output_dir / f"{base_name}_prompts"
    prompts_dir.mkdir(exist_ok=True)
    
    for prompt_name, prompt_content in prompts.items():
        prompt_file = prompts_dir / f"{prompt_name}.txt"
        with open(prompt_file, 'w') as f:
            f.write(prompt_content)
    
    print(f"\n Analysis results saved to: {output_dir}")
    print(f"    Schema: {schema_file.name}")
    print(f"    Chunks: {chunks_file.name}")
    print(f"    Prompts: {prompts_dir.name}/ ({len(prompts)} files)")

def main():
    """Main demonstration function"""
    
    if len(sys.argv) < 2:
        print("Usage: python demo.py <xml_file_path>")
        print("\nExample:")
        print("  python demo.py /path/to/stig-file.xml")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    if not Path(file_path).exists():
        print(f"Error: File not found: {file_path}")
        sys.exit(1)
    
    try:
        # Run the analysis
        schema, chunks, prompts, strategy = analyze_stig_file(file_path)
        
        # Demonstrate LLM integration
        demonstrate_llm_integration(prompts, schema, chunks)
        
        # Save results
        save_analysis_results(file_path, schema, chunks, prompts, strategy)
        
        print(f"\n Analysis complete! You now have:")
        print("    Deterministic schema extraction")
        print("    Intelligent document chunking") 
        print("    LLM-ready prompts for semantic analysis")
        print("    Processing strategy recommendations")
        print("\n Next steps for your agentic framework:")
        print("   1. Send the schema prompt to your LLM")
        print("   2. Use the response to refine chunk processing")
        print("   3. Process chunks in parallel for efficiency")
        print("   4. Aggregate results for final analysis")
        
    except Exception as e:
        print(f" Error during analysis: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
</file>

<file path="xml_schema_analyzer_fixed.py">
#!/usr/bin/env python3
"""
Fixed XML Schema Analyzer for Large Files

Handles very large XML files efficiently using iterative parsing
instead of recursive analysis to avoid stack overflow.
"""

import xml.etree.ElementTree as ET
from collections import defaultdict, deque
import json
import sys
from typing import Dict, List, Set, Any, Optional
from dataclasses import dataclass, asdict

@dataclass
class ElementInfo:
    """Information about an XML element"""
    tag: str
    count: int
    depth_levels: Set[int]
    attributes: Dict[str, Set[str]]
    text_patterns: List[str]
    parent_elements: Set[str]
    child_elements: Set[str]

@dataclass
class XMLSchema:
    """Complete XML schema information"""
    root_element: str
    namespaces: Dict[str, str]
    elements: Dict[str, ElementInfo]
    max_depth: int
    total_elements: int
    structure_tree: Dict[str, Any]
    sample_paths: List[str]

class XMLSchemaAnalyzer:
    def __init__(self, max_samples=3, max_text_length=100, max_analysis_depth=15):
        self.max_samples = max_samples
        self.max_text_length = max_text_length
        self.max_analysis_depth = max_analysis_depth  # Prevent infinite analysis
        
        self.elements = defaultdict(lambda: ElementInfo(
            tag="", count=0, depth_levels=set(), attributes=defaultdict(set),
            text_patterns=[], parent_elements=set(), child_elements=set()
        ))
        self.namespaces = {}
        self.structure_tree = {}
        self.sample_paths = []
        self.max_depth = 0
        
        # Set a reasonable recursion limit
        sys.setrecursionlimit(3000)

    def clean_tag(self, tag: str) -> str:
        """Remove namespace prefix from tag for cleaner analysis"""
        return tag.split('}')[-1] if '}' in tag else tag

    def get_namespace(self, tag: str) -> Optional[str]:
        """Extract namespace from tag"""
        if '}' in tag:
            return tag.split('}')[0][1:]  # Remove leading {
        return None

    def analyze_file_iterative(self, file_path: str) -> XMLSchema:
        """Analyze XML file using iterative parsing for large files"""
        print(f"Using iterative parsing for large file: {file_path}")
        
        # Use iterparse for memory-efficient parsing
        context = ET.iterparse(file_path, events=('start', 'end', 'start-ns'))
        
        element_stack = []
        path_stack = []
        elements_processed = 0
        max_elements = 50000  # Limit analysis to prevent excessive processing
        
        root_element = None
        
        try:
            for event, elem in context:
                if elements_processed > max_elements:
                    print(f"Reached analysis limit of {max_elements} elements")
                    break
                    
                if event == 'start-ns':
                    # Handle namespace declarations
                    prefix, uri = elem
                    self.namespaces[prefix or 'default'] = uri
                    
                elif event == 'start':
                    if root_element is None:
                        root_element = self.clean_tag(elem.tag)
                    
                    clean_tag = self.clean_tag(elem.tag)
                    depth = len(element_stack)
                    
                    # Limit depth analysis
                    if depth > self.max_analysis_depth:
                        continue
                    
                    self.max_depth = max(self.max_depth, depth)
                    
                    # Update element info
                    element_info = self.elements[clean_tag]
                    element_info.tag = clean_tag
                    element_info.count += 1
                    element_info.depth_levels.add(depth)
                    
                    # Parent-child relationships
                    if element_stack:
                        parent_tag = element_stack[-1]
                        element_info.parent_elements.add(parent_tag)
                        self.elements[parent_tag].child_elements.add(clean_tag)
                    
                    # Analyze attributes (limit to avoid memory issues)
                    for attr_name, attr_value in list(elem.attrib.items())[:10]:
                        clean_attr = self.clean_tag(attr_name)
                        # Limit attribute value length and count
                        if len(element_info.attributes[clean_attr]) < 5:
                            element_info.attributes[clean_attr].add(attr_value[:50])
                    
                    # Store sample paths
                    if depth <= 3 and len(self.sample_paths) < 20:
                        current_path = '/'.join(path_stack + [clean_tag])
                        self.sample_paths.append(current_path)
                    
                    element_stack.append(clean_tag)
                    path_stack.append(clean_tag)
                    elements_processed += 1
                    
                elif event == 'end':
                    clean_tag = self.clean_tag(elem.tag)
                    
                    # Store text content if present and not too deep
                    if elem.text and elem.text.strip() and len(element_stack) <= self.max_analysis_depth:
                        if clean_tag in self.elements:
                            element_info = self.elements[clean_tag]
                            if len(element_info.text_patterns) < self.max_samples:
                                text = elem.text.strip()[:self.max_text_length]
                                if text:  # Only store non-empty text
                                    element_info.text_patterns.append(text)
                    
                    # Pop from stacks
                    if element_stack and element_stack[-1] == clean_tag:
                        element_stack.pop()
                    if path_stack and path_stack[-1] == clean_tag:
                        path_stack.pop()
                    
                    # Clear element to save memory
                    elem.clear()
                    
                # Progress indicator for very large files
                if elements_processed % 10000 == 0 and elements_processed > 0:
                    print(f"Processed {elements_processed:,} elements...")
                    
        except Exception as e:
            print(f"Warning: Parsing stopped early due to: {e}")
            print(f"Analyzed {elements_processed:,} elements before stopping")
        
        # Build structure tree (limited depth to avoid recursion issues)
        self.structure_tree = self._build_structure_tree_iterative()
        
        # Create schema
        total_elements = sum(info.count for info in self.elements.values())
        
        return XMLSchema(
            root_element=root_element or "unknown",
            namespaces=self.namespaces,
            elements=dict(self.elements),  # Convert defaultdict
            max_depth=self.max_depth,
            total_elements=total_elements,
            structure_tree=self.structure_tree,
            sample_paths=self.sample_paths
        )

    def _build_structure_tree_iterative(self) -> Dict[str, Any]:
        """Build structure tree without recursion"""
        tree = {}
        
        # Find root elements (no parents)
        root_elements = []
        for tag, info in self.elements.items():
            if not info.parent_elements:
                root_elements.append(tag)
        
        # Build tree for each root (limit depth)
        for root_tag in root_elements:
            tree[root_tag] = self._build_subtree_limited(root_tag, max_depth=5)
        
        return tree

    def _build_subtree_limited(self, tag: str, current_depth=0, max_depth=5) -> Dict[str, Any]:
        """Build subtree with depth limit to avoid recursion issues"""
        if current_depth >= max_depth or tag not in self.elements:
            return {"truncated": True}
        
        info = self.elements[tag]
        node = {
            "count": info.count,
            "attributes": list(info.attributes.keys())[:5],  # Limit attributes shown
            "has_text": len(info.text_patterns) > 0,
            "children": {}
        }
        
        # Add children (limited)
        for child_tag in list(info.child_elements)[:10]:  # Limit children
            if child_tag in self.elements:
                node["children"][child_tag] = self._build_subtree_limited(
                    child_tag, current_depth + 1, max_depth
                )
        
        return node

    def analyze_file(self, file_path: str) -> XMLSchema:
        """Main analysis method - chooses appropriate strategy"""
        import os
        
        file_size = os.path.getsize(file_path)
        size_mb = file_size / (1024 * 1024)
        
        print(f"File size: {size_mb:.1f} MB")
        
        # Use iterative parsing for files larger than 5MB
        if size_mb > 5:
            return self.analyze_file_iterative(file_path)
        else:
            # For smaller files, use the original method but with safety limits
            return self.analyze_file_iterative(file_path)

    def generate_llm_description(self, schema: XMLSchema) -> str:
        """Generate a concise description suitable for LLM consumption"""
        description = f"""XML Document Schema Analysis

Document Type: {schema.root_element}
Total Elements: {schema.total_elements:,}
Maximum Depth: {schema.max_depth}
Unique Element Types: {len(schema.elements)}

NAMESPACES:
{json.dumps(schema.namespaces, indent=2)}

DOCUMENT STRUCTURE:
Root: {schema.root_element}
Sample Paths: {', '.join(schema.sample_paths[:10])}

KEY ELEMENTS (Top 10 by frequency):
"""
        
        # Sort elements by count
        sorted_elements = sorted(schema.elements.items(), 
                               key=lambda x: x[1].count, reverse=True)[:10]
        
        for tag, info in sorted_elements:
            attrs_summary = f"[{len(info.attributes)} attrs]" if info.attributes else "[no attrs]"
            text_summary = "[has text]" if info.text_patterns else "[no text]"
            children_summary = f"[{len(info.child_elements)} children]" if info.child_elements else "[leaf]"
            
            description += f"\n- {tag}: {info.count:,} occurrences, depths {sorted(info.depth_levels)[:5]} {attrs_summary} {text_summary} {children_summary}"
            
            # Show key attributes
            if info.attributes:
                key_attrs = list(info.attributes.keys())[:3]
                description += f"\n  Key attributes: {', '.join(key_attrs)}"
            
            # Show sample text
            if info.text_patterns:
                sample_text = info.text_patterns[0][:50]
                description += f"\n  Sample text: \"{sample_text}...\""

        # Simplified structure tree (avoid deep nesting in output)
        description += f"\n\nSTRUCTURE SUMMARY:\n"
        for root_elem, tree_info in list(schema.structure_tree.items())[:3]:
            description += f"- {root_elem}: {tree_info.get('count', 0)} occurrences\n"
            if 'children' in tree_info:
                child_count = len(tree_info['children'])
                if child_count > 0:
                    child_names = list(tree_info['children'].keys())[:5]
                    description += f"  Children ({child_count}): {', '.join(child_names)}\n"
        
        return description

def analyze_xml_file(file_path: str, output_json: bool = False) -> str:
    """Main function to analyze XML file"""
    analyzer = XMLSchemaAnalyzer(max_samples=3, max_analysis_depth=15)
    schema = analyzer.analyze_file(file_path)
    
    if output_json:
        # Convert to dict for JSON serialization
        schema_dict = asdict(schema)
        # Convert sets to lists for JSON
        for element_info in schema_dict['elements'].values():
            if 'depth_levels' in element_info:
                element_info['depth_levels'] = list(element_info['depth_levels'])
            if 'parent_elements' in element_info:
                element_info['parent_elements'] = list(element_info['parent_elements'])
            if 'child_elements' in element_info:
                element_info['child_elements'] = list(element_info['child_elements'])
            if 'attributes' in element_info:
                for attr_name in element_info['attributes']:
                    element_info['attributes'][attr_name] = list(element_info['attributes'][attr_name])
        
        return json.dumps(schema_dict, indent=2)
    else:
        return analyzer.generate_llm_description(schema)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python xml_analyzer.py <xml_file> [--json]")
        sys.exit(1)
    
    file_path = sys.argv[1]
    output_json = "--json" in sys.argv
    
    try:
        result = analyze_xml_file(file_path, output_json)
        print(result)
    except Exception as e:
        print(f"Error analyzing XML file: {e}")
        sys.exit(1)
</file>

<file path="xml_schema_analyzer.py">
#!/usr/bin/env python3
"""
XML Schema Analyzer for LLM Agent Framework

This tool deterministically analyzes XML files to extract:
1. Document structure and hierarchy
2. Element patterns and frequencies
3. Attribute schemas
4. Namespace information
5. Sample data for semantic understanding
6. Compact schema description for LLM consumption

Designed to handle large XML files efficiently.
"""

import xml.etree.ElementTree as ET
from collections import defaultdict, Counter
import json
import re
from typing import Dict, List, Set, Any, Optional
from dataclasses import dataclass, asdict
import itertools

@dataclass
class ElementInfo:
    """Information about an XML element"""
    tag: str
    count: int
    depth_levels: Set[int]
    attributes: Dict[str, Set[str]]  # attr_name -> set of observed values
    text_patterns: List[str]  # Sample text content
    parent_elements: Set[str]
    child_elements: Set[str]

@dataclass
class XMLSchema:
    """Complete XML schema information"""
    root_element: str
    namespaces: Dict[str, str]
    elements: Dict[str, ElementInfo]
    max_depth: int
    total_elements: int
    structure_tree: Dict[str, Any]
    sample_paths: List[str]

class XMLSchemaAnalyzer:
    def __init__(self, max_samples=5, max_text_length=100):
        self.max_samples = max_samples
        self.max_text_length = max_text_length
        self.elements = defaultdict(lambda: ElementInfo(
            tag="", count=0, depth_levels=set(), attributes=defaultdict(set),
            text_patterns=[], parent_elements=set(), child_elements=set()
        ))
        self.namespaces = {}
        self.structure_tree = {}
        self.sample_paths = []
        self.max_depth = 0

    def clean_tag(self, tag: str) -> str:
        """Remove namespace prefix from tag for cleaner analysis"""
        return tag.split('}')[-1] if '}' in tag else tag

    def get_namespace(self, tag: str) -> Optional[str]:
        """Extract namespace from tag"""
        if '}' in tag:
            return tag.split('}')[0][1:]  # Remove leading {
        return None

    def analyze_element(self, elem, depth=0, parent_tag=None, path=""):
        """Recursively analyze XML element"""
        self.max_depth = max(self.max_depth, depth)
        
        # Clean tag name
        clean_tag = self.clean_tag(elem.tag)
        full_path = f"{path}/{clean_tag}" if path else clean_tag
        
        # Store namespace info
        ns = self.get_namespace(elem.tag)
        if ns and ns not in self.namespaces:
            # Try to find prefix in element's namespace declarations
            for key, value in elem.attrib.items():
                if value == ns and key.startswith('xmlns:'):
                    self.namespaces[key[6:]] = ns
                elif key == 'xmlns' and value == ns:
                    self.namespaces['default'] = ns

        # Update element info
        element_info = self.elements[clean_tag]
        element_info.tag = clean_tag
        element_info.count += 1
        element_info.depth_levels.add(depth)
        
        # Add parent relationship
        if parent_tag:
            element_info.parent_elements.add(parent_tag)
            self.elements[parent_tag].child_elements.add(clean_tag)

        # Analyze attributes
        for attr_name, attr_value in elem.attrib.items():
            clean_attr = self.clean_tag(attr_name)
            element_info.attributes[clean_attr].add(attr_value[:50])  # Limit attr value length

        # Analyze text content
        if elem.text and elem.text.strip():
            text = elem.text.strip()[:self.max_text_length]
            if len(element_info.text_patterns) < self.max_samples:
                element_info.text_patterns.append(text)

        # Sample some interesting paths for LLM context
        if depth <= 3 and len(self.sample_paths) < 20:
            self.sample_paths.append(full_path)

        # Recursively analyze children
        for child in elem:
            self.analyze_element(child, depth + 1, clean_tag, full_path)

    def build_structure_tree(self) -> Dict[str, Any]:
        """Build hierarchical structure representation"""
        tree = {}
        
        for tag, info in self.elements.items():
            if not info.parent_elements:  # Root elements
                tree[tag] = self._build_subtree(tag)
        
        return tree

    def _build_subtree(self, tag: str) -> Dict[str, Any]:
        """Recursively build structure subtree"""
        info = self.elements[tag]
        node = {
            "count": info.count,
            "attributes": {k: f"{len(v)} unique values" for k, v in info.attributes.items()},
            "has_text": len(info.text_patterns) > 0,
            "children": {}
        }
        
        for child_tag in info.child_elements:
            node["children"][child_tag] = self._build_subtree(child_tag)
        
        return node

    def analyze_file(self, file_path: str) -> XMLSchema:
        """Analyze XML file and return schema information"""
        print(f"Analyzing XML file: {file_path}")
        
        # Parse XML iteratively for large files
        context = ET.iterparse(file_path, events=('start', 'end', 'start-ns'))
        
        root = None
        namespace_stack = [{}]
        
        for event, elem in context:
            if event == 'start-ns':
                # Handle namespace declarations
                prefix, uri = elem
                self.namespaces[prefix or 'default'] = uri
                namespace_stack[-1][prefix or 'default'] = uri
            
            elif event == 'start':
                if root is None:
                    root = elem
                    self.analyze_element(elem)
                    break  # Only analyze from root
        
        # Build structure tree
        self.structure_tree = self.build_structure_tree()
        
        # Convert elements dict for serialization
        elements_dict = {}
        for tag, info in self.elements.items():
            elements_dict[tag] = ElementInfo(
                tag=info.tag,
                count=info.count,
                depth_levels=sorted(info.depth_levels),
                attributes={k: list(v)[:5] for k, v in info.attributes.items()},  # Limit samples
                text_patterns=info.text_patterns[:3],  # Limit samples
                parent_elements=list(info.parent_elements),
                child_elements=list(info.child_elements)
            )

        return XMLSchema(
            root_element=self.clean_tag(root.tag) if root is not None else "",
            namespaces=self.namespaces,
            elements=elements_dict,
            max_depth=self.max_depth,
            total_elements=sum(info.count for info in self.elements.values()),
            structure_tree=self.structure_tree,
            sample_paths=self.sample_paths
        )

    def generate_llm_description(self, schema: XMLSchema) -> str:
        """Generate a concise description suitable for LLM consumption"""
        description = f"""XML Document Schema Analysis

Document Type: {schema.root_element}
Total Elements: {schema.total_elements:,}
Maximum Depth: {schema.max_depth}
Unique Element Types: {len(schema.elements)}

NAMESPACES:
{json.dumps(schema.namespaces, indent=2)}

DOCUMENT STRUCTURE:
Root: {schema.root_element}
Sample Paths: {', '.join(schema.sample_paths[:10])}

KEY ELEMENTS (Top 10 by frequency):
"""
        
        # Sort elements by count
        sorted_elements = sorted(schema.elements.items(), 
                               key=lambda x: x[1].count, reverse=True)[:10]
        
        for tag, info in sorted_elements:
            attrs_summary = f"[{len(info.attributes)} attrs]" if info.attributes else "[no attrs]"
            text_summary = "[has text]" if info.text_patterns else "[no text]"
            children_summary = f"[{len(info.child_elements)} children]" if info.child_elements else "[leaf]"
            
            description += f"\n- {tag}: {info.count:,} occurrences, depths {info.depth_levels} {attrs_summary} {text_summary} {children_summary}"
            
            # Show key attributes
            if info.attributes:
                key_attrs = list(info.attributes.keys())[:3]
                description += f"\n  Key attributes: {', '.join(key_attrs)}"
            
            # Show sample text
            if info.text_patterns:
                sample_text = info.text_patterns[0][:50]
                description += f"\n  Sample text: \"{sample_text}...\""

        description += f"\n\nHIERARCHICAL STRUCTURE:\n{json.dumps(schema.structure_tree, indent=2)}"
        
        return description

def analyze_xml_file(file_path: str, output_json: bool = False) -> str:
    """Main function to analyze XML file"""
    analyzer = XMLSchemaAnalyzer()
    schema = analyzer.analyze_file(file_path)
    
    if output_json:
        # Convert to dict for JSON serialization
        schema_dict = asdict(schema)
        # Convert sets to lists for JSON
        for element_info in schema_dict['elements'].values():
            element_info['depth_levels'] = list(element_info['depth_levels'])
            element_info['parent_elements'] = list(element_info['parent_elements'])
            element_info['child_elements'] = list(element_info['child_elements'])
            for attr_name in element_info['attributes']:
                element_info['attributes'][attr_name] = list(element_info['attributes'][attr_name])
        
        return json.dumps(schema_dict, indent=2)
    else:
        return analyzer.generate_llm_description(schema)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python xml_analyzer.py <xml_file> [--json]")
        sys.exit(1)
    
    file_path = sys.argv[1]
    output_json = "--json" in sys.argv
    
    try:
        result = analyze_xml_file(file_path, output_json)
        print(result)
    except Exception as e:
        print(f"Error analyzing XML file: {e}")
        sys.exit(1)
</file>

<file path="xml_specialized_handlers.py">
#!/usr/bin/env python3
"""
XML Specialized Handlers System

This module provides a flexible framework for detecting and handling different XML document types.
Each handler provides specialized analysis and extraction logic for its document type.

Key features:
- Automatic document type detection
- Pluggable handler architecture
- Type-specific analysis and insights
- Standardized output format
"""

import xml.etree.ElementTree as ET
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Type, Tuple
from dataclasses import dataclass, field
import re
from pathlib import Path
import json

@dataclass
class DocumentTypeInfo:
    """Information about a detected document type"""
    type_name: str
    confidence: float  # 0.0 to 1.0
    version: Optional[str] = None
    schema_uri: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class SpecializedAnalysis:
    """Results from specialized handler analysis"""
    document_type: str
    key_findings: Dict[str, Any]
    recommendations: List[str]
    data_inventory: Dict[str, int]  # What types of data found and counts
    ai_use_cases: List[str]  # Potential AI/ML applications
    structured_data: Dict[str, Any]  # Extracted structured data
    quality_metrics: Dict[str, float]  # Data quality indicators

class XMLHandler(ABC):
    """Abstract base class for XML document handlers"""
    
    @abstractmethod
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        """
        Check if this handler can process the document
        Returns: (can_handle: bool, confidence: float)
        """
        pass
    
    @abstractmethod
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        """Detect specific document type and version"""
        pass
    
    @abstractmethod
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        """Perform specialized analysis on the document"""
        pass
    
    @abstractmethod
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        """Extract the most important data from this document type"""
        pass

class SCAPHandler(XMLHandler):
    """Handler for SCAP (Security Content Automation Protocol) documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for SCAP-specific namespaces and elements
        scap_indicators = [
            'http://scap.nist.gov/schema/',
            'asset-report-collection',
            'data-stream-collection',
            'xccdf',
            'oval'
        ]
        
        score = 0.0
        if any(uri in str(namespaces.values()) for uri in scap_indicators[:1]):
            score += 0.5
        if root.tag.endswith('asset-report-collection'):
            score += 0.3
        if 'xccdf' in str(namespaces.values()).lower():
            score += 0.2
            
        return score > 0.5, score
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        version = None
        schema_uri = None
        
        # Extract version from namespaces
        for prefix, uri in namespaces.items():
            if 'scap.nist.gov' in uri:
                schema_uri = uri
                # Extract version from URI if present
                version_match = re.search(r'/(\d+\.\d+)/?$', uri)
                if version_match:
                    version = version_match.group(1)
        
        return DocumentTypeInfo(
            type_name="SCAP Security Report",
            confidence=0.9,
            version=version,
            schema_uri=schema_uri,
            metadata={
                "standard": "NIST SCAP",
                "category": "security_compliance"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {}
        data_inventory = {}
        
        # Analyze SCAP-specific elements
        # Count security rules
        rules = root.findall('.//*[@id]')
        findings['total_rules'] = len(rules)
        
        # Count vulnerabilities/findings
        findings['vulnerabilities'] = self._count_vulnerabilities(root)
        
        # Extract compliance status
        findings['compliance_summary'] = self._extract_compliance_summary(root)
        
        recommendations = [
            "Use for automated compliance monitoring",
            "Extract failed rules for remediation workflows",
            "Trend analysis on compliance scores over time",
            "Risk scoring based on vulnerability severity"
        ]
        
        ai_use_cases = [
            "Automated compliance report generation",
            "Predictive risk analysis",
            "Remediation recommendation engine",
            "Compliance trend forecasting",
            "Security posture classification"
        ]
        
        return SpecializedAnalysis(
            document_type="SCAP Security Report",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=data_inventory,
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_quality_metrics(root)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        # Extract key SCAP data
        return {
            "scan_results": self._extract_scan_results(root),
            "system_info": self._extract_system_info(root),
            "compliance_scores": self._extract_compliance_scores(root)
        }
    
    def _count_vulnerabilities(self, root: ET.Element) -> Dict[str, int]:
        # Implementation for counting vulnerabilities by severity
        return {"high": 0, "medium": 0, "low": 0}
    
    def _extract_compliance_summary(self, root: ET.Element) -> Dict[str, Any]:
        # Implementation for extracting compliance summary
        return {}
    
    def _extract_scan_results(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Implementation for extracting scan results
        return []
    
    def _extract_system_info(self, root: ET.Element) -> Dict[str, Any]:
        # Implementation for extracting system information
        return {}
    
    def _extract_compliance_scores(self, root: ET.Element) -> Dict[str, float]:
        # Implementation for extracting compliance scores
        return {}
    
    def _calculate_quality_metrics(self, root: ET.Element) -> Dict[str, float]:
        return {
            "completeness": 0.85,
            "consistency": 0.90,
            "data_density": 0.75
        }

class RSSHandler(XMLHandler):
    """Handler for RSS feed documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        if root.tag == 'rss' or root.tag.endswith('}rss'):
            return True, 1.0
        if root.tag == 'feed':  # Atom feeds
            return True, 0.9
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        version = root.get('version', '2.0')
        feed_type = 'RSS' if root.tag.endswith('rss') else 'Atom'
        
        return DocumentTypeInfo(
            type_name=f"{feed_type} Feed",
            confidence=1.0,
            version=version,
            metadata={
                "standard": feed_type,
                "category": "content_syndication"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        channel = root.find('.//channel') or root
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        findings = {
            'total_items': len(items),
            'has_descriptions': sum(1 for item in items if item.find('.//description') is not None),
            'has_dates': sum(1 for item in items if item.find('.//pubDate') is not None),
            'categories': self._extract_categories(items)
        }
        
        recommendations = [
            "Use for content aggregation and analysis",
            "Extract for trend analysis and topic modeling",
            "Monitor for content updates and changes"
        ]
        
        ai_use_cases = [
            "Content categorization and tagging",
            "Trend detection and analysis",
            "Sentiment analysis on articles",
            "Topic modeling and clustering",
            "Content recommendation systems"
        ]
        
        return SpecializedAnalysis(
            document_type="RSS/Atom Feed",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={'articles': len(items), 'categories': len(findings['categories'])},
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_feed_quality(root, items)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        return {
            'feed_metadata': self._extract_feed_metadata(root),
            'items': [self._extract_item_data(item) for item in items[:10]]  # First 10 items
        }
    
    def _extract_categories(self, items) -> List[str]:
        categories = set()
        for item in items:
            for cat in item.findall('.//category'):
                if cat.text:
                    categories.add(cat.text)
        return list(categories)
    
    def _extract_feed_metadata(self, root: ET.Element) -> Dict[str, Any]:
        channel = root.find('.//channel') or root
        return {
            'title': getattr(channel.find('.//title'), 'text', None),
            'description': getattr(channel.find('.//description'), 'text', None),
            'link': getattr(channel.find('.//link'), 'text', None)
        }
    
    def _extract_item_data(self, item: ET.Element) -> Dict[str, Any]:
        return {
            'title': getattr(item.find('.//title'), 'text', None),
            'description': getattr(item.find('.//description'), 'text', None),
            'pubDate': getattr(item.find('.//pubDate'), 'text', None),
            'link': getattr(item.find('.//link'), 'text', None)
        }
    
    def _calculate_feed_quality(self, root: ET.Element, items: List[ET.Element]) -> Dict[str, float]:
        total = len(items)
        if total == 0:
            return {"completeness": 0.0, "consistency": 0.0, "data_density": 0.0}
        
        with_desc = sum(1 for item in items if item.find('.//description') is not None)
        with_date = sum(1 for item in items if item.find('.//pubDate') is not None)
        
        return {
            "completeness": (with_desc + with_date) / (2 * total),
            "consistency": 1.0 if with_desc == total else with_desc / total,
            "data_density": 0.8  # Typical for RSS feeds
        }

class SVGHandler(XMLHandler):
    """Handler for SVG (Scalable Vector Graphics) documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        if root.tag == '{http://www.w3.org/2000/svg}svg' or root.tag == 'svg':
            return True, 1.0
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        return DocumentTypeInfo(
            type_name="SVG Graphics",
            confidence=1.0,
            version=root.get('version', '1.1'),
            schema_uri="http://www.w3.org/2000/svg",
            metadata={
                "standard": "W3C SVG",
                "category": "graphics"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'dimensions': {
                'width': root.get('width'),
                'height': root.get('height'),
                'viewBox': root.get('viewBox')
            },
            'element_types': self._count_svg_elements(root),
            'has_animations': self._check_animations(root),
            'has_scripts': len(root.findall('.//script')) > 0,
            'complexity_score': self._calculate_complexity(root)
        }
        
        recommendations = [
            "Extract for design system documentation",
            "Analyze for accessibility improvements",
            "Convert to other formats for broader compatibility"
        ]
        
        ai_use_cases = [
            "Automatic icon/graphic classification",
            "Design pattern recognition",
            "Accessibility analysis",
            "Style extraction for design systems",
            "Vector graphic optimization"
        ]
        
        return SpecializedAnalysis(
            document_type="SVG Graphics",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=findings['element_types'],
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_svg_quality(root)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'metadata': self._extract_svg_metadata(root),
            'structure': self._extract_structure(root),
            'styles': self._extract_styles(root)
        }
    
    def _count_svg_elements(self, root: ET.Element) -> Dict[str, int]:
        elements = {}
        for elem in root.iter():
            tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            elements[tag] = elements.get(tag, 0) + 1
        return elements
    
    def _check_animations(self, root: ET.Element) -> bool:
        animation_tags = ['animate', 'animateTransform', 'animateMotion', 'set']
        for tag in animation_tags:
            if root.find(f'.//{{{root.tag.split("}")[0][1:] if "}" in root.tag else ""}}{tag}'):
                return True
        return False
    
    def _calculate_complexity(self, root: ET.Element) -> float:
        total_elements = len(list(root.iter()))
        return min(total_elements / 100.0, 1.0)
    
    def _extract_svg_metadata(self, root: ET.Element) -> Dict[str, Any]:
        metadata = {}
        for elem in root:
            if elem.tag.endswith('metadata'):
                # Extract metadata content
                pass
        return metadata
    
    def _extract_structure(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'groups': len(root.findall('.//g')),
            'paths': len(root.findall('.//path')),
            'max_depth': self._calculate_max_depth(root)
        }
    
    def _extract_styles(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'inline_styles': len([e for e in root.iter() if e.get('style')]),
            'classes': len(set(e.get('class', '') for e in root.iter() if e.get('class')))
        }
    
    def _calculate_max_depth(self, elem: ET.Element, depth: int = 0) -> int:
        if not list(elem):
            return depth
        return max(self._calculate_max_depth(child, depth + 1) for child in elem)
    
    def _calculate_svg_quality(self, root: ET.Element) -> Dict[str, float]:
        has_viewbox = 1.0 if root.get('viewBox') else 0.0
        has_title = 1.0 if root.find('.//title') is not None else 0.0
        
        return {
            "completeness": (has_viewbox + has_title) / 2,
            "accessibility": has_title,
            "scalability": has_viewbox
        }

class GenericXMLHandler(XMLHandler):
    """Fallback handler for generic XML documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # This handler can handle any XML
        return True, 0.1  # Low confidence as it's a fallback
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Try to infer type from root element and namespaces
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        return DocumentTypeInfo(
            type_name=f"Generic XML ({root_tag})",
            confidence=0.5,
            metadata={
                "root_element": root_tag,
                "namespace_count": len(namespaces)
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'structure': self._analyze_structure(root),
            'data_patterns': self._detect_patterns(root),
            'attribute_usage': self._analyze_attributes(root)
        }
        
        recommendations = [
            "Review structure for data extraction opportunities",
            "Consider creating a specialized handler for this document type",
            "Analyze repeating patterns for structured data extraction"
        ]
        
        ai_use_cases = [
            "Schema learning and validation",
            "Data extraction and transformation",
            "Pattern recognition",
            "Anomaly detection in structure"
        ]
        
        return SpecializedAnalysis(
            document_type="Generic XML",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=self._inventory_data(root),
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._analyze_quality(root)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'sample_data': self._extract_samples(root),
            'schema_inference': self._infer_schema(root)
        }
    
    def _analyze_structure(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'max_depth': self._calculate_depth(root),
            'element_count': len(list(root.iter())),
            'unique_paths': len(self._get_unique_paths(root))
        }
    
    def _detect_patterns(self, root: ET.Element) -> Dict[str, Any]:
        # Detect repeating structures
        element_counts = {}
        for elem in root.iter():
            tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            element_counts[tag] = element_counts.get(tag, 0) + 1
        
        return {
            'repeating_elements': {k: v for k, v in element_counts.items() if v > 5},
            'likely_records': [k for k, v in element_counts.items() if v > 10]
        }
    
    def _analyze_attributes(self, root: ET.Element) -> Dict[str, Any]:
        attr_usage = {}
        for elem in root.iter():
            for attr in elem.attrib:
                attr_usage[attr] = attr_usage.get(attr, 0) + 1
        return attr_usage
    
    def _inventory_data(self, root: ET.Element) -> Dict[str, int]:
        inventory = {}
        for elem in root.iter():
            tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            inventory[tag] = inventory.get(tag, 0) + 1
        return inventory
    
    def _extract_samples(self, root: ET.Element, max_samples: int = 5) -> List[Dict[str, Any]]:
        samples = []
        for i, elem in enumerate(root.iter()):
            if i >= max_samples:
                break
            if elem.text and elem.text.strip():
                samples.append({
                    'path': self._get_path(elem),
                    'tag': elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag,
                    'text': elem.text.strip()[:100],
                    'attributes': dict(elem.attrib)
                })
        return samples
    
    def _infer_schema(self, root: ET.Element) -> Dict[str, Any]:
        # Basic schema inference
        return {
            'probable_record_types': self._detect_patterns(root)['likely_records'],
            'hierarchical': self._calculate_depth(root) > 3
        }
    
    def _calculate_depth(self, elem: ET.Element, depth: int = 0) -> int:
        if not list(elem):
            return depth
        return max(self._calculate_depth(child, depth + 1) for child in elem)
    
    def _get_unique_paths(self, root: ET.Element) -> set:
        paths = set()
        
        def traverse(elem, path):
            current_path = f"{path}/{elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag}"
            paths.add(current_path)
            for child in elem:
                traverse(child, current_path)
        
        traverse(root, "")
        return paths
    
    def _get_path(self, elem: ET.Element) -> str:
        # Simple path extraction (would need more complex logic for full path)
        return elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
    
    def _analyze_quality(self, root: ET.Element) -> Dict[str, float]:
        total_elements = len(list(root.iter()))
        elements_with_text = sum(1 for e in root.iter() if e.text and e.text.strip())
        elements_with_attrs = sum(1 for e in root.iter() if e.attrib)
        
        return {
            "data_density": elements_with_text / total_elements if total_elements > 0 else 0,
            "attribute_usage": elements_with_attrs / total_elements if total_elements > 0 else 0,
            "structure_consistency": 0.7  # Would need more analysis
        }

class XMLDocumentAnalyzer:
    """Main analyzer that uses specialized handlers"""
    
    def __init__(self):
        # Register all handlers
        self.handlers: List[Type[XMLHandler]] = [
            SCAPHandler(),
            RSSHandler(),
            SVGHandler(),
            # Add more handlers here as needed
            GenericXMLHandler()  # Always last as fallback
        ]
        
        # Try to import additional handlers
        try:
            from additional_xml_handlers import (
                MavenPOMHandler, Log4jConfigHandler, SpringConfigHandler,
                DocBookHandler, SitemapHandler
            )
            # Insert before GenericXMLHandler
            self.handlers.insert(-1, MavenPOMHandler())
            self.handlers.insert(-1, Log4jConfigHandler())
            self.handlers.insert(-1, SpringConfigHandler())
            self.handlers.insert(-1, DocBookHandler())
            self.handlers.insert(-1, SitemapHandler())
        except ImportError:
            # Additional handlers not available
            pass
    
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """Analyze an XML document using the appropriate handler"""
        
        # Parse the document
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
        except ET.ParseError as e:
            return {
                "error": f"Failed to parse XML: {e}",
                "file_path": file_path
            }
        
        # Extract namespaces
        namespaces = self._extract_namespaces(root)
        
        # Find the best handler
        best_handler = None
        best_confidence = 0.0
        
        for handler in self.handlers:
            can_handle, confidence = handler.can_handle(root, namespaces)
            if can_handle and confidence > best_confidence:
                best_handler = handler
                best_confidence = confidence
        
        if not best_handler:
            best_handler = self.handlers[-1]  # Use generic handler
        
        # Detect document type
        doc_type = best_handler.detect_type(root, namespaces)
        
        # Perform specialized analysis
        analysis = best_handler.analyze(root, file_path)
        
        # Combine results
        return {
            "file_path": file_path,
            "document_type": doc_type,
            "handler_used": best_handler.__class__.__name__,
            "confidence": best_confidence,
            "analysis": analysis,
            "namespaces": namespaces,
            "file_size": Path(file_path).stat().st_size
        }
    
    def _extract_namespaces(self, root: ET.Element) -> Dict[str, str]:
        """Extract all namespaces from the document"""
        namespaces = {}
        
        # Get namespaces from root element
        for key, value in root.attrib.items():
            if key.startswith('xmlns'):
                prefix = key.split(':')[1] if ':' in key else 'default'
                namespaces[prefix] = value
        
        # Also check for namespaces in element tags
        for elem in root.iter():
            if '}' in elem.tag:
                uri = elem.tag.split('}')[0][1:]
                # Try to find a prefix for this URI
                prefix = None
                for p, u in namespaces.items():
                    if u == uri:
                        prefix = p
                        break
                if not prefix:
                    prefix = f"ns{len(namespaces)}"
                    namespaces[prefix] = uri
        
        return namespaces

# Example usage
if __name__ == "__main__":
    analyzer = XMLDocumentAnalyzer()
    
    # Example: Analyze a file
    result = analyzer.analyze_document("sample_data/example.xml")
    
    print(json.dumps(result, indent=2, default=str))
</file>

<file path="xml-architecture-diagram.svg">
<?xml version="1.0" encoding="UTF-8"?>
<svg width="800" height="600" viewBox="0 0 800 600" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <filter id="shadow" x="-50%" y="-50%" width="200%" height="200%">
      <feGaussianBlur in="SourceAlpha" stdDeviation="3"/>
      <feOffset dx="2" dy="2" result="offsetblur"/>
      <feComponentTransfer>
        <feFuncA type="linear" slope="0.2"/>
      </feComponentTransfer>
      <feMerge>
        <feMergeNode/>
        <feMergeNode in="SourceGraphic"/>
      </feMerge>
    </filter>
  </defs>
  
  <!-- Title -->
  <text x="400" y="30" text-anchor="middle" font-size="24" font-weight="bold" fill="#333">
    XML Analysis Framework Architecture
  </text>
  
  <!-- Input Layer -->
  <g id="input-layer">
    <rect x="50" y="60" width="120" height="60" rx="5" fill="#e8f4f8" stroke="#4a90e2" stroke-width="2" filter="url(#shadow)"/>
    <text x="110" y="95" text-anchor="middle" font-size="14" fill="#333">XML Files</text>
  </g>
  
  <!-- Core Analyzer -->
  <g id="core-analyzer">
    <rect x="250" y="60" width="300" height="120" rx="5" fill="#f0f8ff" stroke="#4a90e2" stroke-width="2" filter="url(#shadow)"/>
    <text x="400" y="85" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">Core Analyzer</text>
    
    <!-- Sub-components -->
    <rect x="270" y="100" width="120" height="30" rx="3" fill="#e6f3ff" stroke="#7bb3e8" stroke-width="1"/>
    <text x="330" y="120" text-anchor="middle" font-size="12" fill="#333">Schema Analyzer</text>
    
    <rect x="410" y="100" width="120" height="30" rx="3" fill="#e6f3ff" stroke="#7bb3e8" stroke-width="1"/>
    <text x="470" y="120" text-anchor="middle" font-size="12" fill="#333">Type Detector</text>
    
    <rect x="340" y="140" width="120" height="30" rx="3" fill="#e6f3ff" stroke="#7bb3e8" stroke-width="1"/>
    <text x="400" y="160" text-anchor="middle" font-size="12" fill="#333">Basic Analysis</text>
  </g>
  
  <!-- Specialized Handlers -->
  <g id="handlers">
    <rect x="50" y="220" width="700" height="140" rx="5" fill="#fff5e6" stroke="#ff9800" stroke-width="2" filter="url(#shadow)"/>
    <text x="400" y="245" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">Specialized Handlers</text>
    
    <!-- Individual handlers -->
    <rect x="70" y="260" width="90" height="40" rx="3" fill="#ffe0b2" stroke="#ff9800" stroke-width="1"/>
    <text x="115" y="285" text-anchor="middle" font-size="11" fill="#333">SCAP</text>
    
    <rect x="170" y="260" width="90" height="40" rx="3" fill="#ffe0b2" stroke="#ff9800" stroke-width="1"/>
    <text x="215" y="285" text-anchor="middle" font-size="11" fill="#333">RSS/Atom</text>
    
    <rect x="270" y="260" width="90" height="40" rx="3" fill="#ffe0b2" stroke="#ff9800" stroke-width="1"/>
    <text x="315" y="285" text-anchor="middle" font-size="11" fill="#333">Maven POM</text>
    
    <rect x="370" y="260" width="90" height="40" rx="3" fill="#ffe0b2" stroke="#ff9800" stroke-width="1"/>
    <text x="415" y="285" text-anchor="middle" font-size="11" fill="#333">Log4j</text>
    
    <rect x="470" y="260" width="90" height="40" rx="3" fill="#ffe0b2" stroke="#ff9800" stroke-width="1"/>
    <text x="515" y="285" text-anchor="middle" font-size="11" fill="#333">Spring</text>
    
    <rect x="570" y="260" width="90" height="40" rx="3" fill="#ffe0b2" stroke="#ff9800" stroke-width="1"/>
    <text x="615" y="285" text-anchor="middle" font-size="11" fill="#333">DocBook</text>
    
    <rect x="320" y="310" width="160" height="40" rx="3" fill="#ffccbc" stroke="#ff9800" stroke-width="1"/>
    <text x="400" y="335" text-anchor="middle" font-size="11" fill="#333">Generic (Fallback)</text>
  </g>
  
  <!-- Chunking Strategies -->
  <g id="chunking">
    <rect x="50" y="380" width="340" height="100" rx="5" fill="#f0fff0" stroke="#4caf50" stroke-width="2" filter="url(#shadow)"/>
    <text x="220" y="405" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">Chunking Strategies</text>
    
    <rect x="70" y="425" width="100" height="30" rx="3" fill="#e8f5e9" stroke="#4caf50" stroke-width="1"/>
    <text x="120" y="445" text-anchor="middle" font-size="11" fill="#333">Hierarchical</text>
    
    <rect x="180" y="425" width="100" height="30" rx="3" fill="#e8f5e9" stroke="#4caf50" stroke-width="1"/>
    <text x="230" y="445" text-anchor="middle" font-size="11" fill="#333">Sliding Window</text>
    
    <rect x="290" y="425" width="90" height="30" rx="3" fill="#e8f5e9" stroke="#4caf50" stroke-width="1"/>
    <text x="335" y="445" text-anchor="middle" font-size="11" fill="#333">Content-Aware</text>
  </g>
  
  <!-- Output -->
  <g id="output">
    <rect x="410" y="380" width="340" height="100" rx="5" fill="#fff0f5" stroke="#e91e63" stroke-width="2" filter="url(#shadow)"/>
    <text x="580" y="405" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">AI-Ready Output</text>
    
    <rect x="430" y="425" width="100" height="30" rx="3" fill="#fce4ec" stroke="#e91e63" stroke-width="1"/>
    <text x="480" y="445" text-anchor="middle" font-size="11" fill="#333">JSON Analysis</text>
    
    <rect x="540" y="425" width="100" height="30" rx="3" fill="#fce4ec" stroke="#e91e63" stroke-width="1"/>
    <text x="590" y="445" text-anchor="middle" font-size="11" fill="#333">LLM Chunks</text>
    
    <rect x="650" y="425" width="90" height="30" rx="3" fill="#fce4ec" stroke="#e91e63" stroke-width="1"/>
    <text x="695" y="445" text-anchor="middle" font-size="11" fill="#333">Use Cases</text>
  </g>
  
  <!-- Arrows -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
    </marker>
  </defs>
  
  <!-- Flow arrows -->
  <path d="M 170 90 L 250 90" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 400 180 L 400 220" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 220 360 L 220 380" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 530 360 L 580 380" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- LLM Integration (future) -->
  <g id="llm-integration">
    <rect x="300" y="520" width="200" height="50" rx="5" fill="#f5f5f5" stroke="#999" stroke-width="2" stroke-dasharray="5,5" filter="url(#shadow)"/>
    <text x="400" y="550" text-anchor="middle" font-size="14" fill="#666" font-style="italic">LLM Integration (Future)</text>
  </g>
  
  <path d="M 580 480 L 400 520" stroke="#999" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead)"/>
</svg>
</file>

<file path="xml-chunking-strategy.py">
#!/usr/bin/env python3
"""
XML Chunking Strategy and Integration Module

This module provides intelligent chunking strategies for XML documents
to prepare them for LLM processing while preserving context and structure.
"""

import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional, Generator, Tuple
from dataclasses import dataclass
import hashlib
import json
from pathlib import Path

@dataclass
class ChunkingConfig:
    """Configuration for chunking strategy"""
    max_chunk_size: int = 3000  # tokens (approximate)
    min_chunk_size: int = 500
    overlap_size: int = 200
    preserve_hierarchy: bool = True
    include_parent_context: bool = True
    semantic_boundaries: List[str] = None  # Element names that are natural boundaries

@dataclass
class XMLChunk:
    """Represents a chunk of XML content"""
    chunk_id: str
    content: str
    element_path: str
    start_line: int
    end_line: int
    parent_context: Optional[str]
    metadata: Dict[str, Any]
    token_estimate: int
    elements_included: List[str]
    
class XMLChunkingStrategy:
    """Base class for different chunking strategies"""
    
    def __init__(self, config: ChunkingConfig = None):
        self.config = config or ChunkingConfig()
        
    def estimate_tokens(self, text: str) -> int:
        """Rough estimation of tokens (words * 1.3)"""
        return int(len(text.split()) * 1.3)
    
    def generate_chunk_id(self, content: str, index: int) -> str:
        """Generate unique chunk ID"""
        hash_content = hashlib.md5(content.encode()).hexdigest()[:8]
        return f"chunk_{index}_{hash_content}"
    
    def chunk_document(self, file_path: str, 
                      specialized_analysis: Dict[str, Any] = None) -> List[XMLChunk]:
        """Chunk an XML document based on the strategy"""
        raise NotImplementedError

class HierarchicalChunking(XMLChunkingStrategy):
    """Chunks based on XML hierarchy, respecting element boundaries"""
    
    def chunk_document(self, file_path: str, 
                      specialized_analysis: Dict[str, Any] = None) -> List[XMLChunk]:
        chunks = []
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        # Determine semantic boundaries based on document type
        if specialized_analysis:
            self.config.semantic_boundaries = self._get_semantic_boundaries(
                specialized_analysis.get('document_type', {}).get('type_name', '')
            )
        
        # Start chunking from root
        chunk_index = 0
        for chunk in self._chunk_element(root, "", chunk_index):
            chunks.append(chunk)
            chunk_index += 1
            
        return chunks
    
    def _chunk_element(self, element: ET.Element, path: str, 
                       start_index: int) -> Generator[XMLChunk, None, None]:
        """Recursively chunk an element and its children"""
        current_path = f"{path}/{element.tag}" if path else element.tag
        
        # Check if this element is a semantic boundary
        if self._is_semantic_boundary(element):
            # Create a chunk for this entire element
            content = ET.tostring(element, encoding='unicode')
            tokens = self.estimate_tokens(content)
            
            if tokens <= self.config.max_chunk_size:
                # Element fits in one chunk
                yield self._create_chunk(element, current_path, start_index, content)
            else:
                # Element too large, need to split children
                yield from self._split_large_element(element, current_path, start_index)
        else:
            # Not a boundary, continue processing children
            yield from self._process_children(element, current_path, start_index)
    
    def _is_semantic_boundary(self, element: ET.Element) -> bool:
        """Check if element is a natural chunking boundary"""
        if not self.config.semantic_boundaries:
            return False
            
        tag = element.tag.split('}')[-1] if '}' in element.tag else element.tag
        return tag in self.config.semantic_boundaries
    
    def _create_chunk(self, element: ET.Element, path: str, 
                     index: int, content: str = None) -> XMLChunk:
        """Create a chunk from an element"""
        if content is None:
            content = ET.tostring(element, encoding='unicode')
            
        # Get parent context if configured
        parent_context = None
        if self.config.include_parent_context:
            parent_context = self._get_parent_context(element)
            
        # Extract metadata
        metadata = {
            'tag': element.tag,
            'attributes': dict(element.attrib),
            'namespace': element.tag.split('}')[0][1:] if '}' in element.tag else None
        }
        
        # Get included elements
        elements_included = list(set(
            e.tag.split('}')[-1] if '}' in e.tag else e.tag 
            for e in element.iter()
        ))
        
        return XMLChunk(
            chunk_id=self.generate_chunk_id(content, index),
            content=content,
            element_path=path,
            start_line=0,  # Would need line tracking for accurate values
            end_line=0,
            parent_context=parent_context,
            metadata=metadata,
            token_estimate=self.estimate_tokens(content),
            elements_included=elements_included
        )
    
    def _split_large_element(self, element: ET.Element, path: str, 
                            start_index: int) -> Generator[XMLChunk, None, None]:
        """Split a large element into multiple chunks"""
        # Strategy: Group children until size limit reached
        current_chunk_elements = []
        current_size = 0
        chunk_index = start_index
        
        for child in element:
            child_content = ET.tostring(child, encoding='unicode')
            child_size = self.estimate_tokens(child_content)
            
            if current_size + child_size > self.config.max_chunk_size and current_chunk_elements:
                # Create chunk from accumulated elements
                yield self._create_chunk_from_elements(
                    current_chunk_elements, element, path, chunk_index
                )
                chunk_index += 1
                current_chunk_elements = [child]
                current_size = child_size
            else:
                current_chunk_elements.append(child)
                current_size += child_size
        
        # Don't forget the last chunk
        if current_chunk_elements:
            yield self._create_chunk_from_elements(
                current_chunk_elements, element, path, chunk_index
            )
    
    def _create_chunk_from_elements(self, elements: List[ET.Element], 
                                   parent: ET.Element, path: str, 
                                   index: int) -> XMLChunk:
        """Create a chunk from a list of elements"""
        # Create a wrapper element
        wrapper = ET.Element(parent.tag, parent.attrib)
        for elem in elements:
            wrapper.append(elem)
            
        return self._create_chunk(wrapper, path, index)
    
    def _process_children(self, element: ET.Element, path: str, 
                         start_index: int) -> Generator[XMLChunk, None, None]:
        """Process children of a non-boundary element"""
        chunk_index = start_index
        for child in element:
            for chunk in self._chunk_element(child, path, chunk_index):
                yield chunk
                chunk_index += 1
    
    def _get_parent_context(self, element: ET.Element) -> str:
        """Get context from parent elements"""
        # This would need access to parent in real implementation
        return f"Parent: {element.tag}"
    
    def _get_semantic_boundaries(self, doc_type: str) -> List[str]:
        """Get semantic boundaries based on document type"""
        boundaries = {
            "SCAP Security Report": ["Rule", "Group", "Benchmark"],
            "RSS/Atom Feed": ["item", "entry"],
            "Maven POM": ["dependency", "plugin", "profile"],
            "Spring Configuration": ["bean", "component-scan"],
            "DocBook Documentation": ["chapter", "section", "article"],
            "Log4j Configuration": ["appender", "logger"]
        }
        
        return boundaries.get(doc_type, ["section", "record", "item", "entry"])

class SlidingWindowChunking(XMLChunkingStrategy):
    """Chunks using a sliding window approach with overlap"""
    
    def chunk_document(self, file_path: str, 
                      specialized_analysis: Dict[str, Any] = None) -> List[XMLChunk]:
        chunks = []
        
        # Read and parse the document
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        # Convert to a list of elements with their content
        elements = self._flatten_elements(root)
        
        # Create chunks with sliding window
        chunk_index = 0
        i = 0
        
        while i < len(elements):
            chunk_elements = []
            chunk_size = 0
            
            # Build chunk up to max size
            j = i
            while j < len(elements) and chunk_size < self.config.max_chunk_size:
                elem_content = elements[j]['content']
                elem_size = self.estimate_tokens(elem_content)
                
                if chunk_size + elem_size <= self.config.max_chunk_size:
                    chunk_elements.append(elements[j])
                    chunk_size += elem_size
                    j += 1
                else:
                    break
            
            # Create chunk if we have content
            if chunk_elements:
                chunk = self._create_chunk_from_flattened(
                    chunk_elements, chunk_index
                )
                chunks.append(chunk)
                chunk_index += 1
                
                # Move window (with overlap)
                overlap_size = 0
                overlap_count = 0
                
                # Count backwards to find overlap point
                for k in range(len(chunk_elements) - 1, -1, -1):
                    overlap_size += self.estimate_tokens(chunk_elements[k]['content'])
                    overlap_count += 1
                    if overlap_size >= self.config.overlap_size:
                        break
                
                i = j - overlap_count + 1
            else:
                i += 1
        
        return chunks
    
    def _flatten_elements(self, root: ET.Element) -> List[Dict[str, Any]]:
        """Flatten XML tree into a list of elements with metadata"""
        flattened = []
        
        def traverse(elem, path=""):
            current_path = f"{path}/{elem.tag}" if path else elem.tag
            
            # Add element info
            flattened.append({
                'element': elem,
                'path': current_path,
                'content': ET.tostring(elem, encoding='unicode'),
                'tag': elem.tag,
                'depth': current_path.count('/')
            })
            
            # Process children
            for child in elem:
                traverse(child, current_path)
        
        traverse(root)
        return flattened
    
    def _create_chunk_from_flattened(self, elements: List[Dict[str, Any]], 
                                    index: int) -> XMLChunk:
        """Create chunk from flattened elements"""
        # Combine content
        content_parts = []
        paths = []
        tags = set()
        
        for elem_info in elements:
            content_parts.append(elem_info['content'])
            paths.append(elem_info['path'])
            tags.add(elem_info['tag'].split('}')[-1] if '}' in elem_info['tag'] else elem_info['tag'])
        
        content = '\n'.join(content_parts)
        
        return XMLChunk(
            chunk_id=self.generate_chunk_id(content, index),
            content=content,
            element_path='; '.join(set(paths[:3])),  # First 3 unique paths
            start_line=0,
            end_line=0,
            parent_context=None,
            metadata={
                'elements_count': len(elements),
                'depth_range': (
                    min(e['depth'] for e in elements),
                    max(e['depth'] for e in elements)
                )
            },
            token_estimate=self.estimate_tokens(content),
            elements_included=list(tags)
        )

class ContentAwareChunking(XMLChunkingStrategy):
    """Chunks based on content type and meaning"""
    
    def __init__(self, config: ChunkingConfig = None):
        super().__init__(config)
        self.content_patterns = {
            'narrative': ['para', 'p', 'description', 'abstract', 'summary'],
            'structured': ['table', 'list', 'itemizedlist', 'orderedlist'],
            'code': ['code', 'programlisting', 'screen', 'computeroutput'],
            'metadata': ['info', 'meta', 'metadata', 'properties']
        }
    
    def chunk_document(self, file_path: str, 
                      specialized_analysis: Dict[str, Any] = None) -> List[XMLChunk]:
        chunks = []
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        # Group elements by content type
        content_groups = self._group_by_content_type(root)
        
        # Create chunks for each content group
        chunk_index = 0
        for content_type, elements in content_groups.items():
            for chunk in self._chunk_content_group(content_type, elements, chunk_index):
                chunks.append(chunk)
                chunk_index += 1
        
        return chunks
    
    def _group_by_content_type(self, root: ET.Element) -> Dict[str, List[ET.Element]]:
        """Group elements by their content type"""
        groups = {
            'narrative': [],
            'structured': [],
            'code': [],
            'metadata': [],
            'other': []
        }
        
        for elem in root.iter():
            content_type = self._determine_content_type(elem)
            groups[content_type].append(elem)
        
        return {k: v for k, v in groups.items() if v}  # Remove empty groups
    
    def _determine_content_type(self, element: ET.Element) -> str:
        """Determine the content type of an element"""
        tag = element.tag.split('}')[-1] if '}' in element.tag else element.tag
        
        for content_type, patterns in self.content_patterns.items():
            if tag.lower() in patterns:
                return content_type
        
        # Check children for hints
        child_tags = [
            child.tag.split('}')[-1] if '}' in child.tag else child.tag 
            for child in element
        ]
        
        for content_type, patterns in self.content_patterns.items():
            if any(ct.lower() in patterns for ct in child_tags):
                return content_type
        
        return 'other'
    
    def _chunk_content_group(self, content_type: str, elements: List[ET.Element], 
                           start_index: int) -> Generator[XMLChunk, None, None]:
        """Create chunks for a group of similar content"""
        # Use different strategies based on content type
        if content_type == 'narrative':
            yield from self._chunk_narrative(elements, start_index)
        elif content_type == 'code':
            yield from self._chunk_code(elements, start_index)
        elif content_type == 'structured':
            yield from self._chunk_structured(elements, start_index)
        else:
            yield from self._chunk_generic(elements, start_index)
    
    def _chunk_narrative(self, elements: List[ET.Element], 
                        start_index: int) -> Generator[XMLChunk, None, None]:
        """Chunk narrative content, trying to keep paragraphs together"""
        current_content = []
        current_size = 0
        chunk_index = start_index
        
        for elem in elements:
            elem_text = ET.tostring(elem, encoding='unicode')
            elem_size = self.estimate_tokens(elem_text)
            
            if current_size + elem_size > self.config.max_chunk_size and current_content:
                # Create chunk
                yield self._create_narrative_chunk(current_content, chunk_index)
                chunk_index += 1
                
                # Start new chunk with overlap
                overlap_elements = self._get_overlap_elements(current_content)
                current_content = overlap_elements + [elem_text]
                current_size = sum(self.estimate_tokens(e) for e in current_content)
            else:
                current_content.append(elem_text)
                current_size += elem_size
        
        if current_content:
            yield self._create_narrative_chunk(current_content, chunk_index)
    
    def _chunk_code(self, elements: List[ET.Element], 
                   start_index: int) -> Generator[XMLChunk, None, None]:
        """Chunk code content, trying to keep code blocks intact"""
        for i, elem in enumerate(elements):
            content = ET.tostring(elem, encoding='unicode')
            tokens = self.estimate_tokens(content)
            
            if tokens <= self.config.max_chunk_size:
                # Single code block fits
                yield XMLChunk(
                    chunk_id=self.generate_chunk_id(content, start_index + i),
                    content=content,
                    element_path=self._get_element_path(elem),
                    start_line=0,
                    end_line=0,
                    parent_context=None,
                    metadata={'content_type': 'code', 'language': elem.get('language', 'unknown')},
                    token_estimate=tokens,
                    elements_included=['code']
                )
            else:
                # Split large code block
                yield from self._split_large_code_block(elem, start_index + i)
    
    def _chunk_structured(self, elements: List[ET.Element], 
                         start_index: int) -> Generator[XMLChunk, None, None]:
        """Chunk structured content like tables and lists"""
        for i, elem in enumerate(elements):
            content = ET.tostring(elem, encoding='unicode')
            
            yield XMLChunk(
                chunk_id=self.generate_chunk_id(content, start_index + i),
                content=content,
                element_path=self._get_element_path(elem),
                start_line=0,
                end_line=0,
                parent_context=None,
                metadata={'content_type': 'structured', 'structure_type': elem.tag},
                token_estimate=self.estimate_tokens(content),
                elements_included=[elem.tag]
            )
    
    def _chunk_generic(self, elements: List[ET.Element], 
                      start_index: int) -> Generator[XMLChunk, None, None]:
        """Generic chunking for other content"""
        for i, elem in enumerate(elements):
            content = ET.tostring(elem, encoding='unicode')
            
            yield XMLChunk(
                chunk_id=self.generate_chunk_id(content, start_index + i),
                content=content,
                element_path=self._get_element_path(elem),
                start_line=0,
                end_line=0,
                parent_context=None,
                metadata={'content_type': 'other'},
                token_estimate=self.estimate_tokens(content),
                elements_included=[elem.tag]
            )
    
    def _create_narrative_chunk(self, content_list: List[str], index: int) -> XMLChunk:
        """Create a chunk from narrative content"""
        content = '\n'.join(content_list)
        
        return XMLChunk(
            chunk_id=self.generate_chunk_id(content, index),
            content=content,
            element_path="narrative_section",
            start_line=0,
            end_line=0,
            parent_context=None,
            metadata={'content_type': 'narrative', 'paragraph_count': len(content_list)},
            token_estimate=self.estimate_tokens(content),
            elements_included=['para', 'p', 'description']
        )
    
    def _get_overlap_elements(self, content_list: List[str]) -> List[str]:
        """Get elements for overlap from the end of content list"""
        overlap_elements = []
        overlap_size = 0
        
        for content in reversed(content_list):
            overlap_size += self.estimate_tokens(content)
            overlap_elements.insert(0, content)
            
            if overlap_size >= self.config.overlap_size:
                break
        
        return overlap_elements
    
    def _split_large_code_block(self, elem: ET.Element, 
                               index: int) -> Generator[XMLChunk, None, None]:
        """Split a large code block into smaller chunks"""
        # This is a simplified version - real implementation would be smarter
        text = elem.text or ""
        lines = text.split('\n')
        
        current_chunk = []
        current_size = 0
        chunk_num = 0
        
        for line in lines:
            line_size = self.estimate_tokens(line)
            
            if current_size + line_size > self.config.max_chunk_size and current_chunk:
                # Create chunk
                content = '\n'.join(current_chunk)
                yield XMLChunk(
                    chunk_id=self.generate_chunk_id(content, index + chunk_num),
                    content=f"<code>{content}</code>",
                    element_path=self._get_element_path(elem),
                    start_line=0,
                    end_line=0,
                    parent_context=None,
                    metadata={
                        'content_type': 'code',
                        'language': elem.get('language', 'unknown'),
                        'part': chunk_num + 1
                    },
                    token_estimate=current_size,
                    elements_included=['code']
                )
                
                chunk_num += 1
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size
        
        # Last chunk
        if current_chunk:
            content = '\n'.join(current_chunk)
            yield XMLChunk(
                chunk_id=self.generate_chunk_id(content, index + chunk_num),
                content=f"<code>{content}</code>",
                element_path=self._get_element_path(elem),
                start_line=0,
                end_line=0,
                parent_context=None,
                metadata={
                    'content_type': 'code',
                    'language': elem.get('language', 'unknown'),
                    'part': chunk_num + 1
                },
                token_estimate=current_size,
                elements_included=['code']
            )
    
    def _get_element_path(self, elem: ET.Element) -> str:
        """Get a simple path representation for an element"""
        # In real implementation, would track actual path
        return elem.tag

class ChunkingOrchestrator:
    """Orchestrates the chunking process using appropriate strategies"""
    
    def __init__(self):
        self.strategies = {
            'hierarchical': HierarchicalChunking,
            'sliding_window': SlidingWindowChunking,
            'content_aware': ContentAwareChunking
        }
    
    def chunk_document(self, file_path: str, 
                      specialized_analysis: Dict[str, Any],
                      strategy: str = 'auto',
                      config: ChunkingConfig = None) -> List[XMLChunk]:
        """Chunk a document using the appropriate strategy"""
        
        if strategy == 'auto':
            strategy = self._select_strategy(specialized_analysis)
        
        if strategy not in self.strategies:
            raise ValueError(f"Unknown strategy: {strategy}")
        
        # Create strategy instance
        strategy_class = self.strategies[strategy]
        chunker = strategy_class(config)
        
        # Apply document-specific configuration
        if config is None:
            config = self._create_config_for_document(specialized_analysis)
            chunker.config = config
        
        # Perform chunking
        chunks = chunker.chunk_document(file_path, specialized_analysis)
        
        # Post-process chunks
        chunks = self._post_process_chunks(chunks, specialized_analysis)
        
        return chunks
    
    def _select_strategy(self, analysis: Dict[str, Any]) -> str:
        """Select the best chunking strategy based on document analysis"""
        doc_type = analysis.get('document_type', {}).get('type_name', '')
        
        # Strategy selection based on document type
        strategy_map = {
            'SCAP Security Report': 'hierarchical',
            'RSS/Atom Feed': 'hierarchical',
            'Maven POM': 'hierarchical',
            'Spring Configuration': 'hierarchical',
            'DocBook Documentation': 'content_aware',
            'Log4j Configuration': 'hierarchical',
            'SVG Graphics': 'hierarchical'
        }
        
        return strategy_map.get(doc_type, 'sliding_window')
    
    def _create_config_for_document(self, analysis: Dict[str, Any]) -> ChunkingConfig:
        """Create optimal chunking configuration for document"""
        doc_type = analysis.get('document_type', {}).get('type_name', '')
        
        # Base configuration
        config = ChunkingConfig()
        
        # Adjust based on document type
        if 'Documentation' in doc_type:
            config.max_chunk_size = 4000  # Larger chunks for documentation
            config.preserve_hierarchy = True
            config.include_parent_context = True
        elif 'Configuration' in doc_type:
            config.max_chunk_size = 2000  # Smaller chunks for configs
            config.preserve_hierarchy = True
        elif 'Feed' in doc_type:
            config.max_chunk_size = 1500  # Individual items
            config.overlap_size = 0  # No overlap needed
        
        return config
    
    def _post_process_chunks(self, chunks: List[XMLChunk], 
                           analysis: Dict[str, Any]) -> List[XMLChunk]:
        """Post-process chunks to add additional metadata"""
        doc_type = analysis.get('document_type', {}).get('type_name', '')
        
        for i, chunk in enumerate(chunks):
            # Add document context
            chunk.metadata['document_type'] = doc_type
            chunk.metadata['chunk_index'] = i
            chunk.metadata['total_chunks'] = len(chunks)
            
            # Add navigation info
            if i > 0:
                chunk.metadata['previous_chunk'] = chunks[i-1].chunk_id
            if i < len(chunks) - 1:
                chunk.metadata['next_chunk'] = chunks[i+1].chunk_id
        
        return chunks

# Example usage
if __name__ == "__main__":
    # Example: Using the chunking orchestrator
    from xml_specialized_handlers import XMLDocumentAnalyzer
    
    # Analyze document first
    analyzer = XMLDocumentAnalyzer()
    analysis = analyzer.analyze_document("sample_data/example.xml")
    
    # Chunk the document
    orchestrator = ChunkingOrchestrator()
    chunks = orchestrator.chunk_document(
        "sample_data/example.xml",
        analysis,
        strategy='auto'  # Let it choose the best strategy
    )
    
    # Display results
    print(f"Document chunked into {len(chunks)} chunks")
    for chunk in chunks[:3]:  # Show first 3 chunks
        print(f"\nChunk {chunk.chunk_id}:")
        print(f"  Path: {chunk.element_path}")
        print(f"  Tokens: ~{chunk.token_estimate}")
        print(f"  Elements: {', '.join(chunk.elements_included[:5])}")
        print(f"  Content preview: {chunk.content[:100]}...")
</file>

<file path="xml-demo-script.py">
#!/usr/bin/env python3
"""
XML Analysis Framework Demonstration

This script demonstrates the full capabilities of the XML analysis framework,
including specialized handlers, chunking strategies, and AI use case identification.
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any

# Add src directory to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from xml_specialized_handlers import XMLDocumentAnalyzer
from xml_chunking_strategy import ChunkingOrchestrator, ChunkingConfig
from xml_schema_analyzer_fixed import XMLSchemaAnalyzer

def print_separator(title: str = ""):
    """Print a nice separator line"""
    if title:
        print(f"\n{'='*20} {title} {'='*20}")
    else:
        print("="*60)

def demonstrate_specialized_handlers():
    """Demonstrate the specialized handler system"""
    print_separator("SPECIALIZED HANDLER DEMONSTRATION")
    
    # Example XML files to test
    test_files = [
        {
            "path": "sample_data/pom.xml",
            "content": """<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.example</groupId>
    <artifactId>demo-app</artifactId>
    <version>1.0.0</version>
    <packaging>jar</packaging>
    
    <properties>
        <java.version>11</java.version>
        <spring.version>5.3.10</spring.version>
    </properties>
    
    <dependencies>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-core</artifactId>
            <version>${spring.version}</version>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.2</version>
            <scope>test</scope>
        </dependency>
    </dependencies>
</project>"""
        },
        {
            "path": "sample_data/log4j2.xml",
            "content": """<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN" monitorInterval="30">
    <Appenders>
        <Console name="Console" target="SYSTEM_OUT">
            <PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"/>
        </Console>
        <RollingFile name="RollingFile" fileName="logs/app.log"
                     filePattern="logs/app-%d{MM-dd-yyyy}-%i.log.gz">
            <PatternLayout pattern="%d{ISO8601} [%t] %-5level %logger{36} - %msg%n"/>
            <Policies>
                <TimeBasedTriggeringPolicy/>
                <SizeBasedTriggeringPolicy size="10MB"/>
            </Policies>
        </RollingFile>
    </Appenders>
    <Loggers>
        <Logger name="com.example" level="DEBUG" additivity="false">
            <AppenderRef ref="Console"/>
            <AppenderRef ref="RollingFile"/>
        </Logger>
        <Root level="INFO">
            <AppenderRef ref="Console"/>
        </Root>
    </Loggers>
</Configuration>"""
        },
        {
            "path": "sample_data/rss_feed.xml",
            "content": """<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
    <channel>
        <title>Tech News Daily</title>
        <link>https://example.com/news</link>
        <description>Latest technology news and updates</description>
        <item>
            <title>AI Breakthrough in Natural Language Processing</title>
            <description>Researchers announce significant improvements in LLM capabilities</description>
            <link>https://example.com/news/ai-breakthrough</link>
            <pubDate>Mon, 23 Jul 2025 10:00:00 GMT</pubDate>
            <category>AI</category>
        </item>
        <item>
            <title>New Security Vulnerability Discovered</title>
            <description>Critical vulnerability affects millions of devices worldwide</description>
            <link>https://example.com/news/security-alert</link>
            <pubDate>Sun, 22 Jul 2025 15:30:00 GMT</pubDate>
            <category>Security</category>
        </item>
    </channel>
</rss>"""
        }
    ]
    
    # Create sample files
    sample_dir = Path("sample_data")
    sample_dir.mkdir(exist_ok=True)
    
    for test_file in test_files:
        file_path = Path(test_file["path"])
        file_path.parent.mkdir(exist_ok=True)
        file_path.write_text(test_file["content"])
    
    # Analyze each file
    analyzer = XMLDocumentAnalyzer()
    
    for test_file in test_files:
        print(f"\n Analyzing: {test_file['path']}")
        print("-" * 40)
        
        result = analyzer.analyze_document(test_file["path"])
        
        if "error" in result:
            print(f" Error: {result['error']}")
            continue
        
        doc_type = result["document_type"]
        analysis = result["analysis"]
        
        print(f" Document Type: {doc_type.type_name}")
        print(f"   Confidence: {doc_type.confidence:.1%}")
        print(f"   Handler: {result['handler_used']}")
        
        if analysis:
            print(f"\n Key Findings:")
            for key, value in list(analysis.key_findings.items())[:3]:
                print(f"   - {key}: {value}")
            
            print(f"\n AI Use Cases:")
            for use_case in analysis.ai_use_cases[:3]:
                print(f"   - {use_case}")
            
            print(f"\n Data Quality:")
            for metric, score in analysis.quality_metrics.items():
                print(f"   - {metric}: {score:.1%}")

def demonstrate_chunking_strategies():
    """Demonstrate different chunking strategies"""
    print_separator("CHUNKING STRATEGY DEMONSTRATION")
    
    # Create a larger sample XML for chunking
    large_xml_path = Path("sample_data/large_document.xml")
    large_xml_content = """<?xml version="1.0" encoding="UTF-8"?>
<documentation>
    <metadata>
        <title>System Administration Guide</title>
        <version>2.0</version>
        <date>2025-07-23</date>
    </metadata>
    <chapters>
        <chapter id="intro">
            <title>Introduction</title>
            <section>
                <title>Overview</title>
                <para>This guide provides comprehensive information about system administration tasks.</para>
                <para>It covers installation, configuration, maintenance, and troubleshooting.</para>
            </section>
            <section>
                <title>Prerequisites</title>
                <para>Before beginning, ensure you have the following:</para>
                <list>
                    <item>Administrative access to the system</item>
                    <item>Basic understanding of command line interfaces</item>
                    <item>Network connectivity for updates</item>
                </list>
            </section>
        </chapter>
        <chapter id="installation">
            <title>Installation</title>
            <section>
                <title>System Requirements</title>
                <para>The following are minimum system requirements:</para>
                <table>
                    <row><cell>CPU</cell><cell>2 cores @ 2.0 GHz</cell></row>
                    <row><cell>RAM</cell><cell>4 GB minimum, 8 GB recommended</cell></row>
                    <row><cell>Storage</cell><cell>20 GB available space</cell></row>
                </table>
            </section>
            <section>
                <title>Installation Steps</title>
                <para>Follow these steps to install the system:</para>
                <code language="bash">
# Download the installer
wget https://example.com/installer.sh

# Make it executable
chmod +x installer.sh

# Run the installer
sudo ./installer.sh
                </code>
            </section>
        </chapter>
        <chapter id="configuration">
            <title>Configuration</title>
            <section>
                <title>Basic Configuration</title>
                <para>After installation, configure the basic settings.</para>
                <para>Edit the main configuration file located at /etc/system/config.xml</para>
            </section>
            <section>
                <title>Advanced Options</title>
                <para>For advanced users, additional options are available.</para>
                <para>These include performance tuning, security hardening, and custom modules.</para>
            </section>
        </chapter>
    </chapters>
</documentation>"""
    
    large_xml_path.write_text(large_xml_content)
    
    # Analyze the document first
    analyzer = XMLDocumentAnalyzer()
    analysis_result = analyzer.analyze_document(str(large_xml_path))
    
    # Test different chunking strategies
    orchestrator = ChunkingOrchestrator()
    
    strategies = ['hierarchical', 'sliding_window', 'content_aware']
    
    for strategy in strategies:
        print(f"\n Testing {strategy.upper()} chunking strategy")
        print("-" * 40)
        
        # Custom config for demonstration
        config = ChunkingConfig(
            max_chunk_size=500,  # Smaller chunks for demo
            min_chunk_size=100,
            overlap_size=50,
            preserve_hierarchy=True
        )
        
        chunks = orchestrator.chunk_document(
            str(large_xml_path),
            analysis_result,
            strategy=strategy,
            config=config
        )
        
        print(f"  Created {len(chunks)} chunks")
        
        # Show first 2 chunks
        for i, chunk in enumerate(chunks[:2]):
            print(f"\n   Chunk {i+1}:")
            print(f"   - ID: {chunk.chunk_id}")
            print(f"   - Path: {chunk.element_path}")
            print(f"   - Tokens: ~{chunk.token_estimate}")
            print(f"   - Elements: {', '.join(chunk.elements_included[:3])}")
            print(f"   - Preview: {chunk.content[:80]}...")

def demonstrate_ai_use_cases():
    """Show how the analysis can be used for AI projects"""
    print_separator("AI USE CASE DEMONSTRATION")
    
    # Create a sample SCAP document
    scap_path = Path("sample_data/security_scan.xml")
    scap_content = """<?xml version="1.0" encoding="UTF-8"?>
<arf:asset-report-collection xmlns:arf="http://scap.nist.gov/schema/asset-reporting-format/1.1">
    <core:relationships xmlns:core="http://scap.nist.gov/schema/reporting-core/1.1">
        <core:relationship type="isAbout" subject="scan1">
            <core:ref>server-01</core:ref>
        </core:relationship>
    </core:relationships>
    <arf:reports>
        <arf:report id="scan1">
            <content>
                <rule-result idref="xccdf_rule_1" severity="high">
                    <result>fail</result>
                    <message>SSH root login is enabled</message>
                </rule-result>
                <rule-result idref="xccdf_rule_2" severity="medium">
                    <result>pass</result>
                    <message>Firewall is properly configured</message>
                </rule-result>
                <rule-result idref="xccdf_rule_3" severity="high">
                    <result>fail</result>
                    <message>System updates are not configured</message>
                </rule-result>
            </content>
        </arf:report>
    </arf:reports>
</arf:asset-report-collection>"""
    
    scap_path.write_text(scap_content)
    
    # Analyze the document
    analyzer = XMLDocumentAnalyzer()
    result = analyzer.analyze_document(str(scap_path))
    
    print("\n AI Project Planning Assistant")
    print("-" * 40)
    
    if result.get("analysis"):
        analysis = result["analysis"]
        doc_type = result["document_type"].type_name
        
        print(f"\n Document Type: {doc_type}")
        print(f"\n Recommended AI Applications:")
        
        for i, use_case in enumerate(analysis.ai_use_cases, 1):
            print(f"\n{i}. {use_case}")
            
            # Provide specific guidance for each use case
            if "compliance" in use_case.lower():
                print("    Implementation approach:")
                print("   - Extract rule violations and patterns")
                print("   - Train classifier on historical compliance data")
                print("   - Build automated remediation suggestions")
                
            elif "risk" in use_case.lower():
                print("    Implementation approach:")
                print("   - Aggregate severity scores and failure patterns")
                print("   - Develop risk scoring model")
                print("   - Create predictive analytics dashboard")
                
            elif "recommendation" in use_case.lower():
                print("    Implementation approach:")
                print("   - Map failures to remediation steps")
                print("   - Use NLP to generate human-readable fixes")
                print("   - Prioritize based on risk and effort")
        
        print(f"\n Data Availability:")
        for data_type, count in analysis.data_inventory.items():
            print(f"   - {data_type}: {count}")
        
        print(f"\n Quick Start Code:")
        print("""
# Load and process the analyzed data
from pathlib import Path
import json

# Load the analysis results
with open('security_scan_enhanced_analysis.json') as f:
    analysis = json.load(f)

# Extract structured data for ML
structured_data = analysis['specialized_analysis']['analysis']['structured_data']

# Example: Build a compliance classifier
failed_rules = [rule for rule in structured_data.get('scan_results', []) 
                if rule.get('result') == 'fail']

# Train your model on the extracted data
# model.train(failed_rules, remediation_labels)
""")

def demonstrate_integration_workflow():
    """Show a complete workflow from analysis to LLM preparation"""
    print_separator("COMPLETE INTEGRATION WORKFLOW")
    
    print("\n Workflow: XML  Analysis  Chunks  LLM-Ready")
    print("-" * 40)
    
    # Use the existing STIG file
    stig_path = Path("sample_data/node2.example.com-STIG-20250710162433.xml")
    
    if not stig_path.exists():
        print(" STIG sample file not found. Using a smaller example.")
        # Create a simple example
        stig_path = Path("sample_data/mini_stig.xml")
        stig_path.write_text("""<?xml version="1.0"?>
<Benchmark id="xccdf_benchmark">
    <Group id="V-1234">
        <title>Security Configuration</title>
        <Rule id="rule_1234" severity="high">
            <title>Ensure secure settings</title>
            <description>This rule checks for secure configurations</description>
            <check system="http://oval.mitre.org/XMLSchema/oval-definitions-5">
                <check-content-ref href="#oval:check:1234"/>
            </check>
        </Rule>
    </Group>
</Benchmark>""")
    
    print(f"\n1 Step 1: Analyze Document")
    analyzer = XMLDocumentAnalyzer()
    analysis = analyzer.analyze_document(str(stig_path))
    
    print(f"    Document type: {analysis['document_type'].type_name}")
    print(f"    Handler confidence: {analysis['confidence']:.1%}")
    
    print(f"\n2 Step 2: Apply Intelligent Chunking")
    orchestrator = ChunkingOrchestrator()
    chunks = orchestrator.chunk_document(
        str(stig_path),
        analysis,
        strategy='auto'
    )
    
    print(f"    Created {len(chunks)} chunks")
    print(f"    Strategy selected: {orchestrator._select_strategy(analysis)}")
    
    print(f"\n3 Step 3: Prepare for LLM Processing")
    
    # Simulate LLM prompts for first chunk
    if chunks:
        chunk = chunks[0]
        prompt = f"""You are analyzing a {analysis['document_type'].type_name} document.

Document Context:
- Type: {analysis['document_type'].type_name}
- Total chunks: {len(chunks)}
- Current chunk: 1 of {len(chunks)}

Chunk Content:
{chunk.content[:500]}...

Based on this security compliance data, please:
1. Identify any high-severity findings
2. Suggest remediation steps
3. Assess overall security posture
"""
        
        print("    Generated LLM Prompt Preview:")
        print("   " + "-" * 35)
        for line in prompt.split('\n')[:10]:
            print(f"   {line}")
        print("   ...")
    
    print(f"\n4 Step 4: Process Results")
    print("    Ready for LLM processing")
    print("    Chunks maintain context")
    print("    Specialized extraction completed")

def main():
    """Run all demonstrations"""
    print("\n XML ANALYSIS FRAMEWORK DEMONSTRATION")
    print("=====================================")
    
    # Create sample directory
    Path("sample_data").mkdir(exist_ok=True)
    
    try:
        # Run demonstrations
        demonstrate_specialized_handlers()
        demonstrate_chunking_strategies()
        demonstrate_ai_use_cases()
        demonstrate_integration_workflow()
        
        print_separator("DEMONSTRATION COMPLETE")
        print("\n All demonstrations completed successfully!")
        print("\n Next Steps:")
        print("1. Try with your own XML files: python analyze_enhanced.py your_file.xml")
        print("2. Create custom handlers for your specific XML formats")
        print("3. Integrate with your LLM pipeline for automated processing")
        print("4. Build AI applications using the extracted structured data")
        
    except Exception as e:
        print(f"\n Error during demonstration: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
</file>

<file path="xml-framework-roadmap.md">
# XML Analysis Framework - Development Roadmap

##  Vision
Create a comprehensive, modular system for analyzing any XML document and preparing it for AI/ML processing, with seamless integration into larger data analysis pipelines.

##  Additional XML Handlers to Implement

### 1. Web Service Definitions

#### WSDL Handler (Web Services Description Language)
- **Purpose**: Analyze SOAP web service definitions
- **Key Extractions**:
  - Service operations and endpoints
  - Input/output message schemas
  - Complex data type definitions
  - Binding protocols and ports
  - WS-Security policies
- **AI Use Cases**:
  - API dependency mapping
  - Service complexity analysis
  - SOAP-to-REST migration planning
  - Integration test generation
  - Service documentation automation

#### WADL Handler (Web Application Description Language)
- **Purpose**: Analyze REST service definitions
- **Key Extractions**:
  - Resource paths and methods
  - Request/response formats
  - Parameter definitions
  - Authentication schemes
- **AI Use Cases**:
  - REST API cataloging
  - API compatibility checking
  - Client SDK generation

### 2. Schema and Validation

#### XSD Handler (XML Schema Definition)
- **Purpose**: Analyze XML schema files
- **Key Extractions**:
  - Element and attribute definitions
  - Data type constraints
  - Namespace declarations
  - Inheritance hierarchies
  - Validation rules
- **AI Use Cases**:
  - Schema evolution tracking
  - Test data generation
  - Schema compatibility analysis
  - Documentation generation

#### RelaxNG Handler
- **Purpose**: Alternative XML schema language
- **Key Extractions**:
  - Pattern definitions
  - Grammar rules
  - Datatype libraries
- **AI Use Cases**:
  - Schema conversion assistance
  - Validation rule extraction

### 3. Geographic and Mapping

#### KML/KMZ Handler (Keyhole Markup Language)
- **Purpose**: Analyze geographic data files
- **Key Extractions**:
  - Placemarks and coordinates
  - Polygons and paths
  - Styles and icons
  - Time-based data
  - Network links
- **AI Use Cases**:
  - Geospatial analysis
  - Route optimization
  - Location clustering
  - Map visualization generation

#### GPX Handler (GPS Exchange Format)
- **Purpose**: GPS track and waypoint data
- **Key Extractions**:
  - Track points with timestamps
  - Waypoints and routes
  - Elevation data
  - Speed and distance calculations
- **AI Use Cases**:
  - Activity pattern analysis
  - Route recommendation
  - Performance analytics

### 4. Build and Configuration

#### Ant/NAnt Build Handler
- **Purpose**: Analyze build scripts
- **Key Extractions**:
  - Build targets and dependencies
  - Property definitions
  - Task sequences
  - File operations
  - External tool calls
- **AI Use Cases**:
  - Build optimization
  - Dependency analysis
  - Migration to modern build tools
  - Build failure prediction

#### NuGet Package Handler (.nuspec)
- **Purpose**: .NET package definitions
- **Key Extractions**:
  - Package metadata
  - Dependencies and versions
  - Target frameworks
  - Package contents
- **AI Use Cases**:
  - Dependency vulnerability scanning
  - Version compatibility checking
  - License compliance

### 5. Document and Content

#### DITA Handler (Darwin Information Typing Architecture)
- **Purpose**: Technical documentation standard
- **Key Extractions**:
  - Topic types and structures
  - Content references (conref)
  - Conditional processing
  - Relationship tables
- **AI Use Cases**:
  - Documentation quality analysis
  - Content reuse optimization
  - Translation preparation

#### TEI Handler (Text Encoding Initiative)
- **Purpose**: Academic and literary texts
- **Key Extractions**:
  - Text structure and divisions
  - Scholarly annotations
  - Manuscript descriptions
  - Critical apparatus
- **AI Use Cases**:
  - Digital humanities research
  - Text analysis and mining
  - Citation extraction

### 6. Industry-Specific

#### HL7 CDA Handler (Clinical Document Architecture)
- **Purpose**: Healthcare clinical documents
- **Key Extractions**:
  - Patient demographics
  - Clinical observations
  - Medications and allergies
  - Procedure codes
- **AI Use Cases**:
  - Clinical decision support
  - Patient outcome prediction
  - Medical coding assistance

#### XBRL Handler (eXtensible Business Reporting Language)
- **Purpose**: Financial reporting
- **Key Extractions**:
  - Financial facts and contexts
  - Taxonomies and calculations
  - Dimensional data
  - Footnotes and labels
- **AI Use Cases**:
  - Financial analysis automation
  - Regulatory compliance checking
  - Fraud detection

#### PMML Handler (Predictive Model Markup Language)
- **Purpose**: Statistical and data mining models
- **Key Extractions**:
  - Model types and parameters
  - Data dictionaries
  - Transformations
  - Mining schemas
- **AI Use Cases**:
  - Model comparison and validation
  - Model documentation
  - Model migration

### 7. Transformation and Styling

#### XSLT Handler (XSL Transformations)
- **Purpose**: XML transformation stylesheets
- **Key Extractions**:
  - Template patterns
  - Transformation rules
  - Variables and parameters
  - Output methods
- **AI Use Cases**:
  - Transformation optimization
  - Code generation
  - Migration assistance

#### XSL-FO Handler (Formatting Objects)
- **Purpose**: Page layout and formatting
- **Key Extractions**:
  - Page layouts and regions
  - Formatting properties
  - Font and style definitions
- **AI Use Cases**:
  - Layout analysis
  - Print optimization

##  Web API Interface Design

### Architecture
```

   REST API      
  (FastAPI/Flask)

  Job Queue      
   (Celery)      

  Cache Layer    
   (Redis)       

 Storage Backend 
    (S3/GCS)     

```

### API Endpoints

#### Analysis Endpoints
```
POST   /api/v1/analyze
       Body: { "file": <upload>, "options": {...} }
       Returns: { "job_id": "...", "status": "queued" }

GET    /api/v1/analyze/{job_id}
       Returns: { "status": "complete", "result": {...} }

POST   /api/v1/analyze/batch
       Body: { "files": [...], "options": {...} }
       Returns: { "batch_id": "...", "jobs": [...] }
```

#### Document Type Detection
```
POST   /api/v1/detect
       Body: { "file": <upload> }
       Returns: { "type": "Maven POM", "confidence": 0.95 }

GET    /api/v1/handlers
       Returns: { "handlers": [...available handlers...] }
```

#### Chunking Operations
```
POST   /api/v1/chunk
       Body: { "file": <upload>, "strategy": "auto", "config": {...} }
       Returns: { "chunks": [...], "metadata": {...} }
```

#### Schema Operations
```
POST   /api/v1/schema/extract
       Body: { "file": <upload> }
       Returns: { "schema": {...}, "statistics": {...} }

POST   /api/v1/schema/validate
       Body: { "file": <upload>, "schema": <xsd_file> }
       Returns: { "valid": true/false, "errors": [...] }
```

### API Features

#### Authentication & Authorization
- API key authentication
- JWT tokens for session management
- Rate limiting per client
- Usage tracking and quotas

#### Async Processing
- Long-running analysis jobs
- WebSocket support for real-time updates
- Batch processing capabilities
- Progress tracking

#### Output Formats
- JSON (default)
- XML (preserved structure)
- CSV (flattened data)
- Parquet (for big data pipelines)

#### Caching Strategy
- Cache analysis results by file hash
- Configurable TTL
- Cache warming for common file types
- Distributed cache for scaling

##  LLM Integration Architecture

### Integration Patterns

#### 1. Direct API Integration
```python
class LLMProvider:
    def __init__(self, provider: str, api_key: str):
        self.provider = provider
        self.client = self._init_client()
    
    def analyze_chunk(self, chunk: XMLChunk, prompt_template: str):
        # Format chunk with context
        # Send to LLM
        # Parse and validate response
        
    def batch_process(self, chunks: List[XMLChunk], concurrency: int = 3):
        # Parallel processing with rate limiting
        # Result aggregation
        # Error handling and retries
```

#### 2. Prompt Engineering System
```python
class PromptLibrary:
    templates = {
        "security_analysis": """
        Analyze this {doc_type} security configuration:
        
        Context: {metadata}
        Chunk {current}/{total}: {content}
        
        Identify:
        1. Security vulnerabilities
        2. Compliance issues
        3. Best practice violations
        """,
        
        "data_extraction": """
        Extract structured data from this {doc_type}:
        
        Previous context: {previous_summary}
        Current content: {content}
        
        Return as JSON with schema: {expected_schema}
        """
    }
```

#### 3. Multi-Provider Support
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Google (Gemini)
- Local models (Ollama, vLLM)
- Custom endpoints

#### 4. Result Processing Pipeline
```python
class ResultProcessor:
    def merge_chunk_results(self, results: List[LLMResponse]):
        # Deduplication
        # Conflict resolution
        # Confidence scoring
        # Final aggregation
    
    def validate_extraction(self, extracted_data: Dict, schema: Dict):
        # Schema validation
        # Business rule checking
        # Anomaly detection
```

### LLM Use Case Implementations

#### 1. Compliance Analysis
```python
class ComplianceAnalyzer:
    def analyze_security_config(self, xml_analysis: Dict):
        # Extract rules and settings
        # Check against compliance frameworks
        # Generate remediation suggestions
        # Create audit report
```

#### 2. Dependency Intelligence
```python
class DependencyAnalyzer:
    def analyze_dependencies(self, pom_analysis: Dict):
        # Vulnerability scanning
        # License compatibility
        # Update recommendations
        # Risk scoring
```

#### 3. Documentation Enhancement
```python
class DocumentationEnhancer:
    def enhance_docs(self, docbook_analysis: Dict):
        # Generate summaries
        # Create examples
        # Improve clarity
        # Add cross-references
```

##  Hybrid Database Architecture (Vector + Graph)

### Storage Strategy
We will implement a hybrid approach using:
- **PostgreSQL + pgvector**: For semantic search and content retrieval
- **Neo4j Community Edition**: For structural analysis and relationship queries

### Architecture Overview
```

         XML Document                

             

      XML Analysis Framework        

             
      
                   
 
  pgvector         Neo4j         
                                 
  Content     Structure        
  Semantic    Relationships    
  Chunks      Dependencies     
  Metadata    Hierarchies      
 
```

### PostgreSQL + pgvector Schema

```sql
-- Main chunks table
CREATE TABLE xml_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL,
    chunk_id VARCHAR(255) UNIQUE NOT NULL,
    content TEXT NOT NULL,
    embedding vector(1536),  -- OpenAI ada-002 dimensions
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Indexes
    INDEX idx_document_id (document_id),
    INDEX idx_metadata_gin (metadata),
    INDEX idx_embedding_ivfflat (embedding vector_l2_ops)
);

-- Documents table
CREATE TABLE xml_documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename VARCHAR(255) NOT NULL,
    file_hash VARCHAR(64) UNIQUE NOT NULL,
    document_type VARCHAR(100),
    size_bytes BIGINT,
    analysis_result JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_document_type (document_type),
    INDEX idx_created_at (created_at)
);

-- Vector similarity search function
CREATE FUNCTION search_similar_chunks(
    query_embedding vector(1536),
    match_count INT = 10,
    filter_metadata JSONB = NULL
) RETURNS TABLE (
    chunk_id VARCHAR,
    content TEXT,
    similarity FLOAT,
    metadata JSONB
) AS $
BEGIN
    RETURN QUERY
    SELECT 
        c.chunk_id,
        c.content,
        1 - (c.embedding <=> query_embedding) AS similarity,
        c.metadata
    FROM xml_chunks c
    WHERE 
        (filter_metadata IS NULL OR c.metadata @> filter_metadata)
    ORDER BY c.embedding <=> query_embedding
    LIMIT match_count;
END;
$ LANGUAGE plpgsql;
```

### Neo4j Graph Schema

```cypher
// Node types
(:Document {
    id: String,
    filename: String,
    type: String,
    hash: String,
    created: DateTime
})

(:Element {
    id: String,
    tag: String,
    path: String,
    document_id: String,
    attributes: Map
})

(:Namespace {
    uri: String,
    prefix: String
})

// Relationship types
(:Document)-[:HAS_ROOT]->(:Element)
(:Element)-[:HAS_CHILD]->(:Element)
(:Element)-[:HAS_NAMESPACE]->(:Namespace)
(:Element)-[:REFERENCES]->(:Element)
(:Element)-[:DEPENDS_ON]->(:Element)

// Indexes for performance
CREATE INDEX document_id_index FOR (d:Document) ON (d.id);
CREATE INDEX element_path_index FOR (e:Element) ON (e.path);
CREATE INDEX element_tag_index FOR (e:Element) ON (e.tag);
```

### Hybrid Storage Implementation

```python
from typing import List, Dict, Any, Optional
import asyncpg
import asyncio
from neo4j import AsyncGraphDatabase
import numpy as np
from pgvector.asyncpg import register_vector

class HybridXMLStorage:
    def __init__(self, pg_config: Dict, neo4j_config: Dict):
        self.pg_config = pg_config
        self.neo4j_config = neo4j_config
        self.pg_pool: Optional[asyncpg.Pool] = None
        self.neo4j_driver = None
        
    async def initialize(self):
        # Initialize PostgreSQL + pgvector
        self.pg_pool = await asyncpg.create_pool(**self.pg_config)
        async with self.pg_pool.acquire() as conn:
            await register_vector(conn)
        
        # Initialize Neo4j
        self.neo4j_driver = AsyncGraphDatabase.driver(
            self.neo4j_config['uri'],
            auth=(self.neo4j_config['user'], self.neo4j_config['password'])
        )
    
    async def store_document(self, analysis: Dict, chunks: List[XMLChunk]):
        """Store document in both databases"""
        
        # 1. Store in PostgreSQL
        doc_id = await self._store_in_postgres(analysis, chunks)
        
        # 2. Store structure in Neo4j
        await self._store_in_neo4j(doc_id, analysis)
        
        return doc_id
    
    async def _store_in_postgres(self, analysis: Dict, chunks: List[XMLChunk]):
        """Store chunks and embeddings in PostgreSQL"""
        async with self.pg_pool.acquire() as conn:
            # Insert document
            doc_id = await conn.fetchval("""
                INSERT INTO xml_documents 
                (filename, file_hash, document_type, size_bytes, analysis_result)
                VALUES ($1, $2, $3, $4, $5)
                RETURNING id
            """, 
                analysis['file_path'],
                analysis['file_hash'],
                analysis['document_type']['type_name'],
                analysis['file_size'],
                analysis
            )
            
            # Bulk insert chunks
            chunk_data = []
            for chunk in chunks:
                # Generate embedding (placeholder - use actual embedding service)
                embedding = await self._generate_embedding(chunk.content)
                
                chunk_data.append((
                    doc_id,
                    chunk.chunk_id,
                    chunk.content,
                    embedding,
                    {
                        'path': chunk.element_path,
                        'tokens': chunk.token_estimate,
                        'elements': chunk.elements_included,
                        **chunk.metadata
                    }
                ))
            
            await conn.executemany("""
                INSERT INTO xml_chunks 
                (document_id, chunk_id, content, embedding, metadata)
                VALUES ($1, $2, $3, $4, $5)
            """, chunk_data)
            
            return doc_id
    
    async def _store_in_neo4j(self, doc_id: str, analysis: Dict):
        """Store document structure in Neo4j"""
        async with self.neo4j_driver.session() as session:
            # Create document node
            await session.run("""
                CREATE (d:Document {
                    id: $id,
                    filename: $filename,
                    type: $type,
                    hash: $hash,
                    created: datetime()
                })
            """, 
                id=str(doc_id),
                filename=analysis['file_path'],
                type=analysis['document_type']['type_name'],
                hash=analysis['file_hash']
            )
            
            # Store element structure
            await self._store_element_tree(session, doc_id, analysis['element_tree'])
            
            # Store relationships based on document type
            await self._store_specialized_relationships(session, doc_id, analysis)
    
    async def hybrid_search(self, 
                          query: str,
                          structural_filter: Optional[str] = None,
                          metadata_filter: Optional[Dict] = None,
                          top_k: int = 10) -> List[Dict]:
        """
        Perform hybrid search combining vector similarity and graph structure
        """
        # 1. Get structural constraints from Neo4j
        valid_paths = []
        if structural_filter:
            async with self.neo4j_driver.session() as session:
                result = await session.run(structural_filter)
                valid_paths = [record['path'] async for record in result]
        
        # 2. Perform vector search with structural constraints
        query_embedding = await self._generate_embedding(query)
        
        async with self.pg_pool.acquire() as conn:
            # Build metadata filter
            combined_filter = metadata_filter or {}
            if valid_paths:
                combined_filter['path'] = {'$in': valid_paths}
            
            results = await conn.fetch("""
                SELECT * FROM search_similar_chunks($1, $2, $3)
            """, query_embedding, top_k, combined_filter)
        
        # 3. Enrich results with graph context
        enriched_results = []
        for row in results:
            # Get parent/child context from Neo4j
            context = await self._get_graph_context(row['chunk_id'])
            enriched_results.append({
                **dict(row),
                'graph_context': context
            })
        
        return enriched_results
    
    async def _get_graph_context(self, chunk_id: str):
        """Get structural context from Neo4j"""
        async with self.neo4j_driver.session() as session:
            result = await session.run("""
                MATCH (e:Element {chunk_id: $chunk_id})
                OPTIONAL MATCH (e)-[:HAS_CHILD]->(child)
                OPTIONAL MATCH (parent)-[:HAS_CHILD]->(e)
                OPTIONAL MATCH (e)-[:REFERENCES]->(ref)
                RETURN 
                    e,
                    collect(DISTINCT child) as children,
                    collect(DISTINCT parent) as parents,
                    collect(DISTINCT ref) as references
            """, chunk_id=chunk_id)
            
            return await result.single()
```

### Query Examples

#### 1. Semantic Search with Structural Constraints
```python
# Find security configurations in Log4j files
results = await storage.hybrid_search(
    query="authentication security settings",
    structural_filter="""
        MATCH (d:Document {type: 'Log4j Configuration'})
        -[:HAS_ROOT]->(:Element)
        -[:HAS_CHILD*]->
        (e:Element {tag: 'appender'})
        RETURN e.path as path
    """,
    top_k=5
)
```

#### 2. Dependency Analysis
```cypher
// Find all Maven dependencies transitively
MATCH (d:Document {type: 'Maven POM'})
-[:HAS_ROOT]->(:Element)
-[:HAS_CHILD*]->(dep:Element {tag: 'dependency'})
-[:DEPENDS_ON*1..3]->(transitive:Element)
RETURN dep, transitive
```

#### 3. Configuration Impact Analysis
```python
# Find all configurations affected by a change
async def analyze_impact(element_id: str):
    # Get references from Neo4j
    graph_query = """
        MATCH (changed:Element {id: $id})
        <-[:REFERENCES|DEPENDS_ON*1..3]-(affected)
        RETURN affected
    """
    
    # Get similar configurations from pgvector
    vector_results = await storage.hybrid_search(
        query=changed_element_content,
        metadata_filter={'document_type': 'Spring Configuration'},
        top_k=20
    )
    
    return combine_results(graph_results, vector_results)
```

### Chunking Strategy Updates

```python
class HybridChunkingStrategy(XMLChunkingStrategy):
    """Chunking optimized for hybrid storage"""
    
    def __init__(self):
        super().__init__()
        # Balanced chunk size for both vector and graph use
        self.config.max_chunk_size = 500  # Smaller for better embeddings
        self.config.preserve_hierarchy = True  # Critical for graph storage
        
    def create_chunk(self, element: ET.Element, path: str) -> XMLChunk:
        chunk = super().create_chunk(element, path)
        
        # Add graph-specific metadata
        chunk.metadata.update({
            'neo4j_labels': self._determine_labels(element),
            'relationship_hints': self._extract_relationships(element),
            'structural_signature': self._create_signature(path)
        })
        
        # Add vector-specific optimizations
        chunk.embedding_text = self._optimize_for_embedding(chunk.content)
        chunk.search_keywords = self._extract_keywords(element)
        
        return chunk
```

### Migration Path

1. **Phase 1**: Implement pgvector storage (Month 1)
   - Set up PostgreSQL with pgvector extension
   - Implement embedding generation
   - Create vector search APIs

2. **Phase 2**: Add Neo4j graph storage (Month 2)
   - Set up Neo4j Community Edition
   - Implement graph schema
   - Create structure import pipeline

3. **Phase 3**: Hybrid query system (Month 3)
   - Implement hybrid search
   - Create query optimizer
   - Build caching layer

4. **Phase 4**: Advanced features (Month 4)
   - Graph algorithms (PageRank, community detection)
   - Vector index optimization
   - Real-time synchronization

##  Integration with Other File Types

### Unified Analysis Framework
```

        Orchestration Layer          

 XML AnalyzerCSV AnalyzerPDF Parser

      Common Output Format           

    Hybrid Storage (PG + Neo4j)     

         LLM Processing              

```

### Planned Analyzers
1. **CSV/Excel Analyzer**
   - Schema inference
   - Data quality metrics
   - Statistical analysis
   - Anomaly detection

2. **PDF Document Analyzer**
   - Text extraction
   - Table recognition
   - Form field detection
   - OCR integration

3. **JSON/YAML Analyzer**
   - Schema validation
   - Configuration analysis
   - API response parsing

4. **Log File Analyzer**
   - Pattern extraction
   - Error categorization
   - Time series analysis

##  Performance Optimizations

### Parallel Processing
- Multi-threaded XML parsing
- Distributed analysis with Ray/Dask
- GPU acceleration for pattern matching
- Streaming for large files
- Parallel vector embedding generation
- Batch Neo4j transactions

### Caching Strategies
- Analysis result caching in Redis
- PostgreSQL query result caching
- Neo4j query caching with TTL
- Embedding cache for repeated content
- LLM response caching
- Chunk reuse across documents

### Memory Management
- Lazy loading for large documents
- Incremental parsing
- Memory-mapped files
- Garbage collection optimization
- Connection pooling for both databases
- Batch processing for bulk operations

### Database-Specific Optimizations

#### PostgreSQL + pgvector
- IVFFlat indexes for fast similarity search
- Partitioning by document type
- JSONB indexes for metadata queries
- Query plan optimization
- Connection pooling with pgbouncer

#### Neo4j
- Composite indexes on frequently queried properties
- Query result caching
- Batch imports with PERIODIC COMMIT
- Memory configuration tuning
- Relationship indexing for fast traversal

##  Security Considerations

### Input Validation
- File size limits
- Malformed XML detection
- XXE attack prevention
- Zip bomb protection

### Data Privacy
- PII detection and masking
- Encryption at rest
- Secure transmission
- Audit logging

### Access Control
- Role-based permissions
- Document-level access
- API rate limiting
- IP whitelisting

##  Monitoring and Analytics

### Metrics to Track
- Analysis performance (time, memory)
- Document type distribution
- Error rates and types
- LLM token usage
- Cache hit rates

### Dashboards
- Real-time processing status
- Historical trends
- Cost analysis
- User activity

##  Deployment Architecture

### Containerization
```dockerfile
# Multi-stage build
FROM python:3.11-slim as builder
# Install dependencies
# Copy source code
# Run tests

FROM python:3.11-slim
# Copy from builder
# Configure runtime
# Health checks
```

### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xml-analyzer
spec:
  replicas: 3
  selector:
    matchLabels:
      app: xml-analyzer
  template:
    spec:
      containers:
      - name: analyzer
        image: xml-analyzer:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
```

### Auto-scaling
- Horizontal pod autoscaling
- Vertical pod autoscaling
- Cluster autoscaling
- Queue-based scaling

##  Implementation Timeline

### Phase 1: Core Enhancements (Month 1-2)
- Complete remaining handlers (WSDL, XSD, KML)
- Implement PostgreSQL + pgvector setup
- Create embedding generation pipeline
- Develop vector search APIs
- Create comprehensive test suite

### Phase 2: Graph Database Integration (Month 2-3)
- Set up Neo4j Community Edition
- Design and implement graph schema
- Build XML-to-graph transformation pipeline
- Create graph query APIs
- Implement relationship extraction

### Phase 3: Hybrid System Development (Month 3-4)
- Build hybrid query orchestrator
- Implement cross-database joins
- Create unified search interface
- Develop result ranking system
- Add caching layer for both databases

### Phase 4: API Development (Month 4-5)
- Build REST API with FastAPI
- Implement async job processing
- Add authentication/authorization
- Create API documentation
- Develop SDKs for common languages

### Phase 5: LLM Integration (Month 5-6)
- Multi-provider support
- Prompt template system
- Result aggregation pipeline
- Cost optimization
- RAG implementation with hybrid search

### Phase 6: Production Readiness (Month 6-7)
- Performance optimization
- Security hardening
- Monitoring setup
- Documentation completion
- Deployment automation

### Phase 7: Advanced Features (Month 7-8)
- Additional file type analyzers
- Graph algorithms implementation
- Advanced analytics
- Machine learning models
- Enterprise features

##  Cost Considerations for Hybrid Approach

### PostgreSQL + pgvector Costs
- **Infrastructure**: ~$100-500/month for managed PostgreSQL
- **Storage**: ~$0.10/GB/month
- **Compute**: Scales with query volume
- **Embedding Generation**: 
  - OpenAI: ~$0.0001/1K tokens
  - Self-hosted: GPU costs (~$0.50-2.00/hour)

### Neo4j Community Edition
- **License**: Free (with limitations)
- **Infrastructure**: ~$50-200/month for hosting
- **Limitations**:
  - Single database
  - No clustering
  - No advanced security features

### Optimization Strategies
1. **Selective Storage**: Only graph high-value relationships
2. **Embedding Cache**: Reuse embeddings for similar content
3. **Batch Processing**: Reduce API calls
4. **Data Lifecycle**: Archive old data to cold storage
5. **Query Optimization**: Cache frequent queries

### Estimated Monthly Costs by Scale
- **Small** (< 1M documents): ~$200-300/month
- **Medium** (1-10M documents): ~$500-1000/month
- **Large** (10M+ documents): ~$2000+/month

### ROI Considerations
- Faster development with pre-analyzed data
- Reduced LLM token usage through better retrieval
- Improved accuracy with structural understanding
- Time savings from automated analysis

##  Benefits of Hybrid Approach

### Use Case Synergies

#### 1. Compliance Analysis
- **Vector DB**: Find similar compliance rules across documents
- **Graph DB**: Track rule dependencies and inheritance
- **Combined**: "Find all rules similar to X and their dependent configurations"

#### 2. Configuration Management
- **Vector DB**: Search configurations by description
- **Graph DB**: Understand configuration relationships
- **Combined**: "Find all Spring beans similar to this one and what depends on them"

#### 3. Documentation Search
- **Vector DB**: Natural language search across docs
- **Graph DB**: Navigate document structure
- **Combined**: "Find sections about security and their parent chapters"

#### 4. Dependency Analysis
- **Vector DB**: Find similar dependencies by description
- **Graph DB**: Map full dependency trees
- **Combined**: "Find all dependencies similar to log4j and their usage patterns"

### Technical Advantages

1. **Query Flexibility**
   - Semantic search when you don't know exact terms
   - Structural queries when you know the pattern
   - Hybrid queries combining both

2. **Performance Optimization**
   - Use graph for known traversals (fast)
   - Use vector for exploratory search (flexible)
   - Cache results from both systems

3. **Data Completeness**
   - Vector DB captures content meaning
   - Graph DB captures relationships
   - Together provide full context

4. **Scalability Path**
   - Start with vector-only for MVP
   - Add graph for advanced features
   - Scale independently based on usage

##  Success Metrics

### Technical Metrics
- Analysis speed: <1 second for files under 10MB
- Accuracy: >95% document type detection
- Scalability: Handle 1000+ concurrent requests
- Availability: 99.9% uptime
- **Vector search latency**: <100ms for top-10 results
- **Graph traversal**: <50ms for 3-hop queries
- **Hybrid query performance**: <200ms combined
- **Embedding generation**: <500ms per chunk

### Storage Metrics
- **PostgreSQL capacity**: 10M+ chunks
- **Neo4j capacity**: 50M+ nodes, 200M+ relationships
- **Storage efficiency**: <2x original XML size
- **Cache hit rate**: >80% for common queries

### Business Metrics
- Developer adoption rate
- API usage growth
- Cost per analysis: <$0.01 per document
- Customer satisfaction scores
- **Time to insight**: 90% faster than manual analysis
- **LLM token reduction**: 60% through better retrieval

##  Integration Points

### CI/CD Integration
- GitHub Actions
- Jenkins plugins
- GitLab CI
- Azure DevOps

### Data Pipeline Integration
- Apache Airflow
- Apache NiFi
- Databricks
- AWS Glue

### Monitoring Integration
- Prometheus/Grafana
  - PostgreSQL metrics via postgres_exporter
  - Neo4j metrics via neo4j_exporter
  - Custom metrics for hybrid queries
- ELK Stack
  - Query logs from both databases
  - Performance analytics
  - Error tracking
- Datadog
  - Unified dashboards
  - Alert management
- pgAdmin for PostgreSQL monitoring
- Neo4j Browser for graph visualization

### Database-Specific Monitoring

#### PostgreSQL + pgvector
- Query performance (EXPLAIN ANALYZE)
- Index usage statistics
- Vector similarity distribution
- Connection pool metrics
- Disk usage and vacuum stats

#### Neo4j
- Query execution plans
- Cache hit ratios
- Transaction metrics
- Memory usage
- Relationship distribution

##  Documentation Plan

### Developer Documentation
- API reference
- SDK documentation
- Integration guides
- Example notebooks

### User Documentation
- Getting started guide
- Handler documentation
- Best practices
- Troubleshooting guide

### Architecture Documentation
- System design
- Data flow diagrams
- Security architecture
- Deployment guides

##  Documentation Plan

### Developer Documentation
- API reference
- SDK documentation
- Integration guides
- Example notebooks
- Database query cookbook

### User Documentation
- Getting started guide
- Handler documentation
- Best practices
- Troubleshooting guide
- Performance tuning guide

### Architecture Documentation
- System design
- Data flow diagrams
- Security architecture
- Deployment guides
- Database schemas and relationships

---

##  Conclusion

This hybrid approach using PostgreSQL + pgvector alongside Neo4j Community Edition provides the best of both worlds:

1. **Semantic Understanding**: pgvector enables natural language queries and content similarity search
2. **Structural Intelligence**: Neo4j captures the inherent hierarchical and relational nature of XML
3. **Flexibility**: Start simple with vector search, add graph capabilities as needed
4. **Cost-Effective**: Community editions and open-source tools keep costs manageable
5. **Future-Proof**: Architecture supports any XML format and scales with your needs

The modular design ensures each component can be developed, tested, and deployed independently, allowing for agile development and easy maintenance. This roadmap provides a clear path from MVP to enterprise-grade XML analysis system.
</file>

<file path="xml-specialized-handlers.py">
#!/usr/bin/env python3
"""
XML Specialized Handlers System

This module provides a flexible framework for detecting and handling different XML document types.
Each handler provides specialized analysis and extraction logic for its document type.

Key features:
- Automatic document type detection
- Pluggable handler architecture
- Type-specific analysis and insights
- Standardized output format
"""

import xml.etree.ElementTree as ET
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Type, Tuple
from dataclasses import dataclass, field
import re
from pathlib import Path
import json

@dataclass
class DocumentTypeInfo:
    """Information about a detected document type"""
    type_name: str
    confidence: float  # 0.0 to 1.0
    version: Optional[str] = None
    schema_uri: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class SpecializedAnalysis:
    """Results from specialized handler analysis"""
    document_type: str
    key_findings: Dict[str, Any]
    recommendations: List[str]
    data_inventory: Dict[str, int]  # What types of data found and counts
    ai_use_cases: List[str]  # Potential AI/ML applications
    structured_data: Dict[str, Any]  # Extracted structured data
    quality_metrics: Dict[str, float]  # Data quality indicators

class XMLHandler(ABC):
    """Abstract base class for XML document handlers"""
    
    @abstractmethod
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        """
        Check if this handler can process the document
        Returns: (can_handle: bool, confidence: float)
        """
        pass
    
    @abstractmethod
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        """Detect specific document type and version"""
        pass
    
    @abstractmethod
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        """Perform specialized analysis on the document"""
        pass
    
    @abstractmethod
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        """Extract the most important data from this document type"""
        pass

class SCAPHandler(XMLHandler):
    """Handler for SCAP (Security Content Automation Protocol) documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # Check for SCAP-specific namespaces and elements
        scap_indicators = [
            'http://scap.nist.gov/schema/',
            'asset-report-collection',
            'data-stream-collection',
            'xccdf',
            'oval'
        ]
        
        score = 0.0
        if any(uri in str(namespaces.values()) for uri in scap_indicators[:1]):
            score += 0.5
        if root.tag.endswith('asset-report-collection'):
            score += 0.3
        if 'xccdf' in str(namespaces.values()).lower():
            score += 0.2
            
        return score > 0.5, score
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        version = None
        schema_uri = None
        
        # Extract version from namespaces
        for prefix, uri in namespaces.items():
            if 'scap.nist.gov' in uri:
                schema_uri = uri
                # Extract version from URI if present
                version_match = re.search(r'/(\d+\.\d+)/?$', uri)
                if version_match:
                    version = version_match.group(1)
        
        return DocumentTypeInfo(
            type_name="SCAP Security Report",
            confidence=0.9,
            version=version,
            schema_uri=schema_uri,
            metadata={
                "standard": "NIST SCAP",
                "category": "security_compliance"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {}
        data_inventory = {}
        
        # Analyze SCAP-specific elements
        # Count security rules
        rules = root.findall('.//*[@id]')
        findings['total_rules'] = len(rules)
        
        # Count vulnerabilities/findings
        findings['vulnerabilities'] = self._count_vulnerabilities(root)
        
        # Extract compliance status
        findings['compliance_summary'] = self._extract_compliance_summary(root)
        
        recommendations = [
            "Use for automated compliance monitoring",
            "Extract failed rules for remediation workflows",
            "Trend analysis on compliance scores over time",
            "Risk scoring based on vulnerability severity"
        ]
        
        ai_use_cases = [
            "Automated compliance report generation",
            "Predictive risk analysis",
            "Remediation recommendation engine",
            "Compliance trend forecasting",
            "Security posture classification"
        ]
        
        return SpecializedAnalysis(
            document_type="SCAP Security Report",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=data_inventory,
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_quality_metrics(root)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        # Extract key SCAP data
        return {
            "scan_results": self._extract_scan_results(root),
            "system_info": self._extract_system_info(root),
            "compliance_scores": self._extract_compliance_scores(root)
        }
    
    def _count_vulnerabilities(self, root: ET.Element) -> Dict[str, int]:
        # Implementation for counting vulnerabilities by severity
        return {"high": 0, "medium": 0, "low": 0}
    
    def _extract_compliance_summary(self, root: ET.Element) -> Dict[str, Any]:
        # Implementation for extracting compliance summary
        return {}
    
    def _extract_scan_results(self, root: ET.Element) -> List[Dict[str, Any]]:
        # Implementation for extracting scan results
        return []
    
    def _extract_system_info(self, root: ET.Element) -> Dict[str, Any]:
        # Implementation for extracting system information
        return {}
    
    def _extract_compliance_scores(self, root: ET.Element) -> Dict[str, float]:
        # Implementation for extracting compliance scores
        return {}
    
    def _calculate_quality_metrics(self, root: ET.Element) -> Dict[str, float]:
        return {
            "completeness": 0.85,
            "consistency": 0.90,
            "data_density": 0.75
        }

class RSSHandler(XMLHandler):
    """Handler for RSS feed documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        if root.tag == 'rss' or root.tag.endswith('}rss'):
            return True, 1.0
        if root.tag == 'feed':  # Atom feeds
            return True, 0.9
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        version = root.get('version', '2.0')
        feed_type = 'RSS' if root.tag.endswith('rss') else 'Atom'
        
        return DocumentTypeInfo(
            type_name=f"{feed_type} Feed",
            confidence=1.0,
            version=version,
            metadata={
                "standard": feed_type,
                "category": "content_syndication"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        channel = root.find('.//channel') or root
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        findings = {
            'total_items': len(items),
            'has_descriptions': sum(1 for item in items if item.find('.//description') is not None),
            'has_dates': sum(1 for item in items if item.find('.//pubDate') is not None),
            'categories': self._extract_categories(items)
        }
        
        recommendations = [
            "Use for content aggregation and analysis",
            "Extract for trend analysis and topic modeling",
            "Monitor for content updates and changes"
        ]
        
        ai_use_cases = [
            "Content categorization and tagging",
            "Trend detection and analysis",
            "Sentiment analysis on articles",
            "Topic modeling and clustering",
            "Content recommendation systems"
        ]
        
        return SpecializedAnalysis(
            document_type="RSS/Atom Feed",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory={'articles': len(items), 'categories': len(findings['categories'])},
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_feed_quality(root, items)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        items = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')
        
        return {
            'feed_metadata': self._extract_feed_metadata(root),
            'items': [self._extract_item_data(item) for item in items[:10]]  # First 10 items
        }
    
    def _extract_categories(self, items) -> List[str]:
        categories = set()
        for item in items:
            for cat in item.findall('.//category'):
                if cat.text:
                    categories.add(cat.text)
        return list(categories)
    
    def _extract_feed_metadata(self, root: ET.Element) -> Dict[str, Any]:
        channel = root.find('.//channel') or root
        return {
            'title': getattr(channel.find('.//title'), 'text', None),
            'description': getattr(channel.find('.//description'), 'text', None),
            'link': getattr(channel.find('.//link'), 'text', None)
        }
    
    def _extract_item_data(self, item: ET.Element) -> Dict[str, Any]:
        return {
            'title': getattr(item.find('.//title'), 'text', None),
            'description': getattr(item.find('.//description'), 'text', None),
            'pubDate': getattr(item.find('.//pubDate'), 'text', None),
            'link': getattr(item.find('.//link'), 'text', None)
        }
    
    def _calculate_feed_quality(self, root: ET.Element, items: List[ET.Element]) -> Dict[str, float]:
        total = len(items)
        if total == 0:
            return {"completeness": 0.0, "consistency": 0.0, "data_density": 0.0}
        
        with_desc = sum(1 for item in items if item.find('.//description') is not None)
        with_date = sum(1 for item in items if item.find('.//pubDate') is not None)
        
        return {
            "completeness": (with_desc + with_date) / (2 * total),
            "consistency": 1.0 if with_desc == total else with_desc / total,
            "data_density": 0.8  # Typical for RSS feeds
        }

class SVGHandler(XMLHandler):
    """Handler for SVG (Scalable Vector Graphics) documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        if root.tag == '{http://www.w3.org/2000/svg}svg' or root.tag == 'svg':
            return True, 1.0
        return False, 0.0
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        return DocumentTypeInfo(
            type_name="SVG Graphics",
            confidence=1.0,
            version=root.get('version', '1.1'),
            schema_uri="http://www.w3.org/2000/svg",
            metadata={
                "standard": "W3C SVG",
                "category": "graphics"
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'dimensions': {
                'width': root.get('width'),
                'height': root.get('height'),
                'viewBox': root.get('viewBox')
            },
            'element_types': self._count_svg_elements(root),
            'has_animations': self._check_animations(root),
            'has_scripts': len(root.findall('.//script')) > 0,
            'complexity_score': self._calculate_complexity(root)
        }
        
        recommendations = [
            "Extract for design system documentation",
            "Analyze for accessibility improvements",
            "Convert to other formats for broader compatibility"
        ]
        
        ai_use_cases = [
            "Automatic icon/graphic classification",
            "Design pattern recognition",
            "Accessibility analysis",
            "Style extraction for design systems",
            "Vector graphic optimization"
        ]
        
        return SpecializedAnalysis(
            document_type="SVG Graphics",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=findings['element_types'],
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._calculate_svg_quality(root)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'metadata': self._extract_svg_metadata(root),
            'structure': self._extract_structure(root),
            'styles': self._extract_styles(root)
        }
    
    def _count_svg_elements(self, root: ET.Element) -> Dict[str, int]:
        elements = {}
        for elem in root.iter():
            tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            elements[tag] = elements.get(tag, 0) + 1
        return elements
    
    def _check_animations(self, root: ET.Element) -> bool:
        animation_tags = ['animate', 'animateTransform', 'animateMotion', 'set']
        for tag in animation_tags:
            if root.find(f'.//{{{root.tag.split("}")[0][1:] if "}" in root.tag else ""}}}{tag}'):
                return True
        return False
    
    def _calculate_complexity(self, root: ET.Element) -> float:
        total_elements = len(list(root.iter()))
        return min(total_elements / 100.0, 1.0)
    
    def _extract_svg_metadata(self, root: ET.Element) -> Dict[str, Any]:
        metadata = {}
        for elem in root:
            if elem.tag.endswith('metadata'):
                # Extract metadata content
                pass
        return metadata
    
    def _extract_structure(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'groups': len(root.findall('.//g')),
            'paths': len(root.findall('.//path')),
            'max_depth': self._calculate_max_depth(root)
        }
    
    def _extract_styles(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'inline_styles': len([e for e in root.iter() if e.get('style')]),
            'classes': len(set(e.get('class', '') for e in root.iter() if e.get('class')))
        }
    
    def _calculate_max_depth(self, elem: ET.Element, depth: int = 0) -> int:
        if not list(elem):
            return depth
        return max(self._calculate_max_depth(child, depth + 1) for child in elem)
    
    def _calculate_svg_quality(self, root: ET.Element) -> Dict[str, float]:
        has_viewbox = 1.0 if root.get('viewBox') else 0.0
        has_title = 1.0 if root.find('.//title') is not None else 0.0
        
        return {
            "completeness": (has_viewbox + has_title) / 2,
            "accessibility": has_title,
            "scalability": has_viewbox
        }

class GenericXMLHandler(XMLHandler):
    """Fallback handler for generic XML documents"""
    
    def can_handle(self, root: ET.Element, namespaces: Dict[str, str]) -> Tuple[bool, float]:
        # This handler can handle any XML
        return True, 0.1  # Low confidence as it's a fallback
    
    def detect_type(self, root: ET.Element, namespaces: Dict[str, str]) -> DocumentTypeInfo:
        # Try to infer type from root element and namespaces
        root_tag = root.tag.split('}')[-1] if '}' in root.tag else root.tag
        
        return DocumentTypeInfo(
            type_name=f"Generic XML ({root_tag})",
            confidence=0.5,
            metadata={
                "root_element": root_tag,
                "namespace_count": len(namespaces)
            }
        )
    
    def analyze(self, root: ET.Element, file_path: str) -> SpecializedAnalysis:
        findings = {
            'structure': self._analyze_structure(root),
            'data_patterns': self._detect_patterns(root),
            'attribute_usage': self._analyze_attributes(root)
        }
        
        recommendations = [
            "Review structure for data extraction opportunities",
            "Consider creating a specialized handler for this document type",
            "Analyze repeating patterns for structured data extraction"
        ]
        
        ai_use_cases = [
            "Schema learning and validation",
            "Data extraction and transformation",
            "Pattern recognition",
            "Anomaly detection in structure"
        ]
        
        return SpecializedAnalysis(
            document_type="Generic XML",
            key_findings=findings,
            recommendations=recommendations,
            data_inventory=self._inventory_data(root),
            ai_use_cases=ai_use_cases,
            structured_data=self.extract_key_data(root),
            quality_metrics=self._analyze_quality(root)
        )
    
    def extract_key_data(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'sample_data': self._extract_samples(root),
            'schema_inference': self._infer_schema(root)
        }
    
    def _analyze_structure(self, root: ET.Element) -> Dict[str, Any]:
        return {
            'max_depth': self._calculate_depth(root),
            'element_count': len(list(root.iter())),
            'unique_paths': len(self._get_unique_paths(root))
        }
    
    def _detect_patterns(self, root: ET.Element) -> Dict[str, Any]:
        # Detect repeating structures
        element_counts = {}
        for elem in root.iter():
            tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            element_counts[tag] = element_counts.get(tag, 0) + 1
        
        return {
            'repeating_elements': {k: v for k, v in element_counts.items() if v > 5},
            'likely_records': [k for k, v in element_counts.items() if v > 10]
        }
    
    def _analyze_attributes(self, root: ET.Element) -> Dict[str, Any]:
        attr_usage = {}
        for elem in root.iter():
            for attr in elem.attrib:
                attr_usage[attr] = attr_usage.get(attr, 0) + 1
        return attr_usage
    
    def _inventory_data(self, root: ET.Element) -> Dict[str, int]:
        inventory = {}
        for elem in root.iter():
            tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
            inventory[tag] = inventory.get(tag, 0) + 1
        return inventory
    
    def _extract_samples(self, root: ET.Element, max_samples: int = 5) -> List[Dict[str, Any]]:
        samples = []
        for i, elem in enumerate(root.iter()):
            if i >= max_samples:
                break
            if elem.text and elem.text.strip():
                samples.append({
                    'path': self._get_path(elem),
                    'tag': elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag,
                    'text': elem.text.strip()[:100],
                    'attributes': dict(elem.attrib)
                })
        return samples
    
    def _infer_schema(self, root: ET.Element) -> Dict[str, Any]:
        # Basic schema inference
        return {
            'probable_record_types': self._detect_patterns(root)['likely_records'],
            'hierarchical': self._calculate_depth(root) > 3
        }
    
    def _calculate_depth(self, elem: ET.Element, depth: int = 0) -> int:
        if not list(elem):
            return depth
        return max(self._calculate_depth(child, depth + 1) for child in elem)
    
    def _get_unique_paths(self, root: ET.Element) -> set:
        paths = set()
        
        def traverse(elem, path):
            current_path = f"{path}/{elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag}"
            paths.add(current_path)
            for child in elem:
                traverse(child, current_path)
        
        traverse(root, "")
        return paths
    
    def _get_path(self, elem: ET.Element) -> str:
        # Simple path extraction (would need more complex logic for full path)
        return elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
    
    def _analyze_quality(self, root: ET.Element) -> Dict[str, float]:
        total_elements = len(list(root.iter()))
        elements_with_text = sum(1 for e in root.iter() if e.text and e.text.strip())
        elements_with_attrs = sum(1 for e in root.iter() if e.attrib)
        
        return {
            "data_density": elements_with_text / total_elements if total_elements > 0 else 0,
            "attribute_usage": elements_with_attrs / total_elements if total_elements > 0 else 0,
            "structure_consistency": 0.7  # Would need more analysis
        }

class XMLDocumentAnalyzer:
    """Main analyzer that uses specialized handlers"""
    
    def __init__(self):
        # Register all handlers
        self.handlers: List[Type[XMLHandler]] = [
            SCAPHandler(),
            RSSHandler(),
            SVGHandler(),
            # Add more handlers here as needed
            GenericXMLHandler()  # Always last as fallback
        ]
    
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """Analyze an XML document using the appropriate handler"""
        
        # Parse the document
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
        except ET.ParseError as e:
            return {
                "error": f"Failed to parse XML: {e}",
                "file_path": file_path
            }
        
        # Extract namespaces
        namespaces = self._extract_namespaces(root)
        
        # Find the best handler
        best_handler = None
        best_confidence = 0.0
        
        for handler in self.handlers:
            can_handle, confidence = handler.can_handle(root, namespaces)
            if can_handle and confidence > best_confidence:
                best_handler = handler
                best_confidence = confidence
        
        if not best_handler:
            best_handler = self.handlers[-1]  # Use generic handler
        
        # Detect document type
        doc_type = best_handler.detect_type(root, namespaces)
        
        # Perform specialized analysis
        analysis = best_handler.analyze(root, file_path)
        
        # Combine results
        return {
            "file_path": file_path,
            "document_type": doc_type,
            "handler_used": best_handler.__class__.__name__,
            "confidence": best_confidence,
            "analysis": analysis,
            "namespaces": namespaces,
            "file_size": Path(file_path).stat().st_size
        }
    
    def _extract_namespaces(self, root: ET.Element) -> Dict[str, str]:
        """Extract all namespaces from the document"""
        namespaces = {}
        
        # Get namespaces from root element
        for key, value in root.attrib.items():
            if key.startswith('xmlns'):
                prefix = key.split(':')[1] if ':' in key else 'default'
                namespaces[prefix] = value
        
        # Also check for namespaces in element tags
        for elem in root.iter():
            if '}' in elem.tag:
                uri = elem.tag.split('}')[0][1:]
                # Try to find a prefix for this URI
                prefix = None
                for p, u in namespaces.items():
                    if u == uri:
                        prefix = p
                        break
                if not prefix:
                    prefix = f"ns{len(namespaces)}"
                    namespaces[prefix] = uri
        
        return namespaces

# Example usage
if __name__ == "__main__":
    analyzer = XMLDocumentAnalyzer()
    
    # Example: Analyze a file
    result = analyzer.analyze_document("sample_data/example.xml")
    
    print(json.dumps(result, indent=2, default=str))
</file>

<file path="xml-test-files-guide.md">
# XML Test Files Collection Guide

## Overview
This guide lists all XML document types needed for testing the XML Analysis Framework, along with suggested searches to find real-world examples.

##  Currently Implemented Handlers

### 1. SCAP Security Reports
**File Types**: `.xml` (usually large, 10-100MB)
**Search Queries**:
- `"asset-report-collection" filetype:xml site:github.com`
- `SCAP XCCDF benchmark filetype:xml`
- `"scap.nist.gov/schema" filetype:xml`
- `STIG SCAP results filetype:xml`

**Good Sources**:
- NIST National Checklist Program Repository
- DoD Cyber Exchange (public STIG benchmarks)
- GitHub repos with "scap" or "openscap" topics

### 2. RSS/Atom Feeds
**File Types**: `.xml`, `.rss`, `.atom`
**Search Queries**:
- `"rss version=" filetype:xml -site:w3.org`
- `atom feed example filetype:xml`
- `podcast RSS feed filetype:xml`
- `news RSS feed example github`

**Good Sources**:
- Major news sites (append `/rss` to URLs)
- Podcast platforms
- Blog platforms (WordPress, Medium exports)

### 3. Maven POM Files
**File Types**: `pom.xml`
**Search Queries**:
- `pom.xml site:github.com`
- `"modelVersion>4.0.0" filetype:xml`
- `maven project pom.xml example`
- `spring boot pom.xml site:github.com`

**Good Sources**:
- Any Java project on GitHub
- Maven Central examples
- Spring Initializr generated projects

### 4. Log4j Configuration
**File Types**: `log4j.xml`, `log4j2.xml`
**Search Queries**:
- `log4j2.xml site:github.com`
- `"Configuration status=" filetype:xml log4j`
- `log4j.xml example configuration`
- `"<Appenders>" "<Loggers>" filetype:xml`

**Good Sources**:
- Java projects on GitHub
- Apache Log4j documentation
- Enterprise Java application repos

### 5. Spring Configuration
**File Types**: `applicationContext.xml`, `*-context.xml`, `beans.xml`
**Search Queries**:
- `applicationContext.xml site:github.com`
- `"springframework.org/schema/beans" filetype:xml`
- `spring beans.xml example`
- `"<bean id=" class=" filetype:xml`

**Good Sources**:
- Legacy Spring projects
- Spring documentation archives
- Enterprise Java applications

### 6. DocBook Documentation
**File Types**: `.xml`, `.docbook`
**Search Queries**:
- `"<book xmlns" docbook filetype:xml`
- `"<chapter>" "<section>" docbook filetype:xml`
- `docbook 5.0 example filetype:xml`
- `technical documentation docbook github`

**Good Sources**:
- Open source documentation projects
- O'Reilly book sources
- Technical manual repositories

### 7. SVG Graphics
**File Types**: `.svg`
**Search Queries**:
- `"<svg" "viewBox" filetype:svg`
- `icon svg site:github.com`
- `"sodipodi" inkscape filetype:svg` (Inkscape files)
- `animated svg example`

**Good Sources**:
- Icon libraries (Font Awesome, Feather)
- Wikimedia Commons
- Design tool exports

### 8. XML Sitemaps
**File Types**: `sitemap.xml`, `sitemap_index.xml`
**Search Queries**:
- `sitemap.xml -site:sitemaps.org`
- `"<urlset" "sitemaps.org/schemas" filetype:xml`
- `sitemap_index.xml example`
- `"<loc>" "<lastmod>" filetype:xml`

**Good Sources**:
- Any website's `/sitemap.xml`
- WordPress sites
- E-commerce platforms

##  Planned Handlers (Need Test Files)

### 9. WSDL (Web Services Description Language)
**File Types**: `.wsdl`, `.xml`
**Search Queries**:
- `"definitions" "xmlns:wsdl" filetype:wsdl`
- `SOAP WSDL example filetype:xml`
- `"<wsdl:portType" filetype:xml`
- `web service WSDL site:github.com`

**Good Sources**:
- Public web service directories
- Government service endpoints
- Legacy enterprise integrations

### 10. XSD (XML Schema Definition)
**File Types**: `.xsd`
**Search Queries**:
- `"<xs:schema" filetype:xsd`
- `"<xsd:complexType" filetype:xsd`
- `XML schema example site:github.com`
- `"targetNamespace" schema filetype:xsd`

**Good Sources**:
- W3C specifications
- Industry standard schemas
- API documentation

### 11. KML/KMZ (Geographic Data)
**File Types**: `.kml`, `.kmz`
**Search Queries**:
- `"<kml xmlns" filetype:kml`
- `Google Earth KML example`
- `"<Placemark>" coordinates filetype:kml`
- `GPS track KML site:github.com`

**Good Sources**:
- Google Earth community
- GIS data repositories
- GPS tracking apps exports

### 12. GPX (GPS Exchange)
**File Types**: `.gpx`
**Search Queries**:
- `"<gpx version" filetype:gpx`
- `GPS track GPX example`
- `Strava GPX export site:github.com`
- `hiking trail GPX file`

**Good Sources**:
- Outdoor activity platforms
- GPS device manufacturers
- OpenStreetMap exports

### 13. Ant/NAnt Build Files
**File Types**: `build.xml`
**Search Queries**:
- `build.xml ant project site:github.com`
- `"<project name=" default=" filetype:xml ant`
- `"<target name=" depends=" filetype:xml`
- `Apache Ant build.xml example`

**Good Sources**:
- Legacy Java projects
- Apache project archives
- Enterprise build systems

### 14. NuGet Package Specs
**File Types**: `.nuspec`
**Search Queries**:
- `"<package xmlns" filetype:nuspec`
- `nuspec example site:github.com`
- `"<metadata>" "<id>" nuget filetype:xml`
- `.nuspec file example`

**Good Sources**:
- .NET projects on GitHub
- NuGet.org package sources
- Visual Studio templates

### 15. WADL (Web Application Description Language)
**File Types**: `.wadl`
**Search Queries**:
- `"<application xmlns" wadl filetype:xml`
- `REST API WADL example`
- `"<resources base=" filetype:wadl`
- `Jersey WADL site:github.com`

**Good Sources**:
- Java REST services
- API documentation
- Jersey/JAX-RS projects

### 16. RelaxNG Schemas
**File Types**: `.rng`, `.rnc`
**Search Queries**:
- `"<grammar" relaxng filetype:rng`
- `RelaxNG schema example`
- `"datatypeLibrary" filetype:rng`
- `compact syntax .rnc file`

**Good Sources**:
- XML validation projects
- DocBook schemas
- TEI schemas

### 17. DITA (Darwin Information Typing Architecture)
**File Types**: `.dita`, `.ditamap`
**Search Queries**:
- `"<topic" dita filetype:xml`
- `"<map>" ditamap filetype:xml`
- `DITA documentation example`
- `"<!DOCTYPE topic" filetype:dita`

**Good Sources**:
- Technical documentation repos
- DITA Open Toolkit
- IBM documentation

### 18. TEI (Text Encoding Initiative)
**File Types**: `.xml`
**Search Queries**:
- `"<TEI xmlns" filetype:xml`
- `"tei-c.org" manuscript filetype:xml`
- `digital humanities TEI XML`
- `"<teiHeader>" filetype:xml`

**Good Sources**:
- Digital humanities projects
- University libraries
- Historical text projects

### 19. HL7 CDA (Clinical Document Architecture)
**File Types**: `.xml`
**Search Queries**:
- `"ClinicalDocument" HL7 filetype:xml`
- `"urn:hl7-org:v3" filetype:xml`
- `CDA R2 example document`
- `"<typeId root=" HL7 filetype:xml`

**Good Sources**:
- HL7 example repository
- Healthcare IT projects
- EHR vendor documentation

### 20. XBRL (Business Reporting)
**File Types**: `.xbrl`, `.xml`
**Search Queries**:
- `"<xbrl" financial filetype:xml`
- `XBRL instance document example`
- `"xbrl.org" context filetype:xml`
- `SEC XBRL filing example`

**Good Sources**:
- SEC EDGAR database
- XBRL.org examples
- Financial reporting tools

### 21. PMML (Predictive Models)
**File Types**: `.pmml`
**Search Queries**:
- `"<PMML" version filetype:pmml`
- `"DataDictionary" model filetype:xml`
- `machine learning PMML example`
- `"<RegressionModel" filetype:xml`

**Good Sources**:
- Data science projects
- ML model repositories
- PMML.org examples

### 22. XSLT (Transformations)
**File Types**: `.xsl`, `.xslt`
**Search Queries**:
- `"<xsl:stylesheet" filetype:xsl`
- `XSLT transformation example`
- `"<xsl:template match=" filetype:xsl`
- `XML to HTML XSLT site:github.com`

**Good Sources**:
- XML processing projects
- Documentation generators
- Web development repos

### 23. XSL-FO (Formatting Objects)
**File Types**: `.fo`, `.xml`
**Search Queries**:
- `"<fo:root" filetype:xml`
- `XSL-FO example PDF generation`
- `"fo:page-sequence" filetype:xml`
- `Apache FOP examples`

**Good Sources**:
- Apache FOP examples
- PDF generation projects
- Publishing workflows

##  General Search Tips

### GitHub Advanced Search
Use GitHub's advanced search with:
- `extension:xml path:/test`
- `extension:xml path:/sample`
- `extension:xml path:/example`
- `language:XML size:>1000`

### Google Dorks
- Add `-inurl:w3.org` to exclude specifications
- Use `site:raw.githubusercontent.com` for direct file access
- Add `"<?xml version"` to ensure valid XML
- Use date filters for recent examples

### File Size Considerations
- **Small files** (<100KB): Good for unit tests
- **Medium files** (100KB-10MB): Good for performance testing
- **Large files** (>10MB): Good for stress testing
- **Various encodings**: UTF-8, UTF-16, ISO-8859-1

##  Recommended Test Set Structure

```
test_files/
 small/          # <100KB files
    pom/
    rss/
    config/
    ...
 medium/         # 100KB-10MB files
    scap/
    docbook/
    ...
 large/          # >10MB files
    scap/
    xbrl/
    ...
 edge_cases/     # Malformed, unusual encodings, etc.
    malformed/
    encodings/
    deeply_nested/
 real_world/     # Actual production files
     enterprise/
     open_source/
     government/
```

##  Legal Considerations

When collecting test files:
1. **Check licenses** - Ensure files are publicly available
2. **Remove sensitive data** - PII, credentials, internal URLs
3. **Attribute sources** - Keep track of where files came from
4. **Respect robots.txt** - When scraping websites
5. **Use synthetic data** - Generate files for sensitive domains (healthcare, finance)

##  Test File Validation

After collecting files, validate them:
```bash
# Check if valid XML
xmllint --noout file.xml

# Check file size
ls -lh file.xml

# Check encoding
file -i file.xml

# Count elements (rough complexity check)
grep -c "<" file.xml
```

##  Coverage Goals

Aim for at least:
- **3-5 examples** per handler
- **Different sizes** (small, medium, large)
- **Different versions** (where applicable)
- **Valid and invalid** examples
- **Real-world complexity** (not just tutorials)

---

This collection guide ensures comprehensive testing coverage for all current and planned XML document types in the framework.
</file>

</files>
