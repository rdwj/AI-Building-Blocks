{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STIG to Ansible Prompt Engineering Workflow\n",
    "\n",
    "This notebook allows manual iteration through the 7-step workflow for converting STIG findings to Ansible playbooks.\n",
    "\n",
    "You can:\n",
    "- Test each prompt individually\n",
    "- Edit prompts in the `prompts/` directory and reload them\n",
    "- See the intermediate results at each step\n",
    "- Manually adjust the workflow state between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from IPython.display import display, Markdown, JSON\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Import our LLM interface\n",
    "from llm_interface import LLMInterface\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLMInterface()\n",
    "print(f\"LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_prompt(prompt_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load a prompt from the prompts directory\"\"\"\n",
    "    prompt_path = Path(f\"../prompts/{prompt_name}.yaml\")\n",
    "    with open(prompt_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def format_prompt(prompt_data: Dict[str, Any], **kwargs) -> str:\n",
    "    \"\"\"Format a prompt template with provided variables\"\"\"\n",
    "    template_str = prompt_data['template']\n",
    "    template = Template(template_str)\n",
    "    return template.safe_substitute(**kwargs)\n",
    "\n",
    "def extract_json_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract JSON from LLM response using regex\"\"\"\n",
    "    # Remove any markdown code blocks\n",
    "    response = re.sub(r'```json\\s*', '', response)\n",
    "    response = re.sub(r'```\\s*$', '', response)\n",
    "    \n",
    "    # Find JSON pattern\n",
    "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "    matches = re.findall(json_pattern, response)\n",
    "    \n",
    "    # Try each match to see if it's valid JSON\n",
    "    for match in matches:\n",
    "        try:\n",
    "            return json.loads(match)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "async def llm_call_with_json(prompt: str, expected_keys: List[str], max_tokens: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"Make LLM call and extract JSON response\"\"\"\n",
    "    response = await llm.generate_ansible_task_async(\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    print(f\"\\nRaw LLM Response:\\n{response}\\n\")\n",
    "    \n",
    "    json_data = extract_json_from_response(response)\n",
    "    if json_data:\n",
    "        print(f\"Extracted JSON: {json.dumps(json_data, indent=2)}\")\n",
    "        return json_data\n",
    "    else:\n",
    "        print(\"Failed to extract JSON from response\")\n",
    "        return {key: \"unknown\" for key in expected_keys}\n",
    "\n",
    "# Display helper\n",
    "def display_prompt(prompt_text: str):\n",
    "    \"\"\"Display a prompt nicely formatted\"\"\"\n",
    "    display(Markdown(f\"### Prompt:\\n```\\n{prompt_text}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test finding from the findings directory\n",
    "findings_file = \"../findings/node2.example.com-STIG-20250710162433_findings.json\"\n",
    "\n",
    "with open(findings_file, 'r') as f:\n",
    "    findings_data = json.load(f)\n",
    "\n",
    "# Extract the findings array from the JSON structure\n",
    "all_findings = findings_data.get('findings', [])\n",
    "\n",
    "print(f\"Loaded {len(all_findings)} findings from the file\")\n",
    "\n",
    "# Select a test finding - let's use the telnet one we've been testing with\n",
    "test_finding = None\n",
    "for finding in all_findings:\n",
    "    if finding.get('rule_id') == 'xccdf_org.ssgproject.content_rule_package_telnet_removed':\n",
    "        test_finding = finding\n",
    "        break\n",
    "\n",
    "# If not found, just use the first one\n",
    "if not test_finding:\n",
    "    test_finding = all_findings[0] if all_findings else {}\n",
    "\n",
    "if test_finding:\n",
    "    print(f\"Selected test finding: {test_finding.get('rule_id', 'Unknown')}\")\n",
    "    print(f\"Title: {test_finding.get('title', 'No title')}\")\n",
    "    print(f\"Severity: {test_finding.get('severity', 'Unknown')}\")\n",
    "    print(f\"\\nDescription: {test_finding.get('description', 'No description')[:200]}...\")\n",
    "    print(f\"\\nFix Text: {test_finding.get('fix_text', 'No fix text')[:200]}...\")\n",
    "else:\n",
    "    print(\"No findings found in the file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize workflow state\n",
    "state = {\n",
    "    'finding': test_finding,\n",
    "    'action_type': None,\n",
    "    'target': None,\n",
    "    'parameters': None,\n",
    "    'task_name': None,\n",
    "    'final_playbook': None,\n",
    "    'validation_result': None,\n",
    "    'annotated_playbook': None,\n",
    "    'errors': [],\n",
    "    'metadata': {\n",
    "        'workflow_start': datetime.now().isoformat(),\n",
    "        'rule_id': test_finding.get('rule_id', 'unknown') if test_finding else 'unknown'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Workflow state initialized\")\n",
    "if test_finding:\n",
    "    print(f\"Working with finding: {state['finding'].get('rule_id', 'unknown')}\")\n",
    "else:\n",
    "    print(\"No finding selected - workflow may not work properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract Action Type\n",
    "\n",
    "This step extracts the primary action type from the STIG finding (e.g., remove_package, install_package, configure_file, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Action Type\n",
    "prompt_data = load_prompt('extract_action')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with finding data\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    title=state['finding'].get('title', ''),\n",
    "    description=state['finding'].get('description', ''),\n",
    "    fix_text=state['finding'].get('fix_text', '')\n",
    ")\n",
    "\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['action_type'], max_tokens=50)\n",
    "\n",
    "# Update state\n",
    "state['action_type'] = result.get('action_type', 'unknown')\n",
    "state['metadata']['step1_complete'] = datetime.now().isoformat()\n",
    "\n",
    "print(f\"\\n✅ Extracted action type: {state['action_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Target\n",
    "\n",
    "This step extracts the target of the action (e.g., package name, file path, service name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Target\n",
    "prompt_data = load_prompt('extract_target')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with finding data and previous step result\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    title=state['finding'].get('title', ''),\n",
    "    fix_text=state['finding'].get('fix_text', ''),\n",
    "    action_type=state.get('action_type', 'other')\n",
    ")\n",
    "\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['target'], max_tokens=50)\n",
    "\n",
    "# Update state\n",
    "state['target'] = result.get('target', 'unknown')\n",
    "state['metadata']['step2_complete'] = datetime.now().isoformat()\n",
    "\n",
    "print(f\"\\n✅ Extracted target: {state['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Parameters\n",
    "\n",
    "This step extracts any parameters needed for the action (e.g., state: absent, mode: 0644)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract Parameters\n",
    "prompt_data = load_prompt('extract_parameters')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with finding data and previous step results\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    title=state['finding'].get('title', ''),\n",
    "    fix_text=state['finding'].get('fix_text', ''),\n",
    "    action_type=state.get('action_type', 'other'),\n",
    "    target=state.get('target', 'unknown')\n",
    ")\n",
    "\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['parameter'], max_tokens=50)\n",
    "\n",
    "# Update state\n",
    "state['parameters'] = result.get('parameter', 'default')\n",
    "state['metadata']['step3_complete'] = datetime.now().isoformat()\n",
    "\n",
    "print(f\"\\n✅ Extracted parameters: {state['parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Task Name\n",
    "\n",
    "This step generates a descriptive task name for the Ansible task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Task Name\n",
    "prompt_data = load_prompt('generate_task_name')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with all extracted data\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    rule_id=state['finding'].get('rule_id', ''),\n",
    "    action_type=state.get('action_type', 'other'),\n",
    "    target=state.get('target', 'unknown'),\n",
    "    severity=state['finding'].get('severity', 'medium')\n",
    ")\n",
    "\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['task_name'], max_tokens=100)\n",
    "\n",
    "# Update state\n",
    "state['task_name'] = result.get('task_name', f\"STIG Task: {state.get('target', 'unknown')}\")\n",
    "state['metadata']['step4_complete'] = datetime.now().isoformat()\n",
    "\n",
    "print(f\"\\n✅ Generated task name: {state['task_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Extracted Components\n",
    "\n",
    "Let's review what we've extracted so far before assembling the playbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review extracted components\n",
    "print(\"🔍 Extracted Components Summary:\\n\")\n",
    "print(f\"1. Action Type: {state['action_type']}\")\n",
    "print(f\"2. Target: {state['target']}\")\n",
    "print(f\"3. Parameters: {state['parameters']}\")\n",
    "print(f\"4. Task Name: {state['task_name']}\")\n",
    "print(f\"\\nRule ID: {state['finding'].get('rule_id', 'unknown')}\")\n",
    "print(f\"Severity: {state['finding'].get('severity', 'unknown')}\")\n",
    "\n",
    "# You can manually adjust these values here if needed\n",
    "# state['action_type'] = 'remove_package'\n",
    "# state['target'] = 'telnet'\n",
    "# state['parameters'] = 'absent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Assemble Playbook\n",
    "\n",
    "This step combines all extracted components into a complete Ansible playbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Assemble Playbook\n",
    "prompt_data = load_prompt('assemble_playbook')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Load the template for reference (if available)\n",
    "template_content = \"\"\n",
    "try:\n",
    "    with open('../examples/ansible_playbook_template.yaml', 'r') as f:\n",
    "        template_content = f.read()\n",
    "    print(\"✅ Loaded Ansible template for reference\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load template: {e}\")\n",
    "\n",
    "# Format the prompt with all components\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    task_name=state.get('task_name', 'STIG Task'),\n",
    "    action_type=state.get('action_type', 'other'),\n",
    "    target=state.get('target', 'unknown'),\n",
    "    parameters=state.get('parameters', 'default'),\n",
    "    rule_id=state['finding'].get('rule_id', ''),\n",
    "    severity=state['finding'].get('severity', 'medium'),\n",
    "    template_content=template_content\n",
    ")\n",
    "\n",
    "# Show a truncated version of the prompt (since template is long)\n",
    "display_prompt(prompt[:1000] + \"\\n...\\n[Template content truncated]...\")\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['playbook'], max_tokens=500)\n",
    "\n",
    "# Extract and clean the playbook\n",
    "playbook = result.get('playbook', '')\n",
    "\n",
    "# Remove markdown formatting if present\n",
    "if playbook.startswith('```yaml'):\n",
    "    playbook = playbook[7:]\n",
    "if playbook.endswith('```'):\n",
    "    playbook = playbook[:-3]\n",
    "\n",
    "# Update state\n",
    "state['final_playbook'] = playbook\n",
    "state['metadata']['step5_complete'] = datetime.now().isoformat()\n",
    "\n",
    "print(f\"\\n✅ Assembled playbook:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(playbook)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validate and Fix Playbook\n",
    "\n",
    "This step validates the generated playbook and fixes any issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Validate and Fix Playbook\n",
    "prompt_data = load_prompt('validate_and_fix_playbook')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with playbook and template\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    playbook_content=state['final_playbook'],\n",
    "    template_content=template_content\n",
    ")\n",
    "\n",
    "# Show a truncated version\n",
    "display_prompt(prompt[:1000] + \"\\n...\\n[Content truncated]...\")\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(\n",
    "    prompt, \n",
    "    ['is_valid', 'issues_found', 'fixes_applied', 'fixed_playbook', 'suggestions'], \n",
    "    max_tokens=800\n",
    ")\n",
    "\n",
    "# Update state\n",
    "state['validation_result'] = result\n",
    "\n",
    "# If fixes were applied, update the playbook\n",
    "if result.get('fixes_applied') and result.get('fixed_playbook'):\n",
    "    state['final_playbook'] = result['fixed_playbook']\n",
    "    print(f\"\\n✅ Applied {len(result['fixes_applied'])} fixes\")\n",
    "else:\n",
    "    print(\"\\n✅ No fixes needed\")\n",
    "\n",
    "state['metadata']['step6_complete'] = datetime.now().isoformat()\n",
    "\n",
    "# Display validation results\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"- Valid: {result.get('is_valid', False)}\")\n",
    "print(f\"- Issues Found: {len(result.get('issues_found', []))}\")\n",
    "if result.get('issues_found'):\n",
    "    for issue in result['issues_found']:\n",
    "        print(f\"  • {issue}\")\n",
    "print(f\"- Fixes Applied: {len(result.get('fixes_applied', []))}\")\n",
    "if result.get('fixes_applied'):\n",
    "    for fix in result['fixes_applied']:\n",
    "        print(f\"  • {fix}\")\n",
    "print(f\"- Suggestions: {len(result.get('suggestions', []))}\")\n",
    "if result.get('suggestions'):\n",
    "    for suggestion in result['suggestions']:\n",
    "        print(f\"  • {suggestion}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Validated Playbook:\")\n",
    "print(state['final_playbook'])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Annotate Playbook\n",
    "\n",
    "This step adds documentation and comments to the validated playbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Annotate Playbook\n",
    "prompt_data = load_prompt('annotate_playbook')\n",
    "print(f\"Prompt: {prompt_data['name']}\")\n",
    "print(f\"Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with playbook and finding details\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    playbook_content=state['final_playbook'],\n",
    "    rule_id=state['finding'].get('rule_id', ''),\n",
    "    title=state['finding'].get('title', ''),\n",
    "    severity=state['finding'].get('severity', ''),\n",
    "    description=state['finding'].get('description', ''),\n",
    "    check_text=state['finding'].get('check_text', ''),\n",
    "    fix_text=state['finding'].get('fix_text', ''),\n",
    "    references=', '.join(state['finding'].get('references', []))\n",
    ")\n",
    "\n",
    "display_prompt(prompt[:800] + \"\\n...\\n[Content truncated]...\")\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['annotated_playbook'], max_tokens=600)\n",
    "\n",
    "# Update state\n",
    "annotated_playbook = result.get('annotated_playbook', '')\n",
    "if annotated_playbook:\n",
    "    state['annotated_playbook'] = annotated_playbook\n",
    "    print(\"\\n✅ Successfully annotated playbook\")\n",
    "else:\n",
    "    state['annotated_playbook'] = state['final_playbook']\n",
    "    print(\"\\n⚠️ No annotation returned, keeping original\")\n",
    "\n",
    "state['metadata']['step7_complete'] = datetime.now().isoformat()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Annotated Playbook:\")\n",
    "print(state['annotated_playbook'])\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Summary\n",
    "\n",
    "Review the complete workflow results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Summary\n",
    "print(\"🎯 Workflow Summary\\n\" + \"=\"*60)\n",
    "print(f\"\\nFinding: {state['finding'].get('rule_id', 'unknown')}\")\n",
    "print(f\"Title: {state['finding']['title']}\")\n",
    "print(f\"Severity: {state['finding'].get('severity', 'unknown')}\")\n",
    "\n",
    "print(\"\\n📊 Extracted Components:\")\n",
    "print(f\"  1. Action Type: {state['action_type']}\")\n",
    "print(f\"  2. Target: {state['target']}\")\n",
    "print(f\"  3. Parameters: {state['parameters']}\")\n",
    "print(f\"  4. Task Name: {state['task_name']}\")\n",
    "\n",
    "print(\"\\n✅ Workflow Steps Completed:\")\n",
    "for i in range(1, 8):\n",
    "    key = f'step{i}_complete'\n",
    "    if key in state['metadata']:\n",
    "        print(f\"  Step {i}: ✓ ({state['metadata'][key]})\")\n",
    "    else:\n",
    "        print(f\"  Step {i}: ✗\")\n",
    "\n",
    "print(\"\\n📝 Final Output:\")\n",
    "final_playbook = state.get('annotated_playbook') or state.get('final_playbook', 'No playbook generated')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(final_playbook)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save to file if desired\n",
    "save_path = f\"../playbooks/manual_test_{state['finding'].get('rule_id', 'unknown')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yml\"\n",
    "print(f\"\\n💾 To save this playbook, run:\")\n",
    "print(f\"with open('{save_path}', 'w') as f:\\n    f.write(final_playbook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Different Findings\n",
    "\n",
    "You can test with different findings by changing the selection criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available findings to test\n",
    "print(\"Available findings to test:\\n\")\n",
    "for i, finding in enumerate(all_findings[:10]):  # Show first 10\n",
    "    print(f\"{i}: {finding.get('rule_id', 'unknown')} ({finding['severity']})\")\n",
    "    print(f\"   {finding['title'][:60]}...\\n\")\n",
    "\n",
    "# To test a different finding, update test_finding and re-run from cell 4\n",
    "# test_finding = all_findings[3]  # Change index to test different finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering Notes\n",
    "\n",
    "Use this cell to document what worked and what didn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering Notes\n",
    "# \n",
    "# What worked:\n",
    "# - JSON schema in prompts helps enforce structured output\n",
    "# - Short, focused prompts work better with small models\n",
    "# - Breaking down complex tasks into simple extractions\n",
    "# \n",
    "# What didn't work:\n",
    "# - Long, complex prompts with multiple instructions\n",
    "# - Asking for too much output at once\n",
    "# \n",
    "# Ideas to try:\n",
    "# - \n",
    "# - \n",
    "# \n",
    "\n",
    "# You can edit prompts in the ../prompts/ directory and re-run cells to test changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
