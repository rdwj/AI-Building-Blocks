{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Process Deterministic Findings\n",
    "\n",
    "This notebook processes findings that can be deterministically converted to Ansible playbooks.\n",
    "\n",
    "**Input:** \n",
    "- Enhanced findings JSON file (from Step 1)\n",
    "- Ansible targets JSON file (from Step 1)\n",
    "\n",
    "**Output:**\n",
    "- Ansible playbooks for deterministic findings\n",
    "- JSON file with findings that need LLM processing\n",
    "- Processing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Libraries imported successfully\n",
      "🐍 Python version: 3.11.12\n",
      "📁 Current working directory: /Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Import our modules\n",
    "from ansible_playbook_generator import DeterministicPlaybookGenerator\n",
    "from shared.prompt_utils import load_findings_file, get_severity_counts\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"📁 Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕐 Processing run: 20250714_120000\n",
      "📂 Input files:\n",
      "   Enhanced findings: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json\n",
      "   Ansible targets: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json\n",
      "📁 Output directory: ../playbooks/20250714_120000\n",
      "✅ Found: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json\n",
      "✅ Found: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths from Step 1 output\n",
    "# Copy these values from the Step 1 notebook output:\n",
    "\n",
    "RUN_TIMESTAMP = \"20250714_120000\"  # Update from Step 1\n",
    "ENHANCED_FINDINGS_FILE = \"../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json\"  # Update from Step 1\n",
    "ANSIBLE_TARGETS_FILE = \"../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json\"  # Update from Step 1\n",
    "\n",
    "# Output configuration\n",
    "PLAYBOOKS_BASE_DIR = \"../playbooks\"\n",
    "PLAYBOOKS_RUN_DIR = f\"{PLAYBOOKS_BASE_DIR}/{RUN_TIMESTAMP}\"\n",
    "\n",
    "print(f\"🕐 Processing run: {RUN_TIMESTAMP}\")\n",
    "print(f\"📂 Input files:\")\n",
    "print(f\"   Enhanced findings: {ENHANCED_FINDINGS_FILE}\")\n",
    "print(f\"   Ansible targets: {ANSIBLE_TARGETS_FILE}\")\n",
    "print(f\"📁 Output directory: {PLAYBOOKS_RUN_DIR}\")\n",
    "\n",
    "# Verify input files exist\n",
    "for file_path in [ENHANCED_FINDINGS_FILE, ANSIBLE_TARGETS_FILE]:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"✅ Found: {file_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Missing: {file_path}\")\n",
    "        print(\"Please update the file paths in the cell above from Step 1 output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Created directory structure:\n",
      "   Main: ../playbooks/20250714_120000\n",
      "   Deterministic: ../playbooks/20250714_120000/deterministic\n",
      "   LLM needed: ../playbooks/20250714_120000/llm_needed\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure\n",
    "playbooks_dir = Path(PLAYBOOKS_RUN_DIR)\n",
    "playbooks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create subdirectories\n",
    "deterministic_dir = playbooks_dir / \"deterministic\"\n",
    "llm_needed_dir = playbooks_dir / \"llm_needed\"\n",
    "\n",
    "deterministic_dir.mkdir(exist_ok=True)\n",
    "llm_needed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 Created directory structure:\")\n",
    "print(f\"   Main: {playbooks_dir}\")\n",
    "print(f\"   Deterministic: {deterministic_dir}\")\n",
    "print(f\"   LLM needed: {llm_needed_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading findings data...\n",
      "📈 Enhanced findings loaded: 1529\n",
      "📄 Metadata: ARF\n",
      "🎯 Actionable targets loaded: 435\n",
      "📊 Total actionable: 435\n",
      "\n",
      "📈 Severity distribution: {'medium': 1221, 'high': 69, 'low': 119, 'unknown': 120}\n",
      "🎯 Target type distribution: {'unknown': 1094, 'package': 118, 'service': 78, 'mount': 50, 'file_ownership': 72, 'file_permission': 53, 'sysctl': 64}\n"
     ]
    }
   ],
   "source": [
    "# Load the findings and targets data\n",
    "print(\"📊 Loading findings data...\")\n",
    "\n",
    "# Load enhanced findings\n",
    "with open(ENHANCED_FINDINGS_FILE, 'r') as f:\n",
    "    enhanced_data = json.load(f)\n",
    "    \n",
    "enhanced_findings = enhanced_data.get('findings', [])\n",
    "metadata = enhanced_data.get('metadata', {})\n",
    "summary = enhanced_data.get('summary', {})\n",
    "\n",
    "print(f\"📈 Enhanced findings loaded: {len(enhanced_findings)}\")\n",
    "print(f\"📄 Metadata: {metadata.get('format', 'Unknown format')}\")\n",
    "\n",
    "# Load ansible targets\n",
    "with open(ANSIBLE_TARGETS_FILE, 'r') as f:\n",
    "    targets_data = json.load(f)\n",
    "    \n",
    "actionable_targets = targets_data.get('targets', [])\n",
    "targets_metadata = targets_data.get('metadata', {})\n",
    "\n",
    "print(f\"🎯 Actionable targets loaded: {len(actionable_targets)}\")\n",
    "print(f\"📊 Total actionable: {targets_metadata.get('total_actionable', 0)}\")\n",
    "\n",
    "# Show severity distribution\n",
    "if 'by_severity' in summary:\n",
    "    print(f\"\\n📈 Severity distribution: {summary['by_severity']}\")\n",
    "if 'by_target_type' in summary:\n",
    "    print(f\"🎯 Target type distribution: {summary['by_target_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing findings for deterministic processing...\n",
      "\n",
      "📊 Processing Analysis:\n",
      "   Total findings: 1529\n",
      "   Deterministic processing: 435\n",
      "   LLM processing needed: 1094\n",
      "   Processing ratio: 28.4% deterministic\n"
     ]
    }
   ],
   "source": [
    "# Analyze which findings can be processed deterministically\n",
    "print(\"🔍 Analyzing findings for deterministic processing...\")\n",
    "\n",
    "# Separate findings into categories\n",
    "deterministic_findings = []\n",
    "llm_needed_findings = []\n",
    "\n",
    "# Create a lookup map of targets by rule_id\n",
    "targets_by_rule = {target['rule_id']: target for target in actionable_targets}\n",
    "\n",
    "for finding in enhanced_findings:\n",
    "    rule_id = finding.get('rule_id', '')\n",
    "    \n",
    "    # Check if we have a deterministic target for this finding\n",
    "    if rule_id in targets_by_rule:\n",
    "        target = targets_by_rule[rule_id]\n",
    "        \n",
    "        # Check if target type is deterministic (not 'unknown')\n",
    "        if target.get('target_type') != 'unknown':\n",
    "            deterministic_findings.append({\n",
    "                'finding': finding,\n",
    "                'target': target\n",
    "            })\n",
    "        else:\n",
    "            llm_needed_findings.append(finding)\n",
    "    else:\n",
    "        # No target info means we need LLM processing\n",
    "        llm_needed_findings.append(finding)\n",
    "\n",
    "print(f\"\\n📊 Processing Analysis:\")\n",
    "print(f\"   Total findings: {len(enhanced_findings)}\")\n",
    "print(f\"   Deterministic processing: {len(deterministic_findings)}\")\n",
    "print(f\"   LLM processing needed: {len(llm_needed_findings)}\")\n",
    "print(f\"   Processing ratio: {len(deterministic_findings)/len(enhanced_findings)*100:.1f}% deterministic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Sample Deterministic Findings (first 3):\n",
      "\n",
      "📋 Finding 1:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_package_prelink_removed\n",
      "   Severity: medium\n",
      "   Title: Package \"prelink\" Must not be Installed...\n",
      "   Target Type: package\n",
      "   Target Name: prelink\n",
      "   Ansible Module: yum\n",
      "\n",
      "📋 Finding 2:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_package_aide_installed\n",
      "   Severity: medium\n",
      "   Title: Install AIDE...\n",
      "   Target Type: package\n",
      "   Target Name: aide\n",
      "   Ansible Module: yum\n",
      "\n",
      "📋 Finding 3:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_package_dracut-fips_installed\n",
      "   Severity: medium\n",
      "   Title: Install the dracut-fips Package...\n",
      "   Target Type: package\n",
      "   Target Name: dracut-fips\n",
      "   Ansible Module: yum\n"
     ]
    }
   ],
   "source": [
    "# Show sample deterministic findings\n",
    "if deterministic_findings:\n",
    "    print(\"🔍 Sample Deterministic Findings (first 3):\")\n",
    "    for i, item in enumerate(deterministic_findings[:3]):\n",
    "        finding = item['finding']\n",
    "        target = item['target']\n",
    "        \n",
    "        print(f\"\\n📋 Finding {i+1}:\")\n",
    "        print(f\"   Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "        print(f\"   Severity: {finding.get('severity', 'Unknown')}\")\n",
    "        print(f\"   Title: {finding.get('title', 'Unknown')[:60]}...\")\n",
    "        print(f\"   Target Type: {target.get('target_type', 'Unknown')}\")\n",
    "        print(f\"   Target Name: {target.get('target_name', 'Unknown')}\")\n",
    "        print(f\"   Ansible Module: {target.get('ansible_module', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating deterministic Ansible playbooks for 435 findings...\n",
      "💾 Saved deterministic targets to: ../playbooks/20250714_120000/deterministic/deterministic_targets.json\n",
      "✅ Generated playbook with 435 tasks: ../playbooks/20250714_120000/deterministic/deterministic_remediation_20250714_120000.yml\n",
      "✅ Successfully generated deterministic playbook!\n",
      "📄 Playbook saved to: ../playbooks/20250714_120000/deterministic/deterministic_remediation_20250714_120000.yml\n",
      "📊 Playbook Statistics:\n",
      "   Total plays: 1\n",
      "   Total tasks: 443\n",
      "   Total handlers: 1\n",
      "   File size: 112.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Generate deterministic Ansible playbooks\n",
    "if deterministic_findings:\n",
    "    print(f\"🚀 Generating deterministic Ansible playbooks for {len(deterministic_findings)} findings...\")\n",
    "    \n",
    "    # Initialize the deterministic playbook generator\n",
    "    generator = DeterministicPlaybookGenerator()\n",
    "    \n",
    "    # Create a targets file for the deterministic findings\n",
    "    deterministic_targets = [item['target'] for item in deterministic_findings]\n",
    "    \n",
    "    deterministic_targets_file = deterministic_dir / \"deterministic_targets.json\"\n",
    "    deterministic_targets_data = {\n",
    "        'metadata': {\n",
    "            'total_actionable': len(deterministic_targets),\n",
    "            'extraction_date': datetime.now().isoformat(),\n",
    "            'source': 'deterministic_processing',\n",
    "            'run_timestamp': RUN_TIMESTAMP\n",
    "        },\n",
    "        'targets': deterministic_targets\n",
    "    }\n",
    "    \n",
    "    with open(deterministic_targets_file, 'w') as f:\n",
    "        json.dump(deterministic_targets_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved deterministic targets to: {deterministic_targets_file}\")\n",
    "    \n",
    "    # Generate the playbook\n",
    "    playbook_file = deterministic_dir / f\"deterministic_remediation_{RUN_TIMESTAMP}.yml\"\n",
    "    \n",
    "    try:\n",
    "        playbook = generator.generate_playbook_from_targets(\n",
    "            str(deterministic_targets_file), \n",
    "            str(playbook_file)\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully generated deterministic playbook!\")\n",
    "        print(f\"📄 Playbook saved to: {playbook_file}\")\n",
    "        \n",
    "        # Show playbook stats\n",
    "        total_tasks = sum(len(play.get('tasks', [])) for play in playbook)\n",
    "        total_handlers = sum(len(play.get('handlers', [])) for play in playbook)\n",
    "        \n",
    "        print(f\"📊 Playbook Statistics:\")\n",
    "        print(f\"   Total plays: {len(playbook)}\")\n",
    "        print(f\"   Total tasks: {total_tasks}\")\n",
    "        print(f\"   Total handlers: {total_handlers}\")\n",
    "        print(f\"   File size: {playbook_file.stat().st_size / 1024:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating deterministic playbook: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"⚠️ No deterministic findings to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving 1094 findings that need LLM processing...\n",
      "💾 Saved LLM needed findings to: ../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\n",
      "📈 LLM needed findings by severity: {'medium': 909, 'high': 52, 'low': 56, 'unknown': 77}\n",
      "\n",
      "🔄 Variable for next notebook:\n",
      "   LLM_NEEDED_FILE = '../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json'\n"
     ]
    }
   ],
   "source": [
    "# Save findings that need LLM processing\n",
    "if llm_needed_findings:\n",
    "    print(f\"💾 Saving {len(llm_needed_findings)} findings that need LLM processing...\")\n",
    "    \n",
    "    # Create LLM needed findings file\n",
    "    llm_needed_file = llm_needed_dir / f\"llm_needed_findings_{RUN_TIMESTAMP}.json\"\n",
    "    \n",
    "    llm_needed_data = {\n",
    "        'metadata': {\n",
    "            'total_findings': len(llm_needed_findings),\n",
    "            'created_date': datetime.now().isoformat(),\n",
    "            'source': 'deterministic_processing_step',\n",
    "            'run_timestamp': RUN_TIMESTAMP,\n",
    "            'description': 'Findings that could not be processed deterministically and require LLM classification/processing'\n",
    "        },\n",
    "        'findings': llm_needed_findings\n",
    "    }\n",
    "    \n",
    "    with open(llm_needed_file, 'w') as f:\n",
    "        json.dump(llm_needed_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved LLM needed findings to: {llm_needed_file}\")\n",
    "    \n",
    "    # Analyze LLM needed findings by severity\n",
    "    llm_severity_counts = {}\n",
    "    for finding in llm_needed_findings:\n",
    "        severity = finding.get('severity', 'unknown')\n",
    "        llm_severity_counts[severity] = llm_severity_counts.get(severity, 0) + 1\n",
    "    \n",
    "    print(f\"📈 LLM needed findings by severity: {llm_severity_counts}\")\n",
    "    \n",
    "    # Store variable for next notebook\n",
    "    LLM_NEEDED_FILE = str(llm_needed_file)\n",
    "    print(f\"\\n🔄 Variable for next notebook:\")\n",
    "    print(f\"   LLM_NEEDED_FILE = '{LLM_NEEDED_FILE}'\")\n",
    "else:\n",
    "    print(\"✅ All findings processed deterministically - no LLM processing needed!\")\n",
    "    LLM_NEEDED_FILE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved processing summary to: ../playbooks/20250714_120000/processing_summary_step2_20250714_120000.json\n"
     ]
    }
   ],
   "source": [
    "# Create processing summary\n",
    "processing_summary = {\n",
    "    'run_timestamp': RUN_TIMESTAMP,\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'input_files': {\n",
    "        'enhanced_findings': ENHANCED_FINDINGS_FILE,\n",
    "        'ansible_targets': ANSIBLE_TARGETS_FILE\n",
    "    },\n",
    "    'statistics': {\n",
    "        'total_findings': len(enhanced_findings),\n",
    "        'deterministic_processed': len(deterministic_findings),\n",
    "        'llm_processing_needed': len(llm_needed_findings),\n",
    "        'deterministic_percentage': len(deterministic_findings)/len(enhanced_findings)*100 if enhanced_findings else 0\n",
    "    },\n",
    "    'output_files': {\n",
    "        'deterministic_playbook': str(playbook_file) if deterministic_findings else None,\n",
    "        'deterministic_targets': str(deterministic_targets_file) if deterministic_findings else None,\n",
    "        'llm_needed_findings': LLM_NEEDED_FILE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save processing summary\n",
    "summary_file = playbooks_dir / f\"processing_summary_step2_{RUN_TIMESTAMP}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(processing_summary, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved processing summary to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 DETERMINISTIC PROCESSING SUMMARY\n",
      "==================================================\n",
      "Run timestamp: 20250714_120000\n",
      "Total findings processed: 1529\n",
      "Deterministic playbooks generated: 435\n",
      "LLM processing needed: 1094\n",
      "Success rate: 28.4% deterministic\n",
      "\n",
      "✅ Deterministic Processing Results:\n",
      "   📄 Playbook: ../playbooks/20250714_120000/deterministic/deterministic_remediation_20250714_120000.yml\n",
      "   🎯 Targets: ../playbooks/20250714_120000/deterministic/deterministic_targets.json\n",
      "   📊 Tasks generated: 443\n",
      "\n",
      "🤖 LLM Processing Needed:\n",
      "   📄 Findings file: ../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\n",
      "   📊 Count: 1094\n",
      "   📈 By severity: {'medium': 909, 'high': 52, 'low': 56, 'unknown': 77}\n",
      "\n",
      "🔄 Ready for Step 3: Batch process LLM findings\n",
      "📝 Use these variables in the next notebook (03_process_llm_batch.ipynb):\n",
      "   RUN_TIMESTAMP = '20250714_120000'\n",
      "   LLM_NEEDED_FILE = '../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json'\n",
      "\n",
      "📁 All outputs saved to: ../playbooks/20250714_120000\n",
      "📋 Processing summary: ../playbooks/20250714_120000/processing_summary_step2_20250714_120000.json\n"
     ]
    }
   ],
   "source": [
    "# Final summary and next steps\n",
    "print(\"🎯 DETERMINISTIC PROCESSING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"Total findings processed: {len(enhanced_findings)}\")\n",
    "print(f\"Deterministic playbooks generated: {len(deterministic_findings)}\")\n",
    "print(f\"LLM processing needed: {len(llm_needed_findings)}\")\n",
    "print(f\"Success rate: {len(deterministic_findings)/len(enhanced_findings)*100:.1f}% deterministic\")\n",
    "\n",
    "if deterministic_findings:\n",
    "    print(f\"\\n✅ Deterministic Processing Results:\")\n",
    "    print(f\"   📄 Playbook: {playbook_file}\")\n",
    "    print(f\"   🎯 Targets: {deterministic_targets_file}\")\n",
    "    print(f\"   📊 Tasks generated: {total_tasks if 'total_tasks' in locals() else 'Unknown'}\")\n",
    "\n",
    "if llm_needed_findings:\n",
    "    print(f\"\\n🤖 LLM Processing Needed:\")\n",
    "    print(f\"   📄 Findings file: {LLM_NEEDED_FILE}\")\n",
    "    print(f\"   📊 Count: {len(llm_needed_findings)}\")\n",
    "    print(f\"   📈 By severity: {llm_severity_counts if 'llm_severity_counts' in locals() else 'Unknown'}\")\n",
    "    print(f\"\\n🔄 Ready for Step 3: Batch process LLM findings\")\n",
    "    print(f\"📝 Use these variables in the next notebook (03_process_llm_batch.ipynb):\")\n",
    "    print(f\"   RUN_TIMESTAMP = '{RUN_TIMESTAMP}'\")\n",
    "    print(f\"   LLM_NEEDED_FILE = '{LLM_NEEDED_FILE}'\")\n",
    "else:\n",
    "    print(f\"\\n✅ All findings processed deterministically!\")\n",
    "    print(f\"📝 No LLM processing needed - workflow complete\")\n",
    "\n",
    "print(f\"\\n📁 All outputs saved to: {playbooks_dir}\")\n",
    "print(f\"📋 Processing summary: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
