{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Extract STIG Findings from SCAP XML\n",
    "\n",
    "This notebook extracts comprehensive STIG findings from SCAP XML files using the enhanced parser.\n",
    "\n",
    "**Input:** SCAP XML file (e.g., `xml_files/sample_data/node2.example.com-STIG-20250710162433.xml`)\n",
    "\n",
    "**Output:** \n",
    "- Enhanced findings JSON file\n",
    "- Ansible targets JSON file\n",
    "- Processing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Libraries imported successfully\n",
      "üêç Python version: 3.11.12\n",
      "üìÅ Current working directory: /Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks\n",
      "üìÇ Source directory: /Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/src\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path - use absolute path resolution\n",
    "notebook_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "src_dir = notebook_dir.parent / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import our enhanced STIG parser\n",
    "from stig_parser_complete_info_extraction import EnhancedSTIGParser  # type: ignore\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully\")\n",
    "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
    "print(f\"üìÅ Current working directory: {os.getcwd()}\")\n",
    "print(f\"üìÇ Source directory: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Run timestamp: 20250714_110147\n",
      "‚úÖ SCAP XML file found: ../../xml_files/sample_data/node2.example.com-STIG-20250710162433.xml\n",
      "üìè File size: 31.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths as needed\n",
    "SCAP_XML_FILE = \"../../xml_files/sample_data/node2.example.com-STIG-20250710162433.xml\"\n",
    "OUTPUT_DIR = \"../findings\"\n",
    "\n",
    "# Create timestamp for this run\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(f\"üïê Run timestamp: {RUN_TIMESTAMP}\")\n",
    "\n",
    "# Verify input file exists\n",
    "scap_file_path = Path(SCAP_XML_FILE)\n",
    "if not scap_file_path.exists():\n",
    "    print(f\"‚ùå SCAP XML file not found: {scap_file_path}\")\n",
    "    print(\"Please update SCAP_XML_FILE path in the cell above\")\n",
    "else:\n",
    "    print(f\"‚úÖ SCAP XML file found: {scap_file_path}\")\n",
    "    print(f\"üìè File size: {scap_file_path.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Enhanced STIG Parser...\n",
      "üîç Parsing SCAP XML file: ../../xml_files/sample_data/node2.example.com-STIG-20250710162433.xml\n",
      "üîç Parsing enhanced STIG file: ../../xml_files/sample_data/node2.example.com-STIG-20250710162433.xml\n",
      "üìã Detected namespaces: ['xccdf', 'arf', 'ds', 'oval', 'cpe']\n",
      "üìÑ Document metadata: ARF\n",
      "üîÑ Phase 1: Extracting rule definitions...\n",
      "üîç Processed 1529 rule elements, extracted 1529 definitions\n",
      "üìö Found 1529 rule definitions\n",
      "üîÑ Phase 2: Extracting test results and merging...\n",
      "üéØ Processing TestResult: xccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_stig\n",
      "üîç Processed 1 TestResult elements\n",
      "‚úÖ Created 1529 enhanced findings\n",
      "\n",
      "üìä Parsing Results:\n",
      "   Total findings extracted: 1529\n",
      "\n",
      "üìà Summary Statistics:\n",
      "   Failed findings: 1529\n",
      "   Actionable findings: 435\n",
      "   Critical severity: 0\n",
      "   High severity: 69\n",
      "\n",
      "üîç By Severity: {'medium': 1221, 'high': 69, 'low': 119, 'unknown': 120}\n",
      "üéØ By Target Type: {'unknown': 1094, 'package': 118, 'service': 78, 'mount': 50, 'file_ownership': 72, 'file_permission': 53, 'sysctl': 64}\n",
      "üìã By Status: {'unknown': 1529}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced STIG parser\n",
    "print(\"üöÄ Initializing Enhanced STIG Parser...\")\n",
    "parser = EnhancedSTIGParser()\n",
    "\n",
    "# Parse the SCAP XML file\n",
    "print(f\"üîç Parsing SCAP XML file: {SCAP_XML_FILE}\")\n",
    "findings = parser.parse_stig_file(SCAP_XML_FILE)\n",
    "\n",
    "print(f\"\\nüìä Parsing Results:\")\n",
    "print(f\"   Total findings extracted: {len(findings)}\")\n",
    "\n",
    "if findings:\n",
    "    # Get summary statistics\n",
    "    summary = parser.get_findings_summary()\n",
    "    print(f\"\\nüìà Summary Statistics:\")\n",
    "    print(f\"   Failed findings: {summary['failed_count']}\")\n",
    "    print(f\"   Actionable findings: {summary['actionable_count']}\")\n",
    "    print(f\"   Critical severity: {summary['critical_count']}\")\n",
    "    print(f\"   High severity: {summary['high_count']}\")\n",
    "    \n",
    "    print(f\"\\nüîç By Severity: {summary['by_severity']}\")\n",
    "    print(f\"üéØ By Target Type: {summary['by_target_type']}\")\n",
    "    print(f\"üìã By Status: {summary['by_status']}\")\n",
    "else:\n",
    "    print(\"‚ùå No findings extracted. Check the XML file format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sample Findings (first 3):\n",
      "\n",
      "üìã Finding 1:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "   Severity: medium\n",
      "   Status: unknown\n",
      "   Title: Prefer to use a 64-bit Operating System when supported...\n",
      "   Target Type: unknown\n",
      "   Target Name: prefer_64bit_os\n",
      "   Ansible Module: debug\n",
      "   Compliance: CCI=0, NIST=1\n",
      "\n",
      "üìã Finding 2:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_package_prelink_removed\n",
      "   Severity: medium\n",
      "   Status: unknown\n",
      "   Title: Package \"prelink\" Must not be Installed...\n",
      "   Target Type: package\n",
      "   Target Name: prelink\n",
      "   Ansible Module: yum\n",
      "   Compliance: CCI=0, NIST=1\n",
      "\n",
      "üìã Finding 3:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_disable_prelink\n",
      "   Severity: medium\n",
      "   Status: unknown\n",
      "   Title: Disable Prelinking...\n",
      "   Target Type: unknown\n",
      "   Target Name: disable_prelink\n",
      "   Ansible Module: debug\n",
      "   Compliance: CCI=0, NIST=1\n"
     ]
    }
   ],
   "source": [
    "# Show sample findings for inspection\n",
    "if findings:\n",
    "    print(\"üîç Sample Findings (first 3):\")\n",
    "    for i, finding in enumerate(findings[:3]):\n",
    "        print(f\"\\nüìã Finding {i+1}:\")\n",
    "        print(f\"   Rule ID: {finding.rule_id}\")\n",
    "        print(f\"   Severity: {finding.severity}\")\n",
    "        print(f\"   Status: {finding.status}\")\n",
    "        print(f\"   Title: {finding.title[:60]}...\")\n",
    "        \n",
    "        if finding.target_info:\n",
    "            print(f\"   Target Type: {finding.target_info.target_type}\")\n",
    "            print(f\"   Target Name: {finding.target_info.target_name}\")\n",
    "            print(f\"   Ansible Module: {finding.target_info.ansible_module}\")\n",
    "        else:\n",
    "            print(f\"   Target Info: None (manual review required)\")\n",
    "        \n",
    "        print(f\"   Compliance: CCI={len(finding.compliance.cci_refs)}, NIST={len(finding.compliance.nist_refs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting enhanced findings to: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json\n",
      "üíæ Exported 1529 enhanced findings to ../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json\n",
      "üíæ Exporting ansible targets to: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json\n",
      "üéØ Exported 435 Ansible targets to ../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json\n",
      "\n",
      "‚úÖ Export completed successfully!\n",
      "üìÅ Output files:\n",
      "   Enhanced findings: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json\n",
      "   Ansible targets: ../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json\n",
      "\n",
      "üîÑ Variables for next notebook:\n",
      "   ENHANCED_FINDINGS_FILE = '../findings/node2.example.com-STIG-20250710162433_20250714_110147_enhanced_findings.json'\n",
      "   ANSIBLE_TARGETS_FILE = '../findings/node2.example.com-STIG-20250710162433_20250714_110147_ansible_targets.json'\n",
      "   RUN_TIMESTAMP = '20250714_110147'\n"
     ]
    }
   ],
   "source": [
    "# Export findings to JSON files\n",
    "if findings:\n",
    "    # Create output directory\n",
    "    output_dir = Path(OUTPUT_DIR)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate output filenames with timestamp\n",
    "    base_name = scap_file_path.stem\n",
    "    findings_file = output_dir / f\"{base_name}_{RUN_TIMESTAMP}_enhanced_findings.json\"\n",
    "    targets_file = output_dir / f\"{base_name}_{RUN_TIMESTAMP}_ansible_targets.json\"\n",
    "    \n",
    "    # Export enhanced findings\n",
    "    print(f\"üíæ Exporting enhanced findings to: {findings_file}\")\n",
    "    parser.export_findings_json(str(findings_file))\n",
    "    \n",
    "    # Export ansible targets\n",
    "    print(f\"üíæ Exporting ansible targets to: {targets_file}\")\n",
    "    parser.export_ansible_targets(str(targets_file))\n",
    "    \n",
    "    print(f\"\\n‚úÖ Export completed successfully!\")\n",
    "    print(f\"üìÅ Output files:\")\n",
    "    print(f\"   Enhanced findings: {findings_file}\")\n",
    "    print(f\"   Ansible targets: {targets_file}\")\n",
    "    \n",
    "    # Store variables for next notebook\n",
    "    ENHANCED_FINDINGS_FILE = str(findings_file)\n",
    "    ANSIBLE_TARGETS_FILE = str(targets_file)\n",
    "    \n",
    "    print(f\"\\nüîÑ Variables for next notebook:\")\n",
    "    print(f\"   ENHANCED_FINDINGS_FILE = '{ENHANCED_FINDINGS_FILE}'\")\n",
    "    print(f\"   ANSIBLE_TARGETS_FILE = '{ANSIBLE_TARGETS_FILE}'\")\n",
    "    print(f\"   RUN_TIMESTAMP = '{RUN_TIMESTAMP}'\")\n",
    "else:\n",
    "    print(\"‚ùå No findings to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ EXTRACTION SUMMARY\n",
      "==================================================\n",
      "Total findings extracted: 1529\n",
      "Actionable with targets: 435\n",
      "Failed findings: 1529\n",
      "Manual review needed: 1094\n",
      "\n",
      "‚úÖ Ready for Step 2: Process 435 actionable findings\n",
      "üìù Use the variables above in the next notebook (02_process_deterministic.ipynb)\n",
      "\n",
      "üìä Processing Strategy:\n",
      "   Deterministic targets: 435 findings\n",
      "   LLM classification needed: 1529 findings\n",
      "   Manual review: -435 findings\n"
     ]
    }
   ],
   "source": [
    "# Final summary and next steps\n",
    "if findings:\n",
    "    summary = parser.get_findings_summary()\n",
    "    actionable_count = summary['actionable_count']\n",
    "    total_count = summary['total_findings']\n",
    "    failed_count = summary['failed_count']\n",
    "    \n",
    "    print(\"üéØ EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total findings extracted: {total_count}\")\n",
    "    print(f\"Actionable with targets: {actionable_count}\")\n",
    "    print(f\"Failed findings: {failed_count}\")\n",
    "    print(f\"Manual review needed: {total_count - actionable_count}\")\n",
    "    \n",
    "    if actionable_count > 0:\n",
    "        print(f\"\\n‚úÖ Ready for Step 2: Process {actionable_count} actionable findings\")\n",
    "        print(f\"üìù Use the variables above in the next notebook (02_process_deterministic.ipynb)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No actionable findings found\")\n",
    "        print(f\"üìù All {total_count} findings require manual review\")\n",
    "        \n",
    "    print(f\"\\nüìä Processing Strategy:\")\n",
    "    print(f\"   Deterministic targets: {actionable_count} findings\")\n",
    "    print(f\"   LLM classification needed: {failed_count} findings\")\n",
    "    print(f\"   Manual review: {total_count - actionable_count - failed_count} findings\")\n",
    "else:\n",
    "    print(\"‚ùå Extraction failed - check the SCAP XML file format\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
