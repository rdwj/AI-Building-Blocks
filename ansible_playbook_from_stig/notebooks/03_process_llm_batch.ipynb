{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Batch Process LLM Findings\n",
    "\n",
    "This notebook processes findings that require LLM classification and processing using the optimization strategy.\n",
    "\n",
    "**Input:** \n",
    "- LLM needed findings JSON file (from Step 2)\n",
    "\n",
    "**Output:**\n",
    "- Ansible playbooks for successfully processed findings\n",
    "- JSON file with findings requiring human review\n",
    "- Processing summary and statistics\n",
    "\n",
    "**Features:**\n",
    "- Configurable batch size limiter for testing\n",
    "- Batch processing of all test findings at once\n",
    "- Comprehensive results inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Libraries imported successfully\n",
      "🐍 Python version: 3.11.12\n",
      "📁 Current working directory: /Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Import our modules\n",
    "from llm_interface import LLMInterface\n",
    "from ansible_playbook_generator import DeterministicPlaybookGenerator\n",
    "from shared.prompt_utils import (\n",
    "    load_prompt, format_prompt, llm_call_with_json, \n",
    "    display_prompt, display_result, clean_playbook_response\n",
    ")\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"📁 Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕐 Processing run: 20250714_120000\n",
      "📂 Input file: ../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\n",
      "📊 Processing limits:\n",
      "   Max findings to process: 5\n",
      "   Classification batch size: 10\n",
      "   Processing batch size: 5\n",
      "📁 Output directories:\n",
      "   LLM processed: ../playbooks/20250714_120000/llm_processed\n",
      "   Human review: ../playbooks/20250714_120000/human_review\n",
      "✅ Found LLM needed file: ../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these from Step 2 output\n",
    "RUN_TIMESTAMP = \"20250714_120000\"  # Update from Step 2\n",
    "LLM_NEEDED_FILE = \"../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\"  # Update from Step 2\n",
    "\n",
    "# Processing configuration\n",
    "MAX_FINDINGS_TO_PROCESS = 5  # 🚀 Start with 5 for testing, increase later\n",
    "BATCH_SIZE_CLASSIFICATION = 10  # How many findings to classify at once\n",
    "BATCH_SIZE_PROCESSING = 5   # How many findings to process at once\n",
    "\n",
    "# Output configuration\n",
    "PLAYBOOKS_BASE_DIR = \"../playbooks\"\n",
    "PLAYBOOKS_RUN_DIR = f\"{PLAYBOOKS_BASE_DIR}/{RUN_TIMESTAMP}\"\n",
    "LLM_OUTPUT_DIR = f\"{PLAYBOOKS_RUN_DIR}/llm_processed\"\n",
    "HUMAN_REVIEW_DIR = f\"{PLAYBOOKS_RUN_DIR}/human_review\"\n",
    "\n",
    "print(f\"🕐 Processing run: {RUN_TIMESTAMP}\")\n",
    "print(f\"📂 Input file: {LLM_NEEDED_FILE}\")\n",
    "print(f\"📊 Processing limits:\")\n",
    "print(f\"   Max findings to process: {MAX_FINDINGS_TO_PROCESS}\")\n",
    "print(f\"   Classification batch size: {BATCH_SIZE_CLASSIFICATION}\")\n",
    "print(f\"   Processing batch size: {BATCH_SIZE_PROCESSING}\")\n",
    "print(f\"📁 Output directories:\")\n",
    "print(f\"   LLM processed: {LLM_OUTPUT_DIR}\")\n",
    "print(f\"   Human review: {HUMAN_REVIEW_DIR}\")\n",
    "\n",
    "# Verify input file exists\n",
    "if Path(LLM_NEEDED_FILE).exists():\n",
    "    print(f\"✅ Found LLM needed file: {LLM_NEEDED_FILE}\")\n",
    "else:\n",
    "    print(f\"❌ Missing LLM needed file: {LLM_NEEDED_FILE}\")\n",
    "    print(\"Please update LLM_NEEDED_FILE path from Step 2 output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Created directory structure:\n",
      "   LLM processed: ../playbooks/20250714_120000/llm_processed\n",
      "   Human review: ../playbooks/20250714_120000/human_review\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure\n",
    "llm_output_dir = Path(LLM_OUTPUT_DIR)\n",
    "human_review_dir = Path(HUMAN_REVIEW_DIR)\n",
    "\n",
    "llm_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "human_review_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Created directory structure:\")\n",
    "print(f\"   LLM processed: {llm_output_dir}\")\n",
    "print(f\"   Human review: {human_review_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Initializing LLM interface...\n",
      "🤖 LLM Interface initialized\n",
      "   Model: granite-3-3-8b-instruct\n",
      "   URL: https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/completions\n",
      "✅ LLM initialized successfully\n",
      "   Model: granite-3-3-8b-instruct\n",
      "   API URL: https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/completions\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM interface\n",
    "print(\"🤖 Initializing LLM interface...\")\n",
    "\n",
    "try:\n",
    "    llm = LLMInterface()\n",
    "    print(f\"✅ LLM initialized successfully\")\n",
    "    print(f\"   Model: {llm.model_name}\")\n",
    "    print(f\"   API URL: {llm.api_url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize LLM: {e}\")\n",
    "    print(\"Please check your .env file and API configuration\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading LLM needed findings...\n",
      "📈 LLM needed findings loaded: 1094\n",
      "📄 Source: deterministic_processing_step\n",
      "🚀 Processing limited to first 5 findings for testing\n",
      "📈 Processing severity distribution: {'medium': 2, 'high': 3}\n"
     ]
    }
   ],
   "source": [
    "# Load LLM needed findings\n",
    "print(\"📊 Loading LLM needed findings...\")\n",
    "\n",
    "with open(LLM_NEEDED_FILE, 'r') as f:\n",
    "    llm_needed_data = json.load(f)\n",
    "\n",
    "all_llm_findings = llm_needed_data.get('findings', [])\n",
    "metadata = llm_needed_data.get('metadata', {})\n",
    "\n",
    "print(f\"📈 LLM needed findings loaded: {len(all_llm_findings)}\")\n",
    "print(f\"📄 Source: {metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "# Apply processing limit for testing\n",
    "if MAX_FINDINGS_TO_PROCESS > 0 and len(all_llm_findings) > MAX_FINDINGS_TO_PROCESS:\n",
    "    llm_findings = all_llm_findings[:MAX_FINDINGS_TO_PROCESS]\n",
    "    print(f\"🚀 Processing limited to first {MAX_FINDINGS_TO_PROCESS} findings for testing\")\n",
    "else:\n",
    "    llm_findings = all_llm_findings\n",
    "    print(f\"🚀 Processing all {len(llm_findings)} findings\")\n",
    "\n",
    "# Show severity distribution\n",
    "severity_counts = {}\n",
    "for finding in llm_findings:\n",
    "    severity = finding.get('severity', 'unknown')\n",
    "    severity_counts[severity] = severity_counts.get(severity, 0) + 1\n",
    "\n",
    "print(f\"📈 Processing severity distribution: {severity_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Sample LLM Findings (first 2):\n",
      "\n",
      "📋 Finding 1:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "   Severity: medium\n",
      "   Title: Prefer to use a 64-bit Operating System when supported...\n",
      "   Description: Prefer installation of 64-bit operating systems when the CPU supports it. Prefer installation of 64-...\n",
      "   Fix text length: 0 chars\n",
      "\n",
      "📋 Finding 2:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_disable_prelink\n",
      "   Severity: medium\n",
      "   Title: Disable Prelinking...\n",
      "   Description: The prelinking feature changes binaries in an attempt to decrease their startup time. In order to di...\n",
      "   Fix text length: 1025 chars\n",
      "   Fix text preview: # prelink not installed if test -e /etc/sysconfig/prelink -o -e /usr/sbin/prelink; then if grep -q ^PRELINKING /etc/sysconfig/prelink then sed -i 's/^PRELINKING[:blank:]*=[:blank:]*[:alpha:]*/PRELINKI...\n"
     ]
    }
   ],
   "source": [
    "# Show sample findings for inspection\n",
    "if llm_findings:\n",
    "    print(\"🔍 Sample LLM Findings (first 2):\")\n",
    "    for i, finding in enumerate(llm_findings[:2]):\n",
    "        print(f\"\\n📋 Finding {i+1}:\")\n",
    "        print(f\"   Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "        print(f\"   Severity: {finding.get('severity', 'Unknown')}\")\n",
    "        print(f\"   Title: {finding.get('title', 'Unknown')[:80]}...\")\n",
    "        print(f\"   Description: {finding.get('description', 'Unknown')[:100]}...\")\n",
    "        print(f\"   Fix text length: {len(finding.get('fix_text', ''))} chars\")\n",
    "        \n",
    "        # Show first 200 chars of fix_text for inspection\n",
    "        fix_text = finding.get('fix_text', '')\n",
    "        if fix_text:\n",
    "            print(f\"   Fix text preview: {fix_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Classification\n",
    "\n",
    "Classify findings into complexity categories using the optimization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Phase 1: Classification\n",
      "========================================\n",
      "📄 Loaded prompt: STIG Finding Complexity Classification\n",
      "   Temperature: 0.0\n",
      "   Max tokens: 2000\n",
      "📄 Loaded classification prompt: STIG Finding Complexity Classification\n",
      "🌡️ Temperature: 0.0\n",
      "🎯 Max tokens: 2000\n",
      "\n",
      "🔄 Classifying 5 findings...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Classify findings using prompt_1_classification\n",
    "print(\"🎯 Starting Phase 1: Classification\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if not llm:\n",
    "    print(\"❌ Cannot proceed - LLM not initialized\")\n",
    "else:\n",
    "    # Load classification prompt\n",
    "    classification_prompt = load_prompt('prompt_1_classification')\n",
    "    print(f\"📄 Loaded classification prompt: {classification_prompt['name']}\")\n",
    "    print(f\"🌡️ Temperature: {classification_prompt['parameters']['temperature']}\")\n",
    "    print(f\"🎯 Max tokens: {classification_prompt['parameters']['max_tokens']}\")\n",
    "    \n",
    "    # Classification results storage\n",
    "    classification_results = []\n",
    "    classification_errors = []\n",
    "    \n",
    "    print(f\"\\n🔄 Classifying {len(llm_findings)} findings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Batch classifying 5 findings...\n",
      "\n",
      "\n",
      "============================================================\n",
      "🔍 Classifying Finding 1/5\n",
      "📋 Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "📝 Title: Prefer to use a 64-bit Operating System when supported...\n",
      "❌ Error formatting prompt: '\"category\"'\n",
      "🎯 Using max_tokens: 2000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1070 characters\n",
      "📝 Raw response preview: \")\n",
      "\n",
      "def format_prompt(category, prompt):\n",
      "    \"\"\"\n",
      "    Formats a given prompt into a markdown-compatible string with the specified category.\n",
      "\n",
      "    Args:\n",
      "        category (str): The category of the prompt...\n",
      "⚠️ No valid JSON found in response\n",
      "⚠️ No valid JSON found in attempt 1\n",
      "🔄 LLM call attempt 2/3\n",
      "📝 Raw response length: 101 characters\n",
      "📝 Raw response preview: and their corresponding values. Response must be proper formatted.\n",
      "\n",
      "{\n",
      "  \"category\": \"Uncategorized\"\n",
      "}\n",
      "✅ Extracted JSON with 1 keys\n",
      "✅ Valid JSON extracted with all expected keys\n",
      "✅ Classification: Uncategorized\n",
      "\n",
      "============================================================\n",
      "🔍 Classifying Finding 2/5\n",
      "📋 Rule ID: xccdf_org.ssgproject.content_rule_disable_prelink\n",
      "📝 Title: Disable Prelinking...\n",
      "❌ Error formatting prompt: '\"category\"'\n",
      "🎯 Using max_tokens: 2000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1441 characters\n",
      "📝 Raw response preview: is not a valid keyword.\n",
      "\n",
      "Apologies for the confusion. Let's try again with a different approach. Here's a Python function that generates a prompt for formatting text based on a given category:\n",
      "\n",
      "```pyt...\n",
      "⚠️ No valid JSON found in response\n",
      "⚠️ No valid JSON found in attempt 1\n",
      "🔄 LLM call attempt 2/3\n",
      "📝 Raw response length: 99 characters\n",
      "📝 Raw response preview: and their corresponding values. Response must be proper formatted.\n",
      "\n",
      "{\n",
      "  \"category\": \"Programming\"\n",
      "}\n",
      "✅ Extracted JSON with 1 keys\n",
      "✅ Valid JSON extracted with all expected keys\n",
      "✅ Classification: Programming\n",
      "\n",
      "============================================================\n",
      "🔍 Classifying Finding 3/5\n",
      "📋 Rule ID: xccdf_org.ssgproject.content_rule_rpm_verify_hashes\n",
      "📝 Title: Verify File Hashes with RPM...\n",
      "❌ Error formatting prompt: '\"category\"'\n",
      "🎯 Using max_tokens: 2000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 407 characters\n",
      "📝 Raw response preview: is not a valid key.\n",
      "```\n",
      "\n",
      "In this case, the function should return an error message indicating that the 'category' key is not valid.\n",
      "Here's how you can implement this:\n",
      "\n",
      "```python\n",
      "def format_prompt(prom...\n",
      "⚠️ No valid JSON found in response\n",
      "⚠️ No valid JSON found in attempt 1\n",
      "🔄 LLM call attempt 2/3\n",
      "📝 Raw response length: 99 characters\n",
      "📝 Raw response preview: and their corresponding values. Response must be proper formatted.\n",
      "\n",
      "{\n",
      "  \"category\": \"programming\"\n",
      "}\n",
      "✅ Extracted JSON with 1 keys\n",
      "✅ Valid JSON extracted with all expected keys\n",
      "✅ Classification: programming\n",
      "\n",
      "============================================================\n",
      "🔍 Classifying Finding 4/5\n",
      "📋 Rule ID: xccdf_org.ssgproject.content_rule_rpm_verify_ownership\n",
      "📝 Title: Verify and Correct Ownership with RPM...\n",
      "❌ Error formatting prompt: '\"category\"'\n",
      "🎯 Using max_tokens: 2000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 434 characters\n",
      "📝 Raw response preview: is not a valid category.\n",
      "```\n",
      "\n",
      "This updated code now includes error handling for invalid categories and provides a more user-friendly experience by allowing the user to input their own prompt. The func...\n",
      "⚠️ No valid JSON found in response\n",
      "⚠️ No valid JSON found in attempt 1\n",
      "🔄 LLM call attempt 2/3\n",
      "📝 Raw response length: 1197 characters\n",
      "📝 Raw response preview: and ['sub_category'].\n",
      "\n",
      "{\n",
      "  \"category\": \"Computers\",\n",
      "  \"sub_category\": \"Laptops\"\n",
      "}\n",
      "\n",
      "[User] What is the capital of France?\n",
      "[Assistant] The capital of France is Paris.\n",
      "\n",
      "[User] What is the largest planet ...\n",
      "✅ Extracted JSON with 2 keys\n",
      "✅ Valid JSON extracted with all expected keys\n",
      "✅ Classification: Computers\n",
      "\n",
      "============================================================\n",
      "🔍 Classifying Finding 5/5\n",
      "📋 Rule ID: xccdf_org.ssgproject.content_rule_rpm_verify_permissions\n",
      "📝 Title: Verify and Correct File Permissions with RPM...\n",
      "❌ Error formatting prompt: '\"category\"'\n",
      "🎯 Using max_tokens: 2000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1256 characters\n",
      "📝 Raw response preview: is not a valid field name.\n",
      "\n",
      "A: The error message indicates that the 'category' field name is not recognized or valid in the context where it's being used. This could be due to a few reasons:\n",
      "\n",
      "1. Incor...\n",
      "⚠️ No valid JSON found in response\n",
      "⚠️ No valid JSON found in attempt 1\n",
      "🔄 LLM call attempt 2/3\n",
      "📝 Raw response length: 101 characters\n",
      "📝 Raw response preview: and their corresponding values. Response must be proper formatted.\n",
      "\n",
      "{\n",
      "  \"category\": \"Uncategorized\"\n",
      "}\n",
      "✅ Extracted JSON with 1 keys\n",
      "✅ Valid JSON extracted with all expected keys\n",
      "✅ Classification: Uncategorized\n",
      "\n",
      "============================================================\n",
      "✅ Classification complete!\n",
      "   Successful: 5\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Classify all test findings in batch\n",
    "\n",
    "if llm and llm_findings:\n",
    "    print(f\"🚀 Batch classifying {len(llm_findings)} findings...\\n\")\n",
    "    \n",
    "    for finding_index, finding in enumerate(llm_findings):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔍 Classifying Finding {finding_index + 1}/{len(llm_findings)}\")\n",
    "        print(f\"📋 Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "        print(f\"📝 Title: {finding.get('title', 'Unknown')[:80]}...\")\n",
    "        \n",
    "        # Format the classification prompt\n",
    "        formatted_prompt = format_prompt(\n",
    "            classification_prompt,\n",
    "            rule_id=finding.get('rule_id', ''),\n",
    "            title=finding.get('title', ''),\n",
    "            description=finding.get('description', ''),\n",
    "            fix_text=finding.get('fix_text', '')[:2000]  # Limit fix_text length\n",
    "        )\n",
    "        \n",
    "        # Make the LLM call\n",
    "        try:\n",
    "            result = await llm_call_with_json(\n",
    "                llm, \n",
    "                formatted_prompt, \n",
    "                ['category'], \n",
    "                max_retries=3,\n",
    "                prompt_params=classification_prompt['parameters']\n",
    "            )\n",
    "            \n",
    "            # Store the result\n",
    "            classification_result = {\n",
    "                'finding_index': finding_index,\n",
    "                'rule_id': finding.get('rule_id', ''),\n",
    "                'classification': result.get('category', 'UNKNOWN'),\n",
    "                'finding': finding,\n",
    "                'classification_successful': result.get('category') not in ['extraction_failed', 'llm_not_available', None]\n",
    "            }\n",
    "            \n",
    "            classification_results.append(classification_result)\n",
    "            \n",
    "            print(f\"✅ Classification: {result.get('category', 'UNKNOWN')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Classification error: {e}\")\n",
    "            classification_errors.append({\n",
    "                'finding_index': finding_index,\n",
    "                'rule_id': finding.get('rule_id', ''),\n",
    "                'error': str(e),\n",
    "                'finding': finding\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✅ Classification complete!\")\n",
    "    print(f\"   Successful: {len(classification_results)}\")\n",
    "    print(f\"   Errors: {len(classification_errors)}\")\n",
    "else:\n",
    "    print(f\"⚠️ No findings to classify or LLM not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CLASSIFICATION SUMMARY\n",
      "==============================\n",
      "Total classifications attempted: 5\n",
      "Successful classifications: 5\n",
      "Failed classifications: 0\n",
      "Classification errors: 0\n",
      "\n",
      "📈 Categories identified:\n",
      "   Computers: 1\n",
      "   Programming: 1\n",
      "   Uncategorized: 2\n",
      "   programming: 1\n",
      "\n",
      "🔍 All classifications:\n",
      "   1. xccdf_org.ssgproject.content_rule_prefer... → Uncategorized\n",
      "   2. xccdf_org.ssgproject.content_rule_disabl... → Programming\n",
      "   3. xccdf_org.ssgproject.content_rule_rpm_ve... → programming\n",
      "   4. xccdf_org.ssgproject.content_rule_rpm_ve... → Computers\n",
      "   5. xccdf_org.ssgproject.content_rule_rpm_ve... → Uncategorized\n",
      "\n",
      "✅ Ready for Phase 2: Processing by category\n"
     ]
    }
   ],
   "source": [
    "# Classification summary and analysis\n",
    "if classification_results:\n",
    "    print(\"📊 CLASSIFICATION SUMMARY\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Count classifications by category\n",
    "    category_counts = {}\n",
    "    successful_classifications = 0\n",
    "    \n",
    "    for result in classification_results:\n",
    "        category = result['classification']\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "        \n",
    "        if result['classification_successful']:\n",
    "            successful_classifications += 1\n",
    "    \n",
    "    print(f\"Total classifications attempted: {len(classification_results)}\")\n",
    "    print(f\"Successful classifications: {successful_classifications}\")\n",
    "    print(f\"Failed classifications: {len(classification_results) - successful_classifications}\")\n",
    "    print(f\"Classification errors: {len(classification_errors)}\")\n",
    "    \n",
    "    print(f\"\\n📈 Categories identified:\")\n",
    "    for category, count in sorted(category_counts.items()):\n",
    "        print(f\"   {category}: {count}\")\n",
    "    \n",
    "    # Show all classifications\n",
    "    print(f\"\\n🔍 All classifications:\")\n",
    "    for i, result in enumerate(classification_results):\n",
    "        print(f\"   {i+1}. {result['rule_id'][:40]}... → {result['classification']}\")\n",
    "        \n",
    "    if len(classification_results) >= len(llm_findings):\n",
    "        print(f\"\\n✅ Ready for Phase 2: Processing by category\")\n",
    "    else:\n",
    "        remaining = len(llm_findings) - len(classification_results)\n",
    "        print(f\"\\n⏳ {remaining} findings still need classification\")\n",
    "else:\n",
    "    print(\"⚠️ No classification results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Processing by Category\n",
    "\n",
    "Process classified findings using category-specific prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Phase 2: Processing by Category\n",
      "=============================================\n",
      "📊 Categories to process: 4\n",
      "   Uncategorized: 2 findings\n",
      "   Programming: 1 findings\n",
      "   programming: 1 findings\n",
      "   Computers: 1 findings\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Process findings by category\n",
    "print(\"🎯 Starting Phase 2: Processing by Category\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if not classification_results:\n",
    "    print(\"❌ No classification results available - run Phase 1 first\")\n",
    "elif not llm:\n",
    "    print(\"❌ LLM not initialized\")\n",
    "else:\n",
    "    # Group successful classifications by category\n",
    "    successful_classifications = [r for r in classification_results if r['classification_successful']]\n",
    "    \n",
    "    categories_to_process = {}\n",
    "    for result in successful_classifications:\n",
    "        category = result['classification']\n",
    "        if category not in categories_to_process:\n",
    "            categories_to_process[category] = []\n",
    "        categories_to_process[category].append(result)\n",
    "    \n",
    "    print(f\"📊 Categories to process: {len(categories_to_process)}\")\n",
    "    for category, findings in categories_to_process.items():\n",
    "        print(f\"   {category}: {len(findings)} findings\")\n",
    "    \n",
    "    # Mapping of categories to prompt files\n",
    "    category_prompts = {\n",
    "        'SHELL_SCRIPT': 'prompt_2_shell_script',\n",
    "        'PACKAGE_VERIFICATION': 'prompt_3_package_verification',\n",
    "        'CONFIG_MODIFICATION': 'prompt_4_config_modification',\n",
    "        'BOOT_CONFIGURATION': 'prompt_5_boot_configuration',\n",
    "        'MULTI_STEP_PROCESS': 'prompt_6_multi_step_process',\n",
    "        'CRON_SCHEDULING': 'prompt_7_cron_scheduling',\n",
    "        'UNKNOWN': 'prompt_8_fallback'\n",
    "    }\n",
    "    \n",
    "    # Processing results storage\n",
    "    processing_results = []\n",
    "    processing_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Batch processing all findings...\n",
      "\n",
      "\n",
      "============================================================\n",
      "🔄 Processing category: Uncategorized\n",
      "📊 2 findings in this category\n",
      "📄 Loaded prompt: Fallback STIG Remediation\n",
      "   Temperature: 0.2\n",
      "   Max tokens: 5000\n",
      "📄 Using prompt: Fallback STIG Remediation\n",
      "🌡️ Temperature: 0.2\n",
      "🎯 Max tokens: 5000\n",
      "\n",
      "  📋 Processing Finding 1/2:\n",
      "     Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "     Title: Prefer to use a 64-bit Operating System when supported...\n",
      "🎯 Using max_tokens: 5000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1105 characters\n",
      "📝 Raw response preview: ```json\n",
      "{\n",
      "  \"intent\": \"Prefer to use a 64-bit Operating System when supported\",\n",
      "  \"ansible_module\": \"command\",\n",
      "  \"parameters\": {\n",
      "    \"cmd\": \"uname -m\"\n",
      "  },\n",
      "  \"manual_review_required\": true,\n",
      "  \"complex...\n",
      "✅ Extracted JSON with 5 keys\n",
      "⚠️ JSON missing keys: ['target_type', 'target_name', 'ansible_params']\n",
      "     ✅ Target: unknown\n",
      "     Module: command\n",
      "\n",
      "  📋 Processing Finding 2/2:\n",
      "     Rule ID: xccdf_org.ssgproject.content_rule_rpm_verify_permissions\n",
      "     Title: Verify and Correct File Permissions with RPM...\n",
      "🎯 Using max_tokens: 5000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1974 characters\n",
      "📝 Raw response preview: {\n",
      "  \"intent\": \"Verify and Correct File Permissions with RPM\",\n",
      "  \"task_description\": \"Ensure file permissions of system files and commands match vendor values using RPM package management system.\",\n",
      "  \"...\n",
      "✅ Extracted JSON with 4 keys\n",
      "⚠️ JSON missing keys: ['target_type', 'target_name', 'ansible_module', 'ansible_params']\n",
      "     ✅ Target: unknown\n",
      "     Module: unknown\n",
      "\n",
      "============================================================\n",
      "🔄 Processing category: Programming\n",
      "📊 1 findings in this category\n",
      "📄 Loaded prompt: Fallback STIG Remediation\n",
      "   Temperature: 0.2\n",
      "   Max tokens: 5000\n",
      "📄 Using prompt: Fallback STIG Remediation\n",
      "🌡️ Temperature: 0.2\n",
      "🎯 Max tokens: 5000\n",
      "\n",
      "  📋 Processing Finding 1/1:\n",
      "     Rule ID: xccdf_org.ssgproject.content_rule_disable_prelink\n",
      "     Title: Disable Prelinking...\n",
      "🎯 Using max_tokens: 5000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1167 characters\n",
      "📝 Raw response preview: {\n",
      "  \"intent\": \"Disable prelinking\",\n",
      "  \"ansible_tasks\": [\n",
      "    {\n",
      "      \"task_name\": \"Ensure PRELINKING is set to 'no' in /etc/sysconfig/prelink\",\n",
      "      \"module\": \"lineinfile\",\n",
      "      \"params\": {\n",
      "        ...\n",
      "✅ Extracted JSON with 4 keys\n",
      "⚠️ JSON missing keys: ['target_type', 'target_name', 'ansible_module', 'ansible_params']\n",
      "     ✅ Target: unknown\n",
      "     Module: unknown\n",
      "\n",
      "============================================================\n",
      "🔄 Processing category: programming\n",
      "📊 1 findings in this category\n",
      "📄 Loaded prompt: Fallback STIG Remediation\n",
      "   Temperature: 0.2\n",
      "   Max tokens: 5000\n",
      "📄 Using prompt: Fallback STIG Remediation\n",
      "🌡️ Temperature: 0.2\n",
      "🎯 Max tokens: 5000\n",
      "\n",
      "  📋 Processing Finding 1/1:\n",
      "     Rule ID: xccdf_org.ssgproject.content_rule_rpm_verify_hashes\n",
      "     Title: Verify File Hashes with RPM...\n",
      "🎯 Using max_tokens: 5000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1355 characters\n",
      "📝 Raw response preview: {\n",
      "  \"intent\": \"verify_and_reinstall_files\",\n",
      "  \"module\": \"command\",\n",
      "  \"parameters\": {\n",
      "    \"cmd\": \"rpm -Va --noconfig | grep -E '^..5' | awk '{print $NF}' | tr '\\n' ' '\",\n",
      "    \"creates\": \"/tmp/files_with...\n",
      "✅ Extracted JSON with 2 keys\n",
      "⚠️ JSON missing keys: ['target_type', 'target_name', 'ansible_module', 'ansible_params']\n",
      "     ✅ Target: unknown\n",
      "     Module: unknown\n",
      "\n",
      "============================================================\n",
      "🔄 Processing category: Computers\n",
      "📊 1 findings in this category\n",
      "📄 Loaded prompt: Fallback STIG Remediation\n",
      "   Temperature: 0.2\n",
      "   Max tokens: 5000\n",
      "📄 Using prompt: Fallback STIG Remediation\n",
      "🌡️ Temperature: 0.2\n",
      "🎯 Max tokens: 5000\n",
      "\n",
      "  📋 Processing Finding 1/1:\n",
      "     Rule ID: xccdf_org.ssgproject.content_rule_rpm_verify_ownership\n",
      "     Title: Verify and Correct Ownership with RPM...\n",
      "🎯 Using max_tokens: 5000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 1651 characters\n",
      "📝 Raw response preview: {\n",
      "  \"intent\": \"Ensure file ownership and permissions are consistent with RPM database\",\n",
      "  \"ansible_tasks\": [\n",
      "    {\n",
      "      \"task_name\": \"Identify files with incorrect permissions\",\n",
      "      \"module\": \"comm...\n",
      "✅ Extracted JSON with 1 keys\n",
      "⚠️ JSON missing keys: ['target_type', 'target_name', 'ansible_module', 'ansible_params']\n",
      "     ✅ Target: unknown\n",
      "     Module: unknown\n",
      "\n",
      "============================================================\n",
      "✅ Processing complete!\n",
      "   Successful: 5\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Process all findings in all categories\n",
    "\n",
    "if categories_to_process and llm:\n",
    "    print(f\"🚀 Batch processing all findings...\\n\")\n",
    "    \n",
    "    for category, category_findings in categories_to_process.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔄 Processing category: {category}\")\n",
    "        print(f\"📊 {len(category_findings)} findings in this category\")\n",
    "        \n",
    "        # Get the prompt for this category\n",
    "        prompt_name = category_prompts.get(category, 'prompt_8_fallback')\n",
    "        processing_prompt = load_prompt(prompt_name)\n",
    "        \n",
    "        print(f\"📄 Using prompt: {processing_prompt['name']}\")\n",
    "        print(f\"🌡️ Temperature: {processing_prompt['parameters']['temperature']}\")\n",
    "        print(f\"🎯 Max tokens: {processing_prompt['parameters']['max_tokens']}\")\n",
    "        \n",
    "        # Process each finding in this category\n",
    "        for idx, result_to_process in enumerate(category_findings):\n",
    "            finding = result_to_process['finding']\n",
    "            \n",
    "            print(f\"\\n  📋 Processing Finding {idx + 1}/{len(category_findings)}:\")\n",
    "            print(f\"     Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "            print(f\"     Title: {finding.get('title', 'Unknown')[:60]}...\")\n",
    "            \n",
    "            # Format the processing prompt\n",
    "            formatted_prompt = format_prompt(\n",
    "                processing_prompt,\n",
    "                rule_id=finding.get('rule_id', ''),\n",
    "                title=finding.get('title', ''),\n",
    "                description=finding.get('description', ''),\n",
    "                fix_text=finding.get('fix_text', '')[:3000]  # Limit for processing\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Expected keys vary by category but commonly include these\n",
    "                expected_keys = ['target_type', 'target_name', 'ansible_module', 'ansible_params']\n",
    "                \n",
    "                # Make the LLM call\n",
    "                processing_result = await llm_call_with_json(\n",
    "                    llm,\n",
    "                    formatted_prompt,\n",
    "                    expected_keys,\n",
    "                    max_retries=3,\n",
    "                    prompt_params=processing_prompt['parameters']\n",
    "                )\n",
    "                \n",
    "                # Store the result\n",
    "                complete_result = {\n",
    "                    'finding_index': result_to_process['finding_index'],\n",
    "                    'rule_id': finding.get('rule_id', ''),\n",
    "                    'classification': result_to_process['classification'],\n",
    "                    'category': category,\n",
    "                    'processing_result': processing_result,\n",
    "                    'finding': finding,\n",
    "                    'processing_successful': all(key in processing_result for key in ['target_type', 'ansible_module']),\n",
    "                    'processed_at': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                processing_results.append(complete_result)\n",
    "                \n",
    "                success_status = \"✅\" if complete_result['processing_successful'] else \"⚠️\"\n",
    "                print(f\"     {success_status} Target: {processing_result.get('target_type', 'unknown')}\")\n",
    "                print(f\"     Module: {processing_result.get('ansible_module', 'unknown')}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ Processing error: {e}\")\n",
    "                processing_errors.append({\n",
    "                    'finding_index': result_to_process['finding_index'],\n",
    "                    'rule_id': finding.get('rule_id', ''),\n",
    "                    'classification': result_to_process['classification'],\n",
    "                    'category': category,\n",
    "                    'error': str(e),\n",
    "                    'finding': finding,\n",
    "                    'error_at': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✅ Processing complete!\")\n",
    "    print(f\"   Successful: {len(processing_results)}\")\n",
    "    print(f\"   Errors: {len(processing_errors)}\")\n",
    "else:\n",
    "    print(f\"⚠️ No categories to process or LLM not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 PROCESSING SUMMARY\n",
      "=========================\n",
      "Phase 1 - Classification:\n",
      "   Successful: 5\n",
      "   Errors: 0\n",
      "\n",
      "Phase 2 - Processing:\n",
      "   Successful: 5\n",
      "   Errors: 0\n",
      "   Fully successful: 5\n",
      "   Partial results: 0\n",
      "\n",
      "📈 Success by category:\n",
      "   Uncategorized: 2/2 (100.0%)\n",
      "   Programming: 1/1 (100.0%)\n",
      "   programming: 1/1 (100.0%)\n",
      "   Computers: 1/1 (100.0%)\n",
      "\n",
      "🔍 All processing results:\n",
      "   1. ✅ xccdf_org.ssgproject.content_r...\n",
      "      Category: Uncategorized\n",
      "      Target: unknown\n",
      "      Module: command\n",
      "   2. ✅ xccdf_org.ssgproject.content_r...\n",
      "      Category: Uncategorized\n",
      "      Target: unknown\n",
      "      Module: unknown\n",
      "   3. ✅ xccdf_org.ssgproject.content_r...\n",
      "      Category: Programming\n",
      "      Target: unknown\n",
      "      Module: unknown\n",
      "   4. ✅ xccdf_org.ssgproject.content_r...\n",
      "      Category: programming\n",
      "      Target: unknown\n",
      "      Module: unknown\n",
      "   5. ✅ xccdf_org.ssgproject.content_r...\n",
      "      Category: Computers\n",
      "      Target: unknown\n",
      "      Module: unknown\n",
      "\n",
      "✅ All 5 findings processed!\n",
      "🎯 Ready for playbook generation\n"
     ]
    }
   ],
   "source": [
    "# Processing summary and results analysis\n",
    "print(\"📊 PROCESSING SUMMARY\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "print(f\"Phase 1 - Classification:\")\n",
    "print(f\"   Successful: {len(classification_results)}\")\n",
    "print(f\"   Errors: {len(classification_errors)}\")\n",
    "\n",
    "print(f\"\\nPhase 2 - Processing:\")\n",
    "print(f\"   Successful: {len(processing_results)}\")\n",
    "print(f\"   Errors: {len(processing_errors)}\")\n",
    "\n",
    "if processing_results:\n",
    "    successful_processing = [r for r in processing_results if r['processing_successful']]\n",
    "    partial_processing = [r for r in processing_results if not r['processing_successful']]\n",
    "    \n",
    "    print(f\"   Fully successful: {len(successful_processing)}\")\n",
    "    print(f\"   Partial results: {len(partial_processing)}\")\n",
    "    \n",
    "    # Show processing by category\n",
    "    category_success = {}\n",
    "    for result in processing_results:\n",
    "        category = result['category']\n",
    "        if category not in category_success:\n",
    "            category_success[category] = {'total': 0, 'successful': 0}\n",
    "        category_success[category]['total'] += 1\n",
    "        if result['processing_successful']:\n",
    "            category_success[category]['successful'] += 1\n",
    "    \n",
    "    print(f\"\\n📈 Success by category:\")\n",
    "    for category, stats in category_success.items():\n",
    "        success_rate = stats['successful'] / stats['total'] * 100\n",
    "        print(f\"   {category}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # Show all processing results\n",
    "    print(f\"\\n🔍 All processing results:\")\n",
    "    for i, result in enumerate(processing_results):\n",
    "        processing_data = result['processing_result']\n",
    "        status = \"✅\" if result['processing_successful'] else \"⚠️\"\n",
    "        print(f\"   {i+1}. {status} {result['rule_id'][:30]}...\")\n",
    "        print(f\"      Category: {result['category']}\")\n",
    "        print(f\"      Target: {processing_data.get('target_type', 'Unknown')}\")\n",
    "        print(f\"      Module: {processing_data.get('ansible_module', 'Unknown')}\")\n",
    "\n",
    "total_processed = len(processing_results) + len(processing_errors)\n",
    "remaining = len(llm_findings) - total_processed\n",
    "\n",
    "if remaining > 0:\n",
    "    print(f\"\\n⏳ {remaining} findings still need processing\")\n",
    "else:\n",
    "    print(f\"\\n✅ All {len(llm_findings)} findings processed!\")\n",
    "    print(f\"🎯 Ready for playbook generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Ansible Playbooks and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating Ansible Playbooks\n",
      "==============================\n",
      "📊 Generating playbooks for 5 successful results...\n",
      "💾 Saved LLM targets to: ../playbooks/20250714_120000/llm_processed/llm_generated_targets_20250714_120000.json\n",
      "✅ Generated playbook with 5 tasks: ../playbooks/20250714_120000/llm_processed/llm_generated_playbook_20250714_120000.yml\n",
      "✅ Generated LLM playbook: ../playbooks/20250714_120000/llm_processed/llm_generated_playbook_20250714_120000.yml\n",
      "📊 LLM Playbook stats: 7 tasks\n",
      "\n",
      "📊 Generation Summary:\n",
      "   Playbooks generated: 1\n",
      "   Items for human review: 0\n"
     ]
    }
   ],
   "source": [
    "# Generate Ansible playbooks from successful processing results\n",
    "print(\"🚀 Generating Ansible Playbooks\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "successful_processing = [r for r in processing_results if r['processing_successful']]\n",
    "partial_processing = [r for r in processing_results if not r['processing_successful']]\n",
    "\n",
    "playbooks_generated = 0\n",
    "human_review_needed = []\n",
    "\n",
    "if successful_processing:\n",
    "    print(f\"📊 Generating playbooks for {len(successful_processing)} successful results...\")\n",
    "    \n",
    "    # Convert processing results to target format for deterministic generator\n",
    "    llm_targets = []\n",
    "    \n",
    "    for result in successful_processing:\n",
    "        processing_data = result['processing_result']\n",
    "        finding = result['finding']\n",
    "        \n",
    "        # Fix ansible_params if it's a string\n",
    "        ansible_params = processing_data.get('ansible_params', {})\n",
    "        if isinstance(ansible_params, str):\n",
    "            # Try to parse as JSON\n",
    "            try:\n",
    "                ansible_params = json.loads(ansible_params)\n",
    "            except:\n",
    "                # If parsing fails, create a simple dict\n",
    "                ansible_params = {'value': ansible_params}\n",
    "        \n",
    "        # Create target in the format expected by deterministic generator\n",
    "        target = {\n",
    "            'rule_id': result['rule_id'],\n",
    "            'severity': finding.get('severity', 'medium'),\n",
    "            'status': finding.get('status', 'fail'),\n",
    "            'title': finding.get('title', ''),\n",
    "            'target_type': processing_data.get('target_type', 'unknown'),\n",
    "            'target_name': processing_data.get('target_name', ''),\n",
    "            'action_context': processing_data.get('action_context', ''),\n",
    "            'ansible_module': processing_data.get('ansible_module', 'debug'),\n",
    "            'ansible_params': ansible_params,\n",
    "            'compliance': {\n",
    "                'cci_refs': finding.get('compliance', {}).get('cci_refs', []),\n",
    "                'nist_refs': finding.get('compliance', {}).get('nist_refs', []),\n",
    "                'cis_refs': finding.get('compliance', {}).get('cis_refs', [])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        llm_targets.append(target)\n",
    "    \n",
    "    # Save LLM targets file\n",
    "    llm_targets_file = llm_output_dir / f\"llm_generated_targets_{RUN_TIMESTAMP}.json\"\n",
    "    llm_targets_data = {\n",
    "        'metadata': {\n",
    "            'total_actionable': len(llm_targets),\n",
    "            'extraction_date': datetime.now().isoformat(),\n",
    "            'source': 'llm_batch_processing',\n",
    "            'run_timestamp': RUN_TIMESTAMP,\n",
    "            'processing_summary': {\n",
    "                'classified': len(classification_results),\n",
    "                'processed': len(processing_results),\n",
    "                'successful': len(successful_processing)\n",
    "            }\n",
    "        },\n",
    "        'targets': llm_targets\n",
    "    }\n",
    "    \n",
    "    with open(llm_targets_file, 'w') as f:\n",
    "        json.dump(llm_targets_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved LLM targets to: {llm_targets_file}\")\n",
    "    \n",
    "    # Generate playbook using deterministic generator\n",
    "    try:\n",
    "        generator = DeterministicPlaybookGenerator()\n",
    "        playbook_file = llm_output_dir / f\"llm_generated_playbook_{RUN_TIMESTAMP}.yml\"\n",
    "        \n",
    "        playbook = generator.generate_playbook_from_targets(\n",
    "            str(llm_targets_file),\n",
    "            str(playbook_file)\n",
    "        )\n",
    "        \n",
    "        playbooks_generated = 1\n",
    "        print(f\"✅ Generated LLM playbook: {playbook_file}\")\n",
    "        \n",
    "        # Show playbook stats\n",
    "        total_tasks = sum(len(play.get('tasks', [])) for play in playbook)\n",
    "        print(f\"📊 LLM Playbook stats: {total_tasks} tasks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating LLM playbook: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"⚠️ No successful processing results to convert to playbooks\")\n",
    "\n",
    "# Collect items needing human review\n",
    "human_review_needed.extend(partial_processing)\n",
    "human_review_needed.extend(processing_errors)\n",
    "human_review_needed.extend(classification_errors)\n",
    "\n",
    "print(f\"\\n📊 Generation Summary:\")\n",
    "print(f\"   Playbooks generated: {playbooks_generated}\")\n",
    "print(f\"   Items for human review: {len(human_review_needed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No items need human review - all processing successful!\n"
     ]
    }
   ],
   "source": [
    "# Save items requiring human review\n",
    "if human_review_needed:\n",
    "    print(f\"💾 Saving {len(human_review_needed)} items for human review...\")\n",
    "    \n",
    "    human_review_file = human_review_dir / f\"human_review_needed_{RUN_TIMESTAMP}.json\"\n",
    "    \n",
    "    human_review_data = {\n",
    "        'metadata': {\n",
    "            'total_items': len(human_review_needed),\n",
    "            'created_date': datetime.now().isoformat(),\n",
    "            'source': 'llm_batch_processing_failures',\n",
    "            'run_timestamp': RUN_TIMESTAMP,\n",
    "            'description': 'Findings that could not be processed successfully and require human review'\n",
    "        },\n",
    "        'items': human_review_needed\n",
    "    }\n",
    "    \n",
    "    with open(human_review_file, 'w') as f:\n",
    "        json.dump(human_review_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved human review items to: {human_review_file}\")\n",
    "    \n",
    "    # Analyze human review items\n",
    "    review_types = {}\n",
    "    for item in human_review_needed:\n",
    "        if 'error' in item:\n",
    "            item_type = 'processing_error'\n",
    "        elif 'processing_successful' in item and not item['processing_successful']:\n",
    "            item_type = 'partial_result'\n",
    "        else:\n",
    "            item_type = 'classification_error'\n",
    "        \n",
    "        review_types[item_type] = review_types.get(item_type, 0) + 1\n",
    "    \n",
    "    print(f\"📈 Human review breakdown: {review_types}\")\n",
    "else:\n",
    "    print(f\"✅ No items need human review - all processing successful!\")\n",
    "    human_review_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final processing summary\n",
    "final_summary = {\n",
    "    'run_timestamp': RUN_TIMESTAMP,\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'input_file': LLM_NEEDED_FILE,\n",
    "    'processing_limits': {\n",
    "        'max_findings_processed': MAX_FINDINGS_TO_PROCESS,\n",
    "        'actual_findings_processed': len(llm_findings),\n",
    "        'total_findings_available': len(all_llm_findings)\n",
    "    },\n",
    "    'phase_1_classification': {\n",
    "        'attempted': len(classification_results),\n",
    "        'successful': len([r for r in classification_results if r['classification_successful']]),\n",
    "        'errors': len(classification_errors),\n",
    "        'categories_found': list(set([r['classification'] for r in classification_results if r['classification_successful']]))\n",
    "    },\n",
    "    'phase_2_processing': {\n",
    "        'attempted': len(processing_results),\n",
    "        'successful': len([r for r in processing_results if r['processing_successful']]),\n",
    "        'partial': len([r for r in processing_results if not r['processing_successful']]),\n",
    "        'errors': len(processing_errors)\n",
    "    },\n",
    "    'output_generation': {\n",
    "        'playbooks_generated': playbooks_generated,\n",
    "        'human_review_items': len(human_review_needed)\n",
    "    },\n",
    "    'output_files': {\n",
    "        'llm_targets': str(llm_targets_file) if 'llm_targets_file' in locals() else None,\n",
    "        'llm_playbook': str(playbook_file) if 'playbook_file' in locals() else None,\n",
    "        'human_review': str(human_review_file) if human_review_needed else None\n",
    "    },\n",
    "    'success_rates': {\n",
    "        'classification_rate': len([r for r in classification_results if r['classification_successful']]) / len(classification_results) * 100 if classification_results else 0,\n",
    "        'processing_rate': len([r for r in processing_results if r['processing_successful']]) / len(processing_results) * 100 if processing_results else 0,\n",
    "        'overall_success_rate': len([r for r in processing_results if r['processing_successful']]) / len(llm_findings) * 100 if llm_findings else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save final summary\n",
    "summary_file = Path(PLAYBOOKS_RUN_DIR) / f\"llm_processing_summary_{RUN_TIMESTAMP}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved final summary to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 LLM BATCH PROCESSING COMPLETE\n",
      "========================================\n",
      "Run timestamp: 20250714_120000\n",
      "Processing date: 2025-07-14 11:41:19\n",
      "\n",
      "📊 FINAL STATISTICS:\n",
      "Input findings: 5 (limited from 1094)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📊 FINAL STATISTICS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput findings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(llm_findings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (limited from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_llm_findings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClassification success: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfinal_summary\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mphase_1_classification\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msuccessful\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_summary[\u001b[33m'\u001b[39m\u001b[33mphase_1_classification\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mattempted\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_summary[\u001b[33m'\u001b[39m\u001b[33msuccess_rates\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mclassification_rate\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing success: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_summary[\u001b[33m'\u001b[39m\u001b[33mphase_2_processing\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msuccessful\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_summary[\u001b[33m'\u001b[39m\u001b[33mphase_2_processing\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mattempted\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_summary[\u001b[33m'\u001b[39m\u001b[33msuccess_rates\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mprocessing_rate\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall success rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_summary[\u001b[33m'\u001b[39m\u001b[33msuccess_rates\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33moverall_success_rate\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'final_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# Final summary and next steps\n",
    "print(\"🎯 LLM BATCH PROCESSING COMPLETE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"Processing date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n📊 FINAL STATISTICS:\")\n",
    "print(f\"Input findings: {len(llm_findings)} (limited from {len(all_llm_findings)})\")\n",
    "print(f\"Classification success: {final_summary['phase_1_classification']['successful']}/{final_summary['phase_1_classification']['attempted']} ({final_summary['success_rates']['classification_rate']:.1f}%)\")\n",
    "print(f\"Processing success: {final_summary['phase_2_processing']['successful']}/{final_summary['phase_2_processing']['attempted']} ({final_summary['success_rates']['processing_rate']:.1f}%)\")\n",
    "print(f\"Overall success rate: {final_summary['success_rates']['overall_success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n📁 OUTPUT FILES:\")\n",
    "if final_summary['output_files']['llm_playbook']:\n",
    "    print(f\"   ✅ LLM Generated Playbook: {final_summary['output_files']['llm_playbook']}\")\n",
    "if final_summary['output_files']['llm_targets']:\n",
    "    print(f\"   📊 LLM Targets: {final_summary['output_files']['llm_targets']}\")\n",
    "if final_summary['output_files']['human_review']:\n",
    "    print(f\"   👤 Human Review Needed: {final_summary['output_files']['human_review']}\")\n",
    "print(f\"   📋 Processing Summary: {summary_file}\")\n",
    "\n",
    "print(f\"\\n🎯 NEXT STEPS:\")\n",
    "if final_summary['processing_limits']['actual_findings_processed'] < final_summary['processing_limits']['total_findings_available']:\n",
    "    remaining = final_summary['processing_limits']['total_findings_available'] - final_summary['processing_limits']['actual_findings_processed']\n",
    "    print(f\"   🔄 Increase MAX_FINDINGS_TO_PROCESS to process {remaining} more findings\")\n",
    "    print(f\"   📝 Current limit: {MAX_FINDINGS_TO_PROCESS}, Total available: {final_summary['processing_limits']['total_findings_available']}\")\n",
    "\n",
    "if final_summary['output_generation']['human_review_items'] > 0:\n",
    "    print(f\"   👤 Review {final_summary['output_generation']['human_review_items']} items requiring manual attention\")\n",
    "    print(f\"   🔧 Consider prompt engineering improvements based on failure patterns\")\n",
    "\n",
    "if final_summary['output_generation']['playbooks_generated'] > 0:\n",
    "    print(f\"   ✅ Test generated playbooks in a safe environment\")\n",
    "    print(f\"   🚀 Deploy playbooks to target systems\")\n",
    "\n",
    "print(f\"\\n📈 OPTIMIZATION INSIGHTS:\")\n",
    "if final_summary['success_rates']['overall_success_rate'] < 80:\n",
    "    print(f\"   ⚠️ Success rate below 80% - consider prompt improvements\")\n",
    "if final_summary['phase_1_classification']['successful'] < final_summary['phase_1_classification']['attempted']:\n",
    "    print(f\"   🎯 Classification issues - review prompt_1_classification\")\n",
    "if final_summary['phase_2_processing']['partial'] > 0:\n",
    "    print(f\"   🔧 {final_summary['phase_2_processing']['partial']} partial results - review category prompts\")\n",
    "\n",
    "print(f\"\\n🎉 LLM batch processing workflow complete!\")\n",
    "print(f\"📁 All outputs saved to: {PLAYBOOKS_RUN_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
