{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Batch Process LLM Findings\n",
    "\n",
    "This notebook processes findings that require LLM classification and processing using the optimization strategy.\n",
    "\n",
    "**Input:** \n",
    "- LLM needed findings JSON file (from Step 2)\n",
    "\n",
    "**Output:**\n",
    "- Ansible playbooks for successfully processed findings\n",
    "- JSON file with findings requiring human review\n",
    "- Processing summary and statistics\n",
    "\n",
    "**Features:**\n",
    "- Configurable batch size limiter for testing\n",
    "- Manual prompt engineering capabilities\n",
    "- Step-by-step inspection of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Libraries imported successfully\n",
      "🐍 Python version: 3.11.12\n",
      "📁 Current working directory: /Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Import our modules\n",
    "from llm_interface import LLMInterface\n",
    "from ansible_playbook_generator import DeterministicPlaybookGenerator\n",
    "from shared.prompt_utils import (\n",
    "    load_prompt, format_prompt, llm_call_with_json, \n",
    "    display_prompt, display_result, clean_playbook_response\n",
    ")\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"📁 Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕐 Processing run: 20250714_120000\n",
      "📂 Input file: ../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\n",
      "📊 Processing limits:\n",
      "   Max findings to process: 5\n",
      "   Classification batch size: 10\n",
      "   Processing batch size: 5\n",
      "📁 Output directories:\n",
      "   LLM processed: ../playbooks/20250714_120000/llm_processed\n",
      "   Human review: ../playbooks/20250714_120000/human_review\n",
      "✅ Found LLM needed file: ../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these from Step 2 output\n",
    "RUN_TIMESTAMP = \"20250714_120000\"  # Update from Step 2\n",
    "LLM_NEEDED_FILE = \"../playbooks/20250714_120000/llm_needed/llm_needed_findings_20250714_120000.json\"  # Update from Step 2\n",
    "\n",
    "# Processing configuration\n",
    "MAX_FINDINGS_TO_PROCESS = 5  # 🚀 Start with 5 for testing, increase later\n",
    "BATCH_SIZE_CLASSIFICATION = 10  # How many findings to classify at once\n",
    "BATCH_SIZE_PROCESSING = 5   # How many findings to process at once\n",
    "\n",
    "# Output configuration\n",
    "PLAYBOOKS_BASE_DIR = \"../playbooks\"\n",
    "PLAYBOOKS_RUN_DIR = f\"{PLAYBOOKS_BASE_DIR}/{RUN_TIMESTAMP}\"\n",
    "LLM_OUTPUT_DIR = f\"{PLAYBOOKS_RUN_DIR}/llm_processed\"\n",
    "HUMAN_REVIEW_DIR = f\"{PLAYBOOKS_RUN_DIR}/human_review\"\n",
    "\n",
    "print(f\"🕐 Processing run: {RUN_TIMESTAMP}\")\n",
    "print(f\"📂 Input file: {LLM_NEEDED_FILE}\")\n",
    "print(f\"📊 Processing limits:\")\n",
    "print(f\"   Max findings to process: {MAX_FINDINGS_TO_PROCESS}\")\n",
    "print(f\"   Classification batch size: {BATCH_SIZE_CLASSIFICATION}\")\n",
    "print(f\"   Processing batch size: {BATCH_SIZE_PROCESSING}\")\n",
    "print(f\"📁 Output directories:\")\n",
    "print(f\"   LLM processed: {LLM_OUTPUT_DIR}\")\n",
    "print(f\"   Human review: {HUMAN_REVIEW_DIR}\")\n",
    "\n",
    "# Verify input file exists\n",
    "if Path(LLM_NEEDED_FILE).exists():\n",
    "    print(f\"✅ Found LLM needed file: {LLM_NEEDED_FILE}\")\n",
    "else:\n",
    "    print(f\"❌ Missing LLM needed file: {LLM_NEEDED_FILE}\")\n",
    "    print(\"Please update LLM_NEEDED_FILE path from Step 2 output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Created directory structure:\n",
      "   LLM processed: ../playbooks/20250714_120000/llm_processed\n",
      "   Human review: ../playbooks/20250714_120000/human_review\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure\n",
    "llm_output_dir = Path(LLM_OUTPUT_DIR)\n",
    "human_review_dir = Path(HUMAN_REVIEW_DIR)\n",
    "\n",
    "llm_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "human_review_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Created directory structure:\")\n",
    "print(f\"   LLM processed: {llm_output_dir}\")\n",
    "print(f\"   Human review: {human_review_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Initializing LLM interface...\n",
      "🤖 LLM Interface initialized\n",
      "   Model: granite-3-3-8b-instruct\n",
      "   URL: https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/completions\n",
      "✅ LLM initialized successfully\n",
      "   Model: granite-3-3-8b-instruct\n",
      "   API URL: https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/completions\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM interface\n",
    "print(\"🤖 Initializing LLM interface...\")\n",
    "\n",
    "try:\n",
    "    llm = LLMInterface()\n",
    "    print(f\"✅ LLM initialized successfully\")\n",
    "    print(f\"   Model: {llm.model_name}\")\n",
    "    print(f\"   API URL: {llm.api_url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize LLM: {e}\")\n",
    "    print(\"Please check your .env file and API configuration\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading LLM needed findings...\n",
      "📈 LLM needed findings loaded: 1094\n",
      "📄 Source: deterministic_processing_step\n",
      "🚀 Processing limited to first 5 findings for testing\n",
      "📈 Processing severity distribution: {'medium': 2, 'high': 3}\n"
     ]
    }
   ],
   "source": [
    "# Load LLM needed findings\n",
    "print(\"📊 Loading LLM needed findings...\")\n",
    "\n",
    "with open(LLM_NEEDED_FILE, 'r') as f:\n",
    "    llm_needed_data = json.load(f)\n",
    "\n",
    "all_llm_findings = llm_needed_data.get('findings', [])\n",
    "metadata = llm_needed_data.get('metadata', {})\n",
    "\n",
    "print(f\"📈 LLM needed findings loaded: {len(all_llm_findings)}\")\n",
    "print(f\"📄 Source: {metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "# Apply processing limit for testing\n",
    "if MAX_FINDINGS_TO_PROCESS > 0 and len(all_llm_findings) > MAX_FINDINGS_TO_PROCESS:\n",
    "    llm_findings = all_llm_findings[:MAX_FINDINGS_TO_PROCESS]\n",
    "    print(f\"🚀 Processing limited to first {MAX_FINDINGS_TO_PROCESS} findings for testing\")\n",
    "else:\n",
    "    llm_findings = all_llm_findings\n",
    "    print(f\"🚀 Processing all {len(llm_findings)} findings\")\n",
    "\n",
    "# Show severity distribution\n",
    "severity_counts = {}\n",
    "for finding in llm_findings:\n",
    "    severity = finding.get('severity', 'unknown')\n",
    "    severity_counts[severity] = severity_counts.get(severity, 0) + 1\n",
    "\n",
    "print(f\"📈 Processing severity distribution: {severity_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Sample LLM Findings (first 2):\n",
      "\n",
      "📋 Finding 1:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "   Severity: medium\n",
      "   Title: Prefer to use a 64-bit Operating System when supported...\n",
      "   Description: Prefer installation of 64-bit operating systems when the CPU supports it. Prefer installation of 64-...\n",
      "   Fix text length: 0 chars\n",
      "\n",
      "📋 Finding 2:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_disable_prelink\n",
      "   Severity: medium\n",
      "   Title: Disable Prelinking...\n",
      "   Description: The prelinking feature changes binaries in an attempt to decrease their startup time. In order to di...\n",
      "   Fix text length: 1025 chars\n",
      "   Fix text preview: # prelink not installed if test -e /etc/sysconfig/prelink -o -e /usr/sbin/prelink; then if grep -q ^PRELINKING /etc/sysconfig/prelink then sed -i 's/^PRELINKING[:blank:]*=[:blank:]*[:alpha:]*/PRELINKI...\n"
     ]
    }
   ],
   "source": [
    "# Show sample findings for inspection\n",
    "if llm_findings:\n",
    "    print(\"🔍 Sample LLM Findings (first 2):\")\n",
    "    for i, finding in enumerate(llm_findings[:2]):\n",
    "        print(f\"\\n📋 Finding {i+1}:\")\n",
    "        print(f\"   Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "        print(f\"   Severity: {finding.get('severity', 'Unknown')}\")\n",
    "        print(f\"   Title: {finding.get('title', 'Unknown')[:80]}...\")\n",
    "        print(f\"   Description: {finding.get('description', 'Unknown')[:100]}...\")\n",
    "        print(f\"   Fix text length: {len(finding.get('fix_text', ''))} chars\")\n",
    "        \n",
    "        # Show first 200 chars of fix_text for inspection\n",
    "        fix_text = finding.get('fix_text', '')\n",
    "        if fix_text:\n",
    "            print(f\"   Fix text preview: {fix_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Classification\n",
    "\n",
    "Classify findings into complexity categories using the optimization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Phase 1: Classification\n",
      "========================================\n",
      "📄 Loaded prompt: STIG Finding Complexity Classification\n",
      "   Temperature: 0.0\n",
      "   Max tokens: 2000\n",
      "📄 Loaded classification prompt: STIG Finding Complexity Classification\n",
      "🌡️ Temperature: 0.0\n",
      "🎯 Max tokens: 2000\n",
      "\n",
      "🔄 Classifying 5 findings...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Classify findings using prompt_1_classification\n",
    "print(\"🎯 Starting Phase 1: Classification\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if not llm:\n",
    "    print(\"❌ Cannot proceed - LLM not initialized\")\n",
    "else:\n",
    "    # Load classification prompt\n",
    "    classification_prompt = load_prompt('prompt_1_classification')\n",
    "    print(f\"📄 Loaded classification prompt: {classification_prompt['name']}\")\n",
    "    print(f\"🌡️ Temperature: {classification_prompt['parameters']['temperature']}\")\n",
    "    print(f\"🎯 Max tokens: {classification_prompt['parameters']['max_tokens']}\")\n",
    "    \n",
    "    # Classification results storage\n",
    "    classification_results = []\n",
    "    classification_errors = []\n",
    "    \n",
    "    print(f\"\\n🔄 Classifying {len(llm_findings)} findings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Classifying Finding 1/5\n",
      "📋 Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "📝 Title: Prefer to use a 64-bit Operating System when supported...\n",
      "\n",
      "📋 Formatted Classification Prompt:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 📋 Prompt (showing first 1000 characters):\n",
       "```\n",
       "You are analyzing a STIG security finding to classify its complexity type.\n",
       "\n",
       "STIG Finding:\n",
       "<rule_id>xccdf_org.ssgproject.content_rule_prefer_64bit_os</rule_id>\n",
       "<title>Prefer to use a 64-bit Operating System when supported</title>\n",
       "<description>Prefer installation of 64-bit operating systems when the CPU supports it. Prefer installation of 64-bit operating systems when the CPU supports it.</description>\n",
       "<fix_text></fix_text>\n",
       "\n",
       "Classify this finding into ONE category:\n",
       "\n",
       "1. SHELL_SCRIPT - Complex shell scripts with conditionals, loops, or multiple commands\n",
       "2. PACKAGE_VERIFICATION - RPM verification, package integrity checks, reinstallation logic\n",
       "3. CONFIG_MODIFICATION - Complex file configuration changes (AIDE, PAM, etc.)\n",
       "4. BOOT_CONFIGURATION - GRUB, kernel parameters, boot-critical changes\n",
       "5. MULTI_STEP_PROCESS - Sequential tasks that must be done in specific order\n",
       "6. CRON_SCHEDULING - Cron job creation with complex timing or piping\n",
       "7. UNKNOWN - Cannot be classified into above categories\n",
       "\n",
       "L\n",
       "\n",
       "... [TRUNCATED] ...\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Using max_tokens: 2000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 106 characters\n",
      "📝 Raw response preview: ```json\n",
      "{\n",
      "  \"ComplexityType\": \"UNKNOWN\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "## response:\n",
      "```json\n",
      "{\n",
      "  \"ComplexityType\": \"UNKNOWN\"\n",
      "}\n",
      "```\n",
      "✅ Extracted JSON with 1 keys\n",
      "⚠️ JSON missing keys: ['category']\n",
      "\n",
      "✅ Classification Result:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ✅ Finding 1 Classification Result:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ComplexityType": "UNKNOWN",
       "category": "unknown"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Stored classification: unknown\n"
     ]
    }
   ],
   "source": [
    "# Classify findings one by one for manual inspection\n",
    "# This allows you to see each classification and adjust prompts if needed\n",
    "\n",
    "current_finding_index = 0  # 🎛️ Change this to start from a different finding\n",
    "\n",
    "if current_finding_index < len(llm_findings) and llm:\n",
    "    finding = llm_findings[current_finding_index]\n",
    "    \n",
    "    print(f\"🔍 Classifying Finding {current_finding_index + 1}/{len(llm_findings)}\")\n",
    "    print(f\"📋 Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "    print(f\"📝 Title: {finding.get('title', 'Unknown')[:60]}...\")\n",
    "    \n",
    "    # Format the classification prompt\n",
    "    formatted_prompt = format_prompt(\n",
    "        classification_prompt,\n",
    "        rule_id=finding.get('rule_id', ''),\n",
    "        title=finding.get('title', ''),\n",
    "        description=finding.get('description', ''),\n",
    "        fix_text=finding.get('fix_text', '')[:2000]  # Limit fix_text length\n",
    "    )\n",
    "    \n",
    "    # Display the formatted prompt for inspection\n",
    "    print(\"\\n📋 Formatted Classification Prompt:\")\n",
    "    display_prompt(formatted_prompt, max_length=1000)\n",
    "    \n",
    "    # Make the LLM call\n",
    "    try:\n",
    "        result = await llm_call_with_json(\n",
    "            llm, \n",
    "            formatted_prompt, \n",
    "            ['category'], \n",
    "            max_retries=3,\n",
    "            prompt_params=classification_prompt['parameters']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ Classification Result:\")\n",
    "        display_result(f\"Finding {current_finding_index + 1} Classification\", result)\n",
    "        \n",
    "        # Store the result\n",
    "        classification_result = {\n",
    "            'finding_index': current_finding_index,\n",
    "            'rule_id': finding.get('rule_id', ''),\n",
    "            'classification': result.get('category', 'UNKNOWN'),\n",
    "            'finding': finding,\n",
    "            'classification_successful': result.get('category') not in ['extraction_failed', 'llm_not_available']\n",
    "        }\n",
    "        \n",
    "        classification_results.append(classification_result)\n",
    "        \n",
    "        print(f\"💾 Stored classification: {result.get('category', 'UNKNOWN')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Classification error: {e}\")\n",
    "        classification_errors.append({\n",
    "            'finding_index': current_finding_index,\n",
    "            'rule_id': finding.get('rule_id', ''),\n",
    "            'error': str(e),\n",
    "            'finding': finding\n",
    "        })\n",
    "        \n",
    "else:\n",
    "    print(f\"⚠️ No more findings to classify or LLM not available\")\n",
    "    print(f\"Current index: {current_finding_index}, Total findings: {len(llm_findings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Auto-advancing to finding 3\n",
      "📊 Progress: 2/5 classified\n",
      "\n",
      "🎛️ Go back to the previous cell to classify this finding\n",
      "   Or update current_finding_index = 2 in the previous cell\n"
     ]
    }
   ],
   "source": [
    "# Continue classification - run this cell multiple times to classify more findings\n",
    "# 🎛️ Increment current_finding_index and re-run the previous cell to classify the next finding\n",
    "# Or run this cell to auto-increment and classify the next finding\n",
    "\n",
    "if len(classification_results) < len(llm_findings) and llm:\n",
    "    current_finding_index = len(classification_results)  # Auto-increment to next unprocessed finding\n",
    "    \n",
    "    if current_finding_index < len(llm_findings):\n",
    "        print(f\"🔄 Auto-advancing to finding {current_finding_index + 1}\")\n",
    "        print(f\"📊 Progress: {len(classification_results)}/{len(llm_findings)} classified\")\n",
    "        print(f\"\\n🎛️ Go back to the previous cell to classify this finding\")\n",
    "        print(f\"   Or update current_finding_index = {current_finding_index} in the previous cell\")\n",
    "    else:\n",
    "        print(f\"✅ All findings classified!\")\n",
    "        print(f\"📊 Total classified: {len(classification_results)}\")\n",
    "        print(f\"❌ Errors: {len(classification_errors)}\")\n",
    "else:\n",
    "    print(f\"✅ Classification phase complete or LLM not available\")\n",
    "    print(f\"📊 Results: {len(classification_results)} successful, {len(classification_errors)} errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary and analysis\n",
    "if classification_results:\n",
    "    print(\"📊 CLASSIFICATION SUMMARY\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Count classifications by category\n",
    "    category_counts = {}\n",
    "    successful_classifications = 0\n",
    "    \n",
    "    for result in classification_results:\n",
    "        category = result['classification']\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "        \n",
    "        if result['classification_successful']:\n",
    "            successful_classifications += 1\n",
    "    \n",
    "    print(f\"Total classifications attempted: {len(classification_results)}\")\n",
    "    print(f\"Successful classifications: {successful_classifications}\")\n",
    "    print(f\"Failed classifications: {len(classification_results) - successful_classifications}\")\n",
    "    print(f\"Classification errors: {len(classification_errors)}\")\n",
    "    \n",
    "    print(f\"\\n📈 Categories identified:\")\n",
    "    for category, count in sorted(category_counts.items()):\n",
    "        print(f\"   {category}: {count}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\n🔍 Sample classifications:\")\n",
    "    for i, result in enumerate(classification_results[:3]):\n",
    "        print(f\"   {i+1}. {result['rule_id'][:40]}... → {result['classification']}\")\n",
    "        \n",
    "    if len(classification_results) >= len(llm_findings):\n",
    "        print(f\"\\n✅ Ready for Phase 2: Processing by category\")\n",
    "    else:\n",
    "        remaining = len(llm_findings) - len(classification_results)\n",
    "        print(f\"\\n⏳ {remaining} findings still need classification\")\n",
    "        print(f\"🔄 Continue running classification cells above\")\n",
    "else:\n",
    "    print(\"⚠️ No classification results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Processing by Category\n",
    "\n",
    "Process classified findings using category-specific prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Phase 2: Processing by Category\n",
      "=============================================\n",
      "📊 Categories to process: 1\n",
      "   unknown: 3 findings\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Process findings by category\n",
    "print(\"🎯 Starting Phase 2: Processing by Category\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if not classification_results:\n",
    "    print(\"❌ No classification results available - run Phase 1 first\")\n",
    "elif not llm:\n",
    "    print(\"❌ LLM not initialized\")\n",
    "else:\n",
    "    # Group successful classifications by category\n",
    "    successful_classifications = [r for r in classification_results if r['classification_successful']]\n",
    "    \n",
    "    categories_to_process = {}\n",
    "    for result in successful_classifications:\n",
    "        category = result['classification']\n",
    "        if category not in categories_to_process:\n",
    "            categories_to_process[category] = []\n",
    "        categories_to_process[category].append(result)\n",
    "    \n",
    "    print(f\"📊 Categories to process: {len(categories_to_process)}\")\n",
    "    for category, findings in categories_to_process.items():\n",
    "        print(f\"   {category}: {len(findings)} findings\")\n",
    "    \n",
    "    # Mapping of categories to prompt files\n",
    "    category_prompts = {\n",
    "        'SHELL_SCRIPT': 'prompt_2_shell_script',\n",
    "        'PACKAGE_VERIFICATION': 'prompt_3_package_verification',\n",
    "        'CONFIG_MODIFICATION': 'prompt_4_config_modification',\n",
    "        'BOOT_CONFIGURATION': 'prompt_5_boot_configuration',\n",
    "        'MULTI_STEP_PROCESS': 'prompt_6_multi_step_process',\n",
    "        'CRON_SCHEDULING': 'prompt_7_cron_scheduling',\n",
    "        'UNKNOWN': 'prompt_8_fallback'\n",
    "    }\n",
    "    \n",
    "    # Processing results storage\n",
    "    processing_results = []\n",
    "    processing_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Auto-selected category: unknown\n",
      "🔄 Processing unknown\n",
      "📊 Finding 1/3 in this category\n",
      "📄 Loaded prompt: Fallback STIG Remediation\n",
      "   Temperature: 0.2\n",
      "   Max tokens: 5000\n",
      "📄 Using prompt: Fallback STIG Remediation\n",
      "🌡️ Temperature: 0.2\n",
      "🎯 Max tokens: 5000\n",
      "\n",
      "📋 Processing Finding:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_prefer_64bit_os\n",
      "   Classification: unknown\n",
      "   Title: Prefer to use a 64-bit Operating System when supported...\n",
      "\n",
      "📋 Formatted Processing Prompt:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 📋 Prompt:\n",
       "```\n",
       "You are analyzing a complex STIG finding that doesn't fit standard patterns.\n",
       "\n",
       "STIG Finding:\n",
       "<rule_id>xccdf_org.ssgproject.content_rule_prefer_64bit_os</rule_id>\n",
       "<title>Prefer to use a 64-bit Operating System when supported</title>\n",
       "<description>Prefer installation of 64-bit operating systems when the CPU supports it. Prefer installation of 64-bit operating systems when the CPU supports it.</description>\n",
       "<fix_text></fix_text>\n",
       "\n",
       "Analyze the intent and create the best possible Ansible automation.\n",
       "\n",
       "Approach:\n",
       "1. Identify the core security objective\n",
       "2. Choose the most appropriate Ansible module\n",
       "3. Extract key parameters from fix_text\n",
       "4. Note any aspects requiring manual review\n",
       "5. Explain complexity in complexity_notes\n",
       "\n",
       "Prefer these modules:\n",
       "- \"lineinfile\" for single line changes\n",
       "- \"file\" for permission/ownership\n",
       "- \"systemd\" for service management\n",
       "- \"command\" as last resort\n",
       "\n",
       "Always set manual_review_required=true for fallback cases.\n",
       "\n",
       "**Crucial Instructions:**\n",
       "* ONLY respond in JSON format\n",
       "* DO NOT include explanations, reasoning, or additional text\n",
       "* Return only valid JSON matching the schema\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process one category at a time for manual inspection\n",
    "# 🎛️ Select which category to process\n",
    "\n",
    "current_category = None  # Set this to process a specific category, e.g., 'SHELL_SCRIPT'\n",
    "current_finding_in_category = 0  # Which finding in the category to process\n",
    "\n",
    "# Auto-select first category if none specified\n",
    "if current_category is None and categories_to_process:\n",
    "    current_category = list(categories_to_process.keys())[0]\n",
    "    print(f\"🎯 Auto-selected category: {current_category}\")\n",
    "\n",
    "if current_category and current_category in categories_to_process:\n",
    "    category_findings = categories_to_process[current_category]\n",
    "    \n",
    "    if current_finding_in_category < len(category_findings):\n",
    "        print(f\"🔄 Processing {current_category}\")\n",
    "        print(f\"📊 Finding {current_finding_in_category + 1}/{len(category_findings)} in this category\")\n",
    "        \n",
    "        # Get the prompt for this category\n",
    "        prompt_name = category_prompts.get(current_category, 'prompt_8_fallback')\n",
    "        processing_prompt = load_prompt(prompt_name)\n",
    "        \n",
    "        print(f\"📄 Using prompt: {processing_prompt['name']}\")\n",
    "        print(f\"🌡️ Temperature: {processing_prompt['parameters']['temperature']}\")\n",
    "        print(f\"🎯 Max tokens: {processing_prompt['parameters']['max_tokens']}\")\n",
    "        \n",
    "        # Get the finding to process\n",
    "        result_to_process = category_findings[current_finding_in_category]\n",
    "        finding = result_to_process['finding']\n",
    "        \n",
    "        print(f\"\\n📋 Processing Finding:\")\n",
    "        print(f\"   Rule ID: {finding.get('rule_id', 'Unknown')}\")\n",
    "        print(f\"   Classification: {result_to_process['classification']}\")\n",
    "        print(f\"   Title: {finding.get('title', 'Unknown')[:60]}...\")\n",
    "        \n",
    "        # Format the processing prompt\n",
    "        formatted_prompt = format_prompt(\n",
    "            processing_prompt,\n",
    "            rule_id=finding.get('rule_id', ''),\n",
    "            title=finding.get('title', ''),\n",
    "            description=finding.get('description', ''),\n",
    "            fix_text=finding.get('fix_text', '')[:3000]  # Limit for processing\n",
    "        )\n",
    "        \n",
    "        # Display the formatted prompt for inspection\n",
    "        print(f\"\\n📋 Formatted Processing Prompt:\")\n",
    "        display_prompt(formatted_prompt, max_length=1200)\n",
    "        \n",
    "    else:\n",
    "        print(f\"✅ All findings in {current_category} category processed\")\n",
    "        print(f\"📊 Processed: {len(category_findings)} findings\")\n",
    "else:\n",
    "    print(f\"⚠️ No category selected or category not found\")\n",
    "    if categories_to_process:\n",
    "        print(f\"Available categories: {list(categories_to_process.keys())}\")\n",
    "        print(f\"Set current_category to one of these values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Executing processing for xccdf_org.ssgproject.content_rule_prefer_64bit_os...\n",
      "🎯 Using max_tokens: 5000\n",
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 835 characters\n",
      "📝 Raw response preview: {\n",
      "  \"core_security_objective\": \"Ensure the system is running on a 64-bit OS\",\n",
      "  \"ansible_module\": \"command\",\n",
      "  \"key_parameters\": {\n",
      "    \"cmd\": \"uname -m\"\n",
      "  },\n",
      "  \"manual_review_required\": true,\n",
      "  \"compl...\n",
      "✅ Extracted JSON with 5 keys\n",
      "⚠️ JSON missing keys: ['target_type', 'target_name', 'ansible_params']\n",
      "\n",
      "✅ Processing Result:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ✅ unknown Processing Result:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ansible_module": "command",
       "ansible_params": "unknown",
       "complexity_notes": "This finding requires checking the output of 'uname -m' to ensure it indicates a 64-bit architecture. Manual review is necessary to interpret the result and decide on further actions if needed.",
       "core_security_objective": "Ensure the system is running on a 64-bit OS",
       "key_parameters": {
        "cmd": "uname -m"
       },
       "manual_review_required": true,
       "target_name": "unknown",
       "target_type": "unknown"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Stored result: ✅ Success\n",
      "📊 Progress: 3 findings processed\n"
     ]
    }
   ],
   "source": [
    "# Execute the processing for the current finding\n",
    "# Run this cell after reviewing the prompt above\n",
    "\n",
    "if (current_category and current_category in categories_to_process and \n",
    "    current_finding_in_category < len(categories_to_process[current_category]) and\n",
    "    'formatted_prompt' in locals() and llm):\n",
    "    \n",
    "    result_to_process = categories_to_process[current_category][current_finding_in_category]\n",
    "    finding = result_to_process['finding']\n",
    "    \n",
    "    print(f\"🚀 Executing processing for {finding.get('rule_id', 'Unknown')}...\")\n",
    "    \n",
    "    try:\n",
    "        # Expected keys vary by category but commonly include these\n",
    "        expected_keys = ['target_type', 'target_name', 'ansible_module', 'ansible_params']\n",
    "        \n",
    "        # Make the LLM call\n",
    "        processing_result = await llm_call_with_json(\n",
    "            llm,\n",
    "            formatted_prompt,\n",
    "            expected_keys,\n",
    "            max_retries=3,\n",
    "            prompt_params=processing_prompt['parameters']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ Processing Result:\")\n",
    "        display_result(f\"{current_category} Processing\", processing_result)\n",
    "        \n",
    "        # Store the result\n",
    "        complete_result = {\n",
    "            'finding_index': result_to_process['finding_index'],\n",
    "            'rule_id': finding.get('rule_id', ''),\n",
    "            'classification': result_to_process['classification'],\n",
    "            'category': current_category,\n",
    "            'processing_result': processing_result,\n",
    "            'finding': finding,\n",
    "            'processing_successful': all(key in processing_result for key in ['target_type', 'ansible_module']),\n",
    "            'processed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        processing_results.append(complete_result)\n",
    "        \n",
    "        success_status = \"✅ Success\" if complete_result['processing_successful'] else \"⚠️ Partial\"\n",
    "        print(f\"💾 Stored result: {success_status}\")\n",
    "        print(f\"📊 Progress: {len(processing_results)} findings processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Processing error: {e}\")\n",
    "        processing_errors.append({\n",
    "            'finding_index': result_to_process['finding_index'],\n",
    "            'rule_id': finding.get('rule_id', ''),\n",
    "            'classification': result_to_process['classification'],\n",
    "            'category': current_category,\n",
    "            'error': str(e),\n",
    "            'finding': finding,\n",
    "            'error_at': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        print(f\"💾 Stored error for analysis\")\n",
    "        \n",
    "else:\n",
    "    print(f\"⚠️ Cannot execute processing:\")\n",
    "    print(f\"   Current category: {current_category}\")\n",
    "    print(f\"   Finding index: {current_finding_in_category}\")\n",
    "    print(f\"   Prompt available: {'formatted_prompt' in locals()}\")\n",
    "    print(f\"   LLM available: {llm is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Advancing to next finding in unknown\n",
      "📊 Now at finding 3/3\n",
      "🎛️ Go back to process the next finding\n",
      "\n",
      "🎛️ Updated variables:\n",
      "   current_category = 'unknown'\n",
      "   current_finding_in_category = 2\n"
     ]
    }
   ],
   "source": [
    "# Advance to next finding - run this to continue processing\n",
    "# 🎛️ This will auto-advance to the next finding in the current category,\n",
    "# or move to the next category when the current one is finished\n",
    "\n",
    "if current_category and categories_to_process:\n",
    "    category_findings = categories_to_process[current_category]\n",
    "    \n",
    "    # Check if current category is finished\n",
    "    if current_finding_in_category + 1 >= len(category_findings):\n",
    "        print(f\"✅ Finished processing category: {current_category}\")\n",
    "        print(f\"📊 Processed {len(category_findings)} findings in this category\")\n",
    "        \n",
    "        # Move to next category\n",
    "        categories_list = list(categories_to_process.keys())\n",
    "        current_category_index = categories_list.index(current_category)\n",
    "        \n",
    "        if current_category_index + 1 < len(categories_list):\n",
    "            current_category = categories_list[current_category_index + 1]\n",
    "            current_finding_in_category = 0\n",
    "            print(f\"🔄 Moving to next category: {current_category}\")\n",
    "            print(f\"📊 {len(categories_to_process[current_category])} findings in this category\")\n",
    "        else:\n",
    "            print(f\"🎉 ALL CATEGORIES COMPLETED!\")\n",
    "            print(f\"📊 Total processed: {len(processing_results)} findings\")\n",
    "            print(f\"❌ Total errors: {len(processing_errors)} findings\")\n",
    "            current_category = None\n",
    "    else:\n",
    "        # Advance within current category\n",
    "        current_finding_in_category += 1\n",
    "        print(f\"🔄 Advancing to next finding in {current_category}\")\n",
    "        print(f\"📊 Now at finding {current_finding_in_category + 1}/{len(category_findings)}\")\n",
    "        print(f\"🎛️ Go back to process the next finding\")\n",
    "        \n",
    "    # Update the variables for the next iteration\n",
    "    print(f\"\\n🎛️ Updated variables:\")\n",
    "    print(f\"   current_category = '{current_category}'\")\n",
    "    print(f\"   current_finding_in_category = {current_finding_in_category}\")\n",
    "else:\n",
    "    print(f\"⚠️ No category processing in progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 PROCESSING SUMMARY\n",
      "=========================\n",
      "Phase 1 - Classification:\n",
      "   Successful: 3\n",
      "   Errors: 0\n",
      "\n",
      "Phase 2 - Processing:\n",
      "   Successful: 3\n",
      "   Errors: 0\n",
      "   Fully successful: 3\n",
      "   Partial results: 0\n",
      "\n",
      "📈 Success by category:\n",
      "   unknown: 3/3 (100.0%)\n",
      "\n",
      "🔍 Sample successful results:\n",
      "   1. xccdf_org.ssgproject.content_r...\n",
      "      Category: unknown\n",
      "      Target: unknown\n",
      "      Module: command\n",
      "   2. xccdf_org.ssgproject.content_r...\n",
      "      Category: unknown\n",
      "      Target: unknown\n",
      "      Module: unknown\n",
      "\n",
      "⏳ 2 findings still need processing\n",
      "🔄 Continue with the processing cells above\n"
     ]
    }
   ],
   "source": [
    "# Processing summary and results analysis\n",
    "print(\"📊 PROCESSING SUMMARY\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "print(f\"Phase 1 - Classification:\")\n",
    "print(f\"   Successful: {len(classification_results)}\")\n",
    "print(f\"   Errors: {len(classification_errors)}\")\n",
    "\n",
    "print(f\"\\nPhase 2 - Processing:\")\n",
    "print(f\"   Successful: {len(processing_results)}\")\n",
    "print(f\"   Errors: {len(processing_errors)}\")\n",
    "\n",
    "if processing_results:\n",
    "    successful_processing = [r for r in processing_results if r['processing_successful']]\n",
    "    partial_processing = [r for r in processing_results if not r['processing_successful']]\n",
    "    \n",
    "    print(f\"   Fully successful: {len(successful_processing)}\")\n",
    "    print(f\"   Partial results: {len(partial_processing)}\")\n",
    "    \n",
    "    # Show processing by category\n",
    "    category_success = {}\n",
    "    for result in processing_results:\n",
    "        category = result['category']\n",
    "        if category not in category_success:\n",
    "            category_success[category] = {'total': 0, 'successful': 0}\n",
    "        category_success[category]['total'] += 1\n",
    "        if result['processing_successful']:\n",
    "            category_success[category]['successful'] += 1\n",
    "    \n",
    "    print(f\"\\n📈 Success by category:\")\n",
    "    for category, stats in category_success.items():\n",
    "        success_rate = stats['successful'] / stats['total'] * 100\n",
    "        print(f\"   {category}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # Show sample successful results\n",
    "    if successful_processing:\n",
    "        print(f\"\\n🔍 Sample successful results:\")\n",
    "        for i, result in enumerate(successful_processing[:2]):\n",
    "            processing_data = result['processing_result']\n",
    "            print(f\"   {i+1}. {result['rule_id'][:30]}...\")\n",
    "            print(f\"      Category: {result['category']}\")\n",
    "            print(f\"      Target: {processing_data.get('target_type', 'Unknown')}\")\n",
    "            print(f\"      Module: {processing_data.get('ansible_module', 'Unknown')}\")\n",
    "\n",
    "total_processed = len(processing_results) + len(processing_errors)\n",
    "remaining = len(llm_findings) - total_processed\n",
    "\n",
    "if remaining > 0:\n",
    "    print(f\"\\n⏳ {remaining} findings still need processing\")\n",
    "    print(f\"🔄 Continue with the processing cells above\")\n",
    "else:\n",
    "    print(f\"\\n✅ All {len(llm_findings)} findings processed!\")\n",
    "    print(f\"🎯 Ready for playbook generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Ansible Playbooks and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating Ansible Playbooks\n",
      "==============================\n",
      "📊 Generating playbooks for 3 successful results...\n",
      "💾 Saved LLM targets to: ../playbooks/20250714_120000/llm_processed/llm_generated_targets_20250714_120000.json\n",
      "❌ Error generating LLM playbook: 'str' object has no attribute 'copy'\n",
      "\n",
      "📊 Generation Summary:\n",
      "   Playbooks generated: 0\n",
      "   Items for human review: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rk/l45jnpxn3g1b9865j26zbbmr0000gn/T/ipykernel_13798/376769460.py\", line 68, in <module>\n",
      "    playbook = generator.generate_playbook_from_targets(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks/../src/ansible_playbook_generator.py\", line 57, in generate_playbook_from_targets\n",
      "    playbook = self._create_playbook_structure(metadata, grouped_targets)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks/../src/ansible_playbook_generator.py\", line 133, in _create_playbook_structure\n",
      "    playbook['tasks'].extend(self._generate_tasks_for_group(group_name, targets))\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks/../src/ansible_playbook_generator.py\", line 156, in _generate_tasks_for_group\n",
      "    task = self._generate_single_task(target, group_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks/../src/ansible_playbook_generator.py\", line 167, in _generate_single_task\n",
      "    target['ansible_module']: target['ansible_params'].copy(),\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'str' object has no attribute 'copy'\n"
     ]
    }
   ],
   "source": [
    "# Generate Ansible playbooks from successful processing results\n",
    "print(\"🚀 Generating Ansible Playbooks\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "successful_processing = [r for r in processing_results if r['processing_successful']]\n",
    "partial_processing = [r for r in processing_results if not r['processing_successful']]\n",
    "\n",
    "playbooks_generated = 0\n",
    "human_review_needed = []\n",
    "\n",
    "if successful_processing:\n",
    "    print(f\"📊 Generating playbooks for {len(successful_processing)} successful results...\")\n",
    "    \n",
    "    # Convert processing results to target format for deterministic generator\n",
    "    llm_targets = []\n",
    "    \n",
    "    for result in successful_processing:\n",
    "        processing_data = result['processing_result']\n",
    "        finding = result['finding']\n",
    "        \n",
    "        # Create target in the format expected by deterministic generator\n",
    "        target = {\n",
    "            'rule_id': result['rule_id'],\n",
    "            'severity': finding.get('severity', 'medium'),\n",
    "            'status': finding.get('status', 'fail'),\n",
    "            'title': finding.get('title', ''),\n",
    "            'target_type': processing_data.get('target_type', 'unknown'),\n",
    "            'target_name': processing_data.get('target_name', ''),\n",
    "            'action_context': processing_data.get('action_context', ''),\n",
    "            'ansible_module': processing_data.get('ansible_module', 'debug'),\n",
    "            'ansible_params': processing_data.get('ansible_params', {}),\n",
    "            'compliance': {\n",
    "                'cci_refs': finding.get('compliance', {}).get('cci_refs', []),\n",
    "                'nist_refs': finding.get('compliance', {}).get('nist_refs', []),\n",
    "                'cis_refs': finding.get('compliance', {}).get('cis_refs', [])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        llm_targets.append(target)\n",
    "    \n",
    "    # Save LLM targets file\n",
    "    llm_targets_file = llm_output_dir / f\"llm_generated_targets_{RUN_TIMESTAMP}.json\"\n",
    "    llm_targets_data = {\n",
    "        'metadata': {\n",
    "            'total_actionable': len(llm_targets),\n",
    "            'extraction_date': datetime.now().isoformat(),\n",
    "            'source': 'llm_batch_processing',\n",
    "            'run_timestamp': RUN_TIMESTAMP,\n",
    "            'processing_summary': {\n",
    "                'classified': len(classification_results),\n",
    "                'processed': len(processing_results),\n",
    "                'successful': len(successful_processing)\n",
    "            }\n",
    "        },\n",
    "        'targets': llm_targets\n",
    "    }\n",
    "    \n",
    "    with open(llm_targets_file, 'w') as f:\n",
    "        json.dump(llm_targets_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved LLM targets to: {llm_targets_file}\")\n",
    "    \n",
    "    # Generate playbook using deterministic generator\n",
    "    try:\n",
    "        generator = DeterministicPlaybookGenerator()\n",
    "        playbook_file = llm_output_dir / f\"llm_generated_playbook_{RUN_TIMESTAMP}.yml\"\n",
    "        \n",
    "        playbook = generator.generate_playbook_from_targets(\n",
    "            str(llm_targets_file),\n",
    "            str(playbook_file)\n",
    "        )\n",
    "        \n",
    "        playbooks_generated = 1\n",
    "        print(f\"✅ Generated LLM playbook: {playbook_file}\")\n",
    "        \n",
    "        # Show playbook stats\n",
    "        total_tasks = sum(len(play.get('tasks', [])) for play in playbook)\n",
    "        print(f\"📊 LLM Playbook stats: {total_tasks} tasks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating LLM playbook: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(f\"⚠️ No successful processing results to convert to playbooks\")\n",
    "\n",
    "# Collect items needing human review\n",
    "human_review_needed.extend(partial_processing)\n",
    "human_review_needed.extend(processing_errors)\n",
    "human_review_needed.extend(classification_errors)\n",
    "\n",
    "print(f\"\\n📊 Generation Summary:\")\n",
    "print(f\"   Playbooks generated: {playbooks_generated}\")\n",
    "print(f\"   Items for human review: {len(human_review_needed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No items need human review - all processing successful!\n"
     ]
    }
   ],
   "source": [
    "# Save items requiring human review\n",
    "if human_review_needed:\n",
    "    print(f\"💾 Saving {len(human_review_needed)} items for human review...\")\n",
    "    \n",
    "    human_review_file = human_review_dir / f\"human_review_needed_{RUN_TIMESTAMP}.json\"\n",
    "    \n",
    "    human_review_data = {\n",
    "        'metadata': {\n",
    "            'total_items': len(human_review_needed),\n",
    "            'created_date': datetime.now().isoformat(),\n",
    "            'source': 'llm_batch_processing_failures',\n",
    "            'run_timestamp': RUN_TIMESTAMP,\n",
    "            'description': 'Findings that could not be processed successfully and require human review'\n",
    "        },\n",
    "        'items': human_review_needed\n",
    "    }\n",
    "    \n",
    "    with open(human_review_file, 'w') as f:\n",
    "        json.dump(human_review_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Saved human review items to: {human_review_file}\")\n",
    "    \n",
    "    # Analyze human review items\n",
    "    review_types = {}\n",
    "    for item in human_review_needed:\n",
    "        if 'error' in item:\n",
    "            item_type = 'processing_error'\n",
    "        elif 'processing_successful' in item and not item['processing_successful']:\n",
    "            item_type = 'partial_result'\n",
    "        else:\n",
    "            item_type = 'classification_error'\n",
    "        \n",
    "        review_types[item_type] = review_types.get(item_type, 0) + 1\n",
    "    \n",
    "    print(f\"📈 Human review breakdown: {review_types}\")\n",
    "else:\n",
    "    print(f\"✅ No items need human review - all processing successful!\")\n",
    "    human_review_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved final summary to: ../playbooks/20250714_120000/llm_processing_summary_20250714_120000.json\n"
     ]
    }
   ],
   "source": [
    "# Create final processing summary\n",
    "final_summary = {\n",
    "    'run_timestamp': RUN_TIMESTAMP,\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'input_file': LLM_NEEDED_FILE,\n",
    "    'processing_limits': {\n",
    "        'max_findings_processed': MAX_FINDINGS_TO_PROCESS,\n",
    "        'actual_findings_processed': len(llm_findings),\n",
    "        'total_findings_available': len(all_llm_findings)\n",
    "    },\n",
    "    'phase_1_classification': {\n",
    "        'attempted': len(classification_results),\n",
    "        'successful': len([r for r in classification_results if r['classification_successful']]),\n",
    "        'errors': len(classification_errors),\n",
    "        'categories_found': list(set([r['classification'] for r in classification_results if r['classification_successful']]))\n",
    "    },\n",
    "    'phase_2_processing': {\n",
    "        'attempted': len(processing_results),\n",
    "        'successful': len([r for r in processing_results if r['processing_successful']]),\n",
    "        'partial': len([r for r in processing_results if not r['processing_successful']]),\n",
    "        'errors': len(processing_errors)\n",
    "    },\n",
    "    'output_generation': {\n",
    "        'playbooks_generated': playbooks_generated,\n",
    "        'human_review_items': len(human_review_needed)\n",
    "    },\n",
    "    'output_files': {\n",
    "        'llm_targets': str(llm_targets_file) if 'llm_targets_file' in locals() else None,\n",
    "        'llm_playbook': str(playbook_file) if 'playbook_file' in locals() else None,\n",
    "        'human_review': str(human_review_file) if human_review_needed else None\n",
    "    },\n",
    "    'success_rates': {\n",
    "        'classification_rate': len([r for r in classification_results if r['classification_successful']]) / len(classification_results) * 100 if classification_results else 0,\n",
    "        'processing_rate': len([r for r in processing_results if r['processing_successful']]) / len(processing_results) * 100 if processing_results else 0,\n",
    "        'overall_success_rate': len([r for r in processing_results if r['processing_successful']]) / len(llm_findings) * 100 if llm_findings else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save final summary\n",
    "summary_file = Path(PLAYBOOKS_RUN_DIR) / f\"llm_processing_summary_{RUN_TIMESTAMP}.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"💾 Saved final summary to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 LLM BATCH PROCESSING COMPLETE\n",
      "========================================\n",
      "Run timestamp: 20250714_120000\n",
      "Processing date: 2025-07-14 11:12:14\n",
      "\n",
      "📊 FINAL STATISTICS:\n",
      "Input findings: 5 (limited from 1094)\n",
      "Classification success: 3/3 (100.0%)\n",
      "Processing success: 3/3 (100.0%)\n",
      "Overall success rate: 60.0%\n",
      "\n",
      "📁 OUTPUT FILES:\n",
      "   ✅ LLM Generated Playbook: ../playbooks/20250714_120000/llm_processed/llm_generated_playbook_20250714_120000.yml\n",
      "   📊 LLM Targets: ../playbooks/20250714_120000/llm_processed/llm_generated_targets_20250714_120000.json\n",
      "   📋 Processing Summary: ../playbooks/20250714_120000/llm_processing_summary_20250714_120000.json\n",
      "\n",
      "🎯 NEXT STEPS:\n",
      "   🔄 Increase MAX_FINDINGS_TO_PROCESS to process 1089 more findings\n",
      "   📝 Current limit: 5, Total available: 1094\n",
      "\n",
      "📈 OPTIMIZATION INSIGHTS:\n",
      "   ⚠️ Success rate below 80% - consider prompt improvements\n",
      "\n",
      "🎉 LLM batch processing workflow complete!\n",
      "📁 All outputs saved to: ../playbooks/20250714_120000\n"
     ]
    }
   ],
   "source": [
    "# Final summary and next steps\n",
    "print(\"🎯 LLM BATCH PROCESSING COMPLETE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"Processing date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\n📊 FINAL STATISTICS:\")\n",
    "print(f\"Input findings: {len(llm_findings)} (limited from {len(all_llm_findings)})\")\n",
    "print(f\"Classification success: {final_summary['phase_1_classification']['successful']}/{final_summary['phase_1_classification']['attempted']} ({final_summary['success_rates']['classification_rate']:.1f}%)\")\n",
    "print(f\"Processing success: {final_summary['phase_2_processing']['successful']}/{final_summary['phase_2_processing']['attempted']} ({final_summary['success_rates']['processing_rate']:.1f}%)\")\n",
    "print(f\"Overall success rate: {final_summary['success_rates']['overall_success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n📁 OUTPUT FILES:\")\n",
    "if final_summary['output_files']['llm_playbook']:\n",
    "    print(f\"   ✅ LLM Generated Playbook: {final_summary['output_files']['llm_playbook']}\")\n",
    "if final_summary['output_files']['llm_targets']:\n",
    "    print(f\"   📊 LLM Targets: {final_summary['output_files']['llm_targets']}\")\n",
    "if final_summary['output_files']['human_review']:\n",
    "    print(f\"   👤 Human Review Needed: {final_summary['output_files']['human_review']}\")\n",
    "print(f\"   📋 Processing Summary: {summary_file}\")\n",
    "\n",
    "print(f\"\\n🎯 NEXT STEPS:\")\n",
    "if final_summary['processing_limits']['actual_findings_processed'] < final_summary['processing_limits']['total_findings_available']:\n",
    "    remaining = final_summary['processing_limits']['total_findings_available'] - final_summary['processing_limits']['actual_findings_processed']\n",
    "    print(f\"   🔄 Increase MAX_FINDINGS_TO_PROCESS to process {remaining} more findings\")\n",
    "    print(f\"   📝 Current limit: {MAX_FINDINGS_TO_PROCESS}, Total available: {final_summary['processing_limits']['total_findings_available']}\")\n",
    "\n",
    "if final_summary['output_generation']['human_review_items'] > 0:\n",
    "    print(f\"   👤 Review {final_summary['output_generation']['human_review_items']} items requiring manual attention\")\n",
    "    print(f\"   🔧 Consider prompt engineering improvements based on failure patterns\")\n",
    "\n",
    "if final_summary['output_generation']['playbooks_generated'] > 0:\n",
    "    print(f\"   ✅ Test generated playbooks in a safe environment\")\n",
    "    print(f\"   🚀 Deploy playbooks to target systems\")\n",
    "\n",
    "print(f\"\\n📈 OPTIMIZATION INSIGHTS:\")\n",
    "if final_summary['success_rates']['overall_success_rate'] < 80:\n",
    "    print(f\"   ⚠️ Success rate below 80% - consider prompt improvements\")\n",
    "if final_summary['phase_1_classification']['successful'] < final_summary['phase_1_classification']['attempted']:\n",
    "    print(f\"   🎯 Classification issues - review prompt_1_classification\")\n",
    "if final_summary['phase_2_processing']['partial'] > 0:\n",
    "    print(f\"   🔧 {final_summary['phase_2_processing']['partial']} partial results - review category prompts\")\n",
    "\n",
    "print(f\"\\n🎉 LLM batch processing workflow complete!\")\n",
    "print(f\"📁 All outputs saved to: {PLAYBOOKS_RUN_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
