{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STIG to Ansible Prompt Engineering Workflow\n",
    "\n",
    "This notebook allows manual iteration through the 7-step workflow for converting STIG findings to Ansible playbooks.\n",
    "\n",
    "You can:\n",
    "- Test each prompt individually\n",
    "- Edit prompts in the `prompts/` directory and reload them\n",
    "- See the intermediate results at each step\n",
    "- Manually adjust the workflow state between steps\n",
    "- Compare with the automated simple_workflow.py implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Libraries imported successfully\n",
      "🐍 Python version: 3.11.12\n",
      "📁 Current working directory: /Users/wjackson/Developer/AI-Building-Blocks/ansible_playbook_from_stig/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "from string import Template\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from IPython.display import display, Markdown, JSON\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"📁 Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLM Interface initialized\n",
      "   Model: granite-3-3-8b-instruct\n",
      "   URL: https://granite-3-3-8b-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/completions\n",
      "✅ LLM initialized: granite-3-3-8b-instruct\n",
      "🔧 Model config: Not available\n"
     ]
    }
   ],
   "source": [
    "# Import LLM interface and initialize\n",
    "try:\n",
    "    from llm_interface import LLMInterface\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = LLMInterface()\n",
    "    print(f\"✅ LLM initialized: {llm.model_name}\")\n",
    "    print(f\"🔧 Model config: {getattr(llm, 'model_config', 'Not available')}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import LLMInterface: {e}\")\n",
    "    print(\"Please ensure llm_interface.py is in the ../src directory\")\n",
    "    llm = None\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to initialize LLM: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Helper functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for prompt engineering workflow\n",
    "\n",
    "def load_prompt(prompt_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load a prompt from the prompts directory\"\"\"\n",
    "    prompt_path = Path(f\"../prompts/{prompt_name}.yaml\")\n",
    "    try:\n",
    "        with open(prompt_path, 'r') as f:\n",
    "            prompt_data = yaml.safe_load(f)\n",
    "            print(f\"📄 Loaded prompt: {prompt_data.get('name', prompt_name)}\")\n",
    "            return prompt_data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Prompt file not found: {prompt_path}\")\n",
    "        return {'name': prompt_name, 'description': 'Not found', 'template': 'Prompt template not available'}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading prompt {prompt_name}: {e}\")\n",
    "        return {'name': prompt_name, 'description': 'Error loading', 'template': 'Error loading prompt template'}\n",
    "\n",
    "def format_prompt(prompt_data: Dict[str, Any], **kwargs) -> str:\n",
    "    \"\"\"Format a prompt template with provided variables\"\"\"\n",
    "    try:\n",
    "        template_str = prompt_data['template']\n",
    "        formatted = template_str.format(**kwargs)\n",
    "        return formatted\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error formatting prompt: {e}\")\n",
    "        return f\"Error formatting prompt: {e}\"\n",
    "\n",
    "def extract_json_from_response(response: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract JSON from LLM response using regex\"\"\"\n",
    "    try:\n",
    "        # Remove any markdown code blocks\n",
    "        response = re.sub(r'```json\\s*', '', response)\n",
    "        response = re.sub(r'```\\s*$', '', response)\n",
    "        \n",
    "        # Find JSON pattern - improved to handle nested objects\n",
    "        json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
    "        matches = re.findall(json_pattern, response, re.DOTALL)\n",
    "        \n",
    "        # Try each match to see if it's valid JSON\n",
    "        for match in matches:\n",
    "            try:\n",
    "                parsed = json.loads(match)\n",
    "                print(f\"✅ Extracted JSON with {len(parsed)} keys\")\n",
    "                return parsed\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        print(\"⚠️ No valid JSON found in response\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "async def llm_call_with_json(prompt: str, expected_keys: List[str], max_tokens: int = 100, max_retries: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"Make LLM call and extract JSON response with retry logic\"\"\"\n",
    "    if llm is None:\n",
    "        print(\"❌ LLM not initialized\")\n",
    "        return {key: \"llm_not_available\" for key in expected_keys}\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"🔄 LLM call attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Adjust prompt for retry attempts\n",
    "            if attempt == 0:\n",
    "                full_prompt = prompt\n",
    "            else:\n",
    "                full_prompt = f\"{prompt}\\n\\nIMPORTANT: The previous response was not valid JSON. Please respond with ONLY valid JSON containing these keys: {expected_keys}\"\n",
    "            \n",
    "            # Make the LLM call\n",
    "            response = await llm.generate_ansible_task_async(\n",
    "                prompt=full_prompt,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            print(f\"📝 Raw response length: {len(response)} characters\")\n",
    "            print(f\"📝 Raw response preview: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "            \n",
    "            # Try to extract JSON\n",
    "            json_data = extract_json_from_response(response)\n",
    "            \n",
    "            if json_data:\n",
    "                # Validate expected keys\n",
    "                missing_keys = [key for key in expected_keys if key not in json_data]\n",
    "                if not missing_keys:\n",
    "                    print(f\"✅ Valid JSON extracted with all expected keys\")\n",
    "                    return json_data\n",
    "                else:\n",
    "                    print(f\"⚠️ JSON missing keys: {missing_keys}\")\n",
    "                    # Fill in missing keys with default values\n",
    "                    for key in missing_keys:\n",
    "                        json_data[key] = \"unknown\"\n",
    "                    return json_data\n",
    "            else:\n",
    "                print(f\"⚠️ No valid JSON found in attempt {attempt + 1}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in LLM call attempt {attempt + 1}: {e}\")\n",
    "    \n",
    "    # If all retries failed, return fallback\n",
    "    print(f\"❌ All {max_retries} attempts failed\")\n",
    "    return {key: \"extraction_failed\" for key in expected_keys}\n",
    "\n",
    "def display_prompt(prompt_text: str, max_length: int = 10000):\n",
    "    \"\"\"Display a prompt nicely formatted with optional truncation\"\"\"\n",
    "    if len(prompt_text) > max_length:\n",
    "        truncated = prompt_text[:max_length] + \"\\n\\n... [TRUNCATED] ...\"\n",
    "        display(Markdown(f\"### 📋 Prompt (showing first {max_length} characters):\\n```\\n{truncated}\\n```\"))\n",
    "    else:\n",
    "        display(Markdown(f\"### 📋 Prompt:\\n```\\n{prompt_text}\\n```\"))\n",
    "\n",
    "def display_result(step_name: str, result: Dict[str, Any]):\n",
    "    \"\"\"Display step results nicely formatted\"\"\"\n",
    "    display(Markdown(f\"### ✅ {step_name} Result:\"))\n",
    "    display(JSON(result, expanded=True))\n",
    "\n",
    "print(\"🔧 Helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loaded 3058 findings from the file\n",
      "📈 Severity distribution:\n",
      "  medium: 2442\n",
      "  high: 138\n",
      "  low: 238\n",
      "  unknown: 240\n"
     ]
    }
   ],
   "source": [
    "# Load test findings data\n",
    "findings_file = \"../findings/node2.example.com-STIG-20250710162433_findings.json\"\n",
    "\n",
    "try:\n",
    "    with open(findings_file, 'r') as f:\n",
    "        findings_data = json.load(f)\n",
    "    \n",
    "    # Extract the findings array from the JSON structure\n",
    "    all_findings = findings_data.get('findings', [])\n",
    "    print(f\"📊 Loaded {len(all_findings)} findings from the file\")\n",
    "    \n",
    "    # Show summary of findings\n",
    "    severity_counts = {}\n",
    "    for finding in all_findings:\n",
    "        severity = finding.get('severity', 'unknown')\n",
    "        severity_counts[severity] = severity_counts.get(severity, 0) + 1\n",
    "    \n",
    "    print(\"📈 Severity distribution:\")\n",
    "    for severity, count in severity_counts.items():\n",
    "        print(f\"  {severity}: {count}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Findings file not found: {findings_file}\")\n",
    "    print(\"Please ensure the findings file exists in the correct location\")\n",
    "    all_findings = []\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading findings: {e}\")\n",
    "    all_findings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Found telnet-related finding: xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed\n",
      "\n",
      "📋 Selected Test Finding:\n",
      "   Rule ID: xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed\n",
      "   Title: Rule xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed\n",
      "   Severity: high\n",
      "   Description: No description available\n",
      "   Fix Text: No fix text available\n"
     ]
    }
   ],
   "source": [
    "# Select a test finding\n",
    "test_finding = None\n",
    "\n",
    "if all_findings:\n",
    "    # Try to find the telnet finding (commonly used for testing)\n",
    "    for finding in all_findings:\n",
    "        if 'telnet' in finding.get('rule_id', '').lower() or 'telnet' in finding.get('title', '').lower():\n",
    "            test_finding = finding\n",
    "            print(f\"🎯 Found telnet-related finding: {finding.get('rule_id', 'Unknown')}\")\n",
    "            break\n",
    "    \n",
    "    # If telnet not found, use the first finding\n",
    "    if not test_finding:\n",
    "        test_finding = all_findings[0]\n",
    "        print(f\"🎯 Using first finding: {test_finding.get('rule_id', 'Unknown')}\")\n",
    "    \n",
    "    # Display finding details\n",
    "    print(f\"\\n📋 Selected Test Finding:\")\n",
    "    print(f\"   Rule ID: {test_finding.get('rule_id', 'Unknown')}\")\n",
    "    print(f\"   Title: {test_finding.get('title', 'No title')}\")\n",
    "    print(f\"   Severity: {test_finding.get('severity', 'Unknown')}\")\n",
    "    print(f\"   Description: {test_finding.get('description', 'No description')[:150]}{'...' if len(test_finding.get('description', '')) > 150 else ''}\")\n",
    "    print(f\"   Fix Text: {test_finding.get('fix_text', 'No fix text')[:150]}{'...' if len(test_finding.get('fix_text', '')) > 150 else ''}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No findings available for testing\")\n",
    "    # Create a mock finding for testing\n",
    "    test_finding = {\n",
    "        'rule_id': 'test_rule_remove_telnet',\n",
    "        'title': 'Remove telnet package',\n",
    "        'severity': 'high',\n",
    "        'description': 'The telnet package should be removed as it poses security risks.',\n",
    "        'fix_text': 'Remove the telnet package using the package manager.',\n",
    "        'check_text': 'Verify telnet package is not installed.'\n",
    "    }\n",
    "    print(\"🧪 Created mock finding for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Workflow state initialized\n",
      "📝 Working with finding: xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed\n",
      "⏰ Started at: 2025-07-12T18:21:41.135594\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 📊 Initial Workflow State:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "finding_id": "xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed",
       "finding_title": "Rule xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed",
       "severity": "high",
       "workflow_start": "2025-07-12T18:21:41.135594"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize workflow state\n",
    "state = {\n",
    "    # Core workflow data\n",
    "    'finding': test_finding,\n",
    "    'action_type': None,\n",
    "    'target': None, \n",
    "    'parameters': None,\n",
    "    'task_name': None,\n",
    "    'final_playbook': None,\n",
    "    'validation_result': None,\n",
    "    'annotated_playbook': None,\n",
    "    \n",
    "    # Tracking and metadata\n",
    "    'errors': [],\n",
    "    'step_results': {},\n",
    "    'metadata': {\n",
    "        'workflow_start': datetime.now().isoformat(),\n",
    "        'rule_id': test_finding.get('rule_id', 'unknown') if test_finding else 'unknown',\n",
    "        'workflow_type': 'manual_prompt_engineering',\n",
    "        'steps_completed': []\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🚀 Workflow state initialized\")\n",
    "print(f\"📝 Working with finding: {state['metadata']['rule_id']}\")\n",
    "print(f\"⏰ Started at: {state['metadata']['workflow_start']}\")\n",
    "\n",
    "# Display current state\n",
    "display(Markdown(\"### 📊 Initial Workflow State:\"))\n",
    "state_summary = {\n",
    "    'finding_id': state['finding'].get('rule_id', 'unknown'),\n",
    "    'finding_title': state['finding'].get('title', 'unknown'),\n",
    "    'severity': state['finding'].get('severity', 'unknown'),\n",
    "    'workflow_start': state['metadata']['workflow_start']\n",
    "}\n",
    "display(JSON(state_summary, expanded=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 1: Extract Action Type\n",
    "\n",
    "This step extracts the primary action type from the STIG finding (e.g., remove_package, install_package, configure_file, etc.)\n",
    "\n",
    "**Prompt:** `extract_action.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Step 1: Extract Action Type\n",
      "📄 Loaded prompt: Extract Action Type\n",
      "📄 Prompt: Extract Action Type\n",
      "📝 Description: Extract the primary action needed to remediate this STIG finding\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 📋 Prompt:\n",
       "```\n",
       "You are analyzing a STIG security finding to determine what action is needed.\n",
       "\n",
       "STIG Finding:\n",
       "Title: Rule xccdf_org.ssgproject.content_rule_package_inetutils-telnetd_removed\n",
       "Description: No description available\n",
       "Fix Text: No fix text available\n",
       "\n",
       "The following is a reference guide for the action types you can choose from:\n",
       "<action_type_reference>\n",
       "# Comprehensive STIG Action Types for All Platforms\n",
       "\n",
       "## Action Type Categories and Definitions\n",
       "\n",
       "### Package Management\n",
       "\n",
       "* **`install_package`** - Install required security software\n",
       "* **`remove_package`** - Remove prohibited or vulnerable packages\n",
       "* **`update_package`** - Update packages to secure versions\n",
       "\n",
       "### File Operations\n",
       "\n",
       "* **`configure_file`** - Modify configuration files (most common)\n",
       "* **`create_file`** - Create required files (banners, policies, etc.)\n",
       "* **`remove_file`** - Delete prohibited or insecure files\n",
       "* **`set_permission`** - Set file/directory permissions and ownership\n",
       "\n",
       "### Service Management\n",
       "\n",
       "* **`configure_service`** - Modify service configuration\n",
       "* **`enable_service`** - Enable required services\n",
       "* **`disable_service`** - Disable prohibited services\n",
       "* **`start_service`** - Start required services\n",
       "* **`stop_service`** - Stop insecure services\n",
       "\n",
       "### User and Group Management\n",
       "\n",
       "* **`configure_user`** - Modify user account properties\n",
       "* **`create_user`** - Create required system accounts\n",
       "* **`remove_user`** - Remove prohibited accounts\n",
       "* **`configure_group`** - Modify group properties\n",
       "* **`create_group`** - Create required groups\n",
       "* **`remove_group`** - Remove unnecessary groups\n",
       "\n",
       "### Security Subsystems\n",
       "\n",
       "* **`configure_audit`** - Audit daemon rules and configuration\n",
       "* **`configure_logging`** - System logging configuration\n",
       "* **`configure_ssh`** - SSH daemon and client hardening\n",
       "* **`configure_pam`** - Pluggable Authentication Modules\n",
       "* **`configure_selinux`** - SELinux policies and booleans\n",
       "* **`configure_apparmor`** - AppArmor profile configuration\n",
       "\n",
       "### Network Security\n",
       "\n",
       "* **`configure_firewall`** - Firewall rules and policies\n",
       "* **`configure_network`** - Network interface and protocol settings\n",
       "* **`configure_tcp_wrappers`** - hosts.allow/hosts.deny configuration\n",
       "\n",
       "### System Configuration\n",
       "\n",
       "* **`configure_mount`** - Filesystem mount options\n",
       "* **`configure_grub`** - Bootloader security settings\n",
       "* **`configure_sysctl`** - Kernel parameters\n",
       "* **`configure_cron`** - Scheduled task management\n",
       "* **`configure_limits`** - System resource limits\n",
       "* **`configure_kernel_module`** - Load/blacklist kernel modules\n",
       "\n",
       "### Authentication and Access Control\n",
       "\n",
       "* **`configure_password_policy`** - Password complexity requirements\n",
       "* **`configure_login_banner`** - Login warning messages\n",
       "* **`configure_sudo`** - Sudo access and restrictions\n",
       "\n",
       "### Cryptography\n",
       "\n",
       "* **`configure_certificate`** - SSL/TLS certificate management\n",
       "* **`configure_encryption`** - Encryption settings and policies\n",
       "\n",
       "### Windows-Specific\n",
       "\n",
       "* **`configure_registry`** - Windows registry modifications\n",
       "* **`configure_gpo`** - Group Policy settings\n",
       "* **`configure_user_rights`** - User rights assignments\n",
       "* **`configure_security_policy`** - Local security policies\n",
       "\n",
       "### Application Services\n",
       "\n",
       "* **`configure_web_server`** - Apache, Nginx, IIS hardening\n",
       "* **`configure_database`** - Database security settings\n",
       "* **`configure_dns`** - DNS server configuration\n",
       "* **`configure_mail_server`** - Email server security\n",
       "* **`configure_ftp_server`** - FTP/SFTP server settings\n",
       "* **`configure_ldap`** - LDAP directory services\n",
       "* **`configure_active_directory`** - Active Directory settings\n",
       "\n",
       "### Cloud Platforms\n",
       "\n",
       "* **`configure_cloud_storage`** - S3, Azure Storage, GCS settings\n",
       "* **`configure_cloud_network`** - VPC, security groups, firewalls\n",
       "* **`configure_cloud_iam`** - Cloud identity and access management\n",
       "\n",
       "### Modern Infrastructure\n",
       "\n",
       "* **`configure_container`** - Docker container security\n",
       "* **`configure_kubernetes`** - Kubernetes cluster hardening\n",
       "* **`configure_virtualization`** - VMware, Hyper-V settings\n",
       "\n",
       "### Security Tools\n",
       "\n",
       "* **`configure_backup`** - Backup system configuration\n",
       "* **`configure_monitoring`** - Security monitoring setup\n",
       "* **`configure_antivirus`** - Antivirus/anti-malware settings\n",
       "* **`configure_patch_management`** - Patch management systems\n",
       "* **`configure_vulnerability_scanner`** - Vulnerability scanning tools\n",
       "* **`configure_intrusion_detection`** - IDS/IPS configuration\n",
       "\n",
       "### Execution and Verification\n",
       "\n",
       "* **`execute_command`** - Run commands for complex configurations\n",
       "* **`verify_configuration`** - Validate system settings\n",
       "* **`verify_compliance`** - Check compliance status\n",
       "\n",
       "### Fallback\n",
       "\n",
       "* **`other`** - Actions not fitting other categories\n",
       "\n",
       "## Platform Coverage\n",
       "\n",
       "This taxonomy covers STIGs for:\n",
       "\n",
       "### Operating Systems\n",
       "\n",
       "* Windows (Server 2016/2019/2022, Windows 10/11)\n",
       "* Linux (RHEL, Ubuntu, SUSE, Amazon Linux)\n",
       "* macOS\n",
       "* VMware ESXi\n",
       "* Cisco IOS\n",
       "* Network device operating systems\n",
       "\n",
       "### Applications\n",
       "\n",
       "* Web servers (Apache, Nginx, IIS)\n",
       "* Databases (Oracle, SQL Server, MySQL, PostgreSQL)\n",
       "* Mail servers (Exchange, Postfix)\n",
       "* DNS servers (BIND, Windows DNS)\n",
       "* FTP servers\n",
       "* Directory services (Active Directory, OpenLDAP)\n",
       "\n",
       "### Cloud Platforms\n",
       "\n",
       "* AWS services\n",
       "* Microsoft Azure\n",
       "* Google Cloud Platform\n",
       "* Cloud security configurations\n",
       "\n",
       "### Infrastructure\n",
       "\n",
       "* Docker containers\n",
       "* Kubernetes\n",
       "* VMware vSphere\n",
       "* Network devices (Cisco, Juniper, etc.)\n",
       "\n",
       "## Expected Coverage\n",
       "\n",
       "With these  **64 action types** , you should be able to categorize **95-98%** of all DISA STIG findings across:\n",
       "\n",
       "* ✅ All major operating systems\n",
       "* ✅ Network infrastructure devices\n",
       "* ✅ Database systems\n",
       "* ✅ Web and application servers\n",
       "* ✅ Cloud platforms and services\n",
       "* ✅ Container and virtualization platforms\n",
       "* ✅ Security tools and appliances\n",
       "\n",
       "The remaining 2-5% would fall into **`other`** for truly unique or complex multi-step procedures that don't fit standard patterns.\n",
       "\n",
       "</action_type_reference>\n",
       "\n",
       "JSON Schema for response:\n",
       "<extract_action_response_schema>\n",
       "{extract_action_response_schema}\n",
       "</extract_action_response_schema>\n",
       "\n",
       "Choose the primary action needed from the enumerated values. Return ONLY valid JSON matching the schema above.\n",
       "\n",
       "**Crucial Instructions:**\n",
       "* ONLY respond in JSON format\n",
       "* DO NOT include explanations, reasoning, or additional text\n",
       "* If the action type is not in the reference guide, use \"other\"\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 LLM call attempt 1/3\n",
      "📝 Raw response length: 74 characters\n",
      "📝 Raw response preview: </Crucial Instructions>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"action_type\": \"remove_package\"\n",
      "}\n",
      "```\n",
      "✅ Extracted JSON with 1 keys\n",
      "✅ Valid JSON extracted with all expected keys\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ✅ Step 1: Extract Action Type Result:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "action_type": "remove_package"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Extracted action type: remove_package\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract Action Type\n",
    "print(\"🎯 Starting Step 1: Extract Action Type\")\n",
    "\n",
    "# Load the prompt\n",
    "prompt_data = load_prompt('extract_action')\n",
    "print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "print(f\"📝 Description: {prompt_data['description']}\")\n",
    "\n",
    "# Get our variables together\n",
    "# First we need to load ansible_playbook_from_stig/reference/stig_action_type_definitions.md \n",
    "# # which we will pass to the prompt as action_type_reference\n",
    "\n",
    "# Read in the file\n",
    "with open('../reference/stig_action_type_definitions.md', 'r') as file:\n",
    "    action_type_reference = file.read()\n",
    "\n",
    "# Now we need our schema from ansible_playbook_from_stig/prompts/extract_action_response_schema.json\n",
    "# which we will pass to the prompt as extract_action_response_schema\n",
    "\n",
    "# Read in the file\n",
    "with open('../prompts/extract_action_response_schema.json', 'r') as file:\n",
    "    extract_action_response_schema = file.read()\n",
    "\n",
    "# Format the prompt with finding data\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    title=state['finding'].get('title', ''),\n",
    "    description=state['finding'].get('description', ''),\n",
    "    fix_text=state['finding'].get('fix_text', ''),\n",
    "    action_type_reference=action_type_reference,\n",
    "    extract_action_response_schema=extract_action_response_schema\n",
    ")\n",
    "\n",
    "# Display the formatted prompt\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['action_type'], max_tokens=50)\n",
    "\n",
    "# Update state\n",
    "state['action_type'] = result.get('action_type', 'unknown')\n",
    "state['step_results']['step1'] = result\n",
    "state['metadata']['steps_completed'].append('extract_action')\n",
    "state['metadata']['step1_complete'] = datetime.now().isoformat()\n",
    "\n",
    "# Display results\n",
    "display_result(\"Step 1: Extract Action Type\", result)\n",
    "print(f\"\\n✅ Extracted action type: {state['action_type']}\")\n",
    "\n",
    "# Note: You can manually override the result here if needed\n",
    "# state['action_type'] = 'remove_package'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 2: Extract Target\n",
    "\n",
    "This step extracts the target of the action (e.g., package name, file path, service name)\n",
    "\n",
    "**Prompt:** `extract_target.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Target\n",
    "print(\"🎯 Starting Step 2: Extract Target\")\n",
    "\n",
    "# Load the prompt\n",
    "prompt_data = load_prompt('extract_target')\n",
    "print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "print(f\"📝 Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with finding data and previous step result\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    title=state['finding'].get('title', ''),\n",
    "    fix_text=state['finding'].get('fix_text', ''),\n",
    "    action_type=state.get('action_type', 'other')\n",
    ")\n",
    "\n",
    "# Display the formatted prompt\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['target'], max_tokens=50)\n",
    "\n",
    "# Update state\n",
    "state['target'] = result.get('target', 'unknown')\n",
    "state['step_results']['step2'] = result\n",
    "state['metadata']['steps_completed'].append('extract_target')\n",
    "state['metadata']['step2_complete'] = datetime.now().isoformat()\n",
    "\n",
    "# Display results\n",
    "display_result(\"Step 2: Extract Target\", result)\n",
    "print(f\"\\n✅ Extracted target: {state['target']}\")\n",
    "\n",
    "# Note: You can manually override the result here if needed\n",
    "# state['target'] = 'telnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 3: Extract Parameters\n",
    "\n",
    "This step extracts any parameters needed for the action (e.g., state: absent, mode: 0644)\n",
    "\n",
    "**Prompt:** `extract_parameters.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract Parameters\n",
    "print(\"🎯 Starting Step 3: Extract Parameters\")\n",
    "\n",
    "# Load the prompt\n",
    "prompt_data = load_prompt('extract_parameters')\n",
    "print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "print(f\"📝 Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with finding data and previous step results\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    title=state['finding'].get('title', ''),\n",
    "    fix_text=state['finding'].get('fix_text', ''),\n",
    "    action_type=state.get('action_type', 'other'),\n",
    "    target=state.get('target', 'unknown')\n",
    ")\n",
    "\n",
    "# Display the formatted prompt\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['parameter'], max_tokens=50)\n",
    "\n",
    "# Update state\n",
    "state['parameters'] = result.get('parameter', 'default')\n",
    "state['step_results']['step3'] = result\n",
    "state['metadata']['steps_completed'].append('extract_parameters')\n",
    "state['metadata']['step3_complete'] = datetime.now().isoformat()\n",
    "\n",
    "# Display results\n",
    "display_result(\"Step 3: Extract Parameters\", result)\n",
    "print(f\"\\n✅ Extracted parameters: {state['parameters']}\")\n",
    "\n",
    "# Note: You can manually override the result here if needed\n",
    "# state['parameters'] = 'absent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 4: Generate Task Name\n",
    "\n",
    "This step generates a descriptive task name for the Ansible task\n",
    "\n",
    "**Prompt:** `generate_task_name.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Task Name\n",
    "print(\"🎯 Starting Step 4: Generate Task Name\")\n",
    "\n",
    "# Load the prompt\n",
    "prompt_data = load_prompt('generate_task_name')\n",
    "print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "print(f\"📝 Description: {prompt_data['description']}\")\n",
    "\n",
    "# Format the prompt with all extracted data\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    rule_id=state['finding'].get('rule_id', ''),\n",
    "    action_type=state.get('action_type', 'other'),\n",
    "    target=state.get('target', 'unknown'),\n",
    "    severity=state['finding'].get('severity', 'medium')\n",
    ")\n",
    "\n",
    "# Display the formatted prompt\n",
    "display_prompt(prompt)\n",
    "\n",
    "# Make the LLM call\n",
    "result = await llm_call_with_json(prompt, ['task_name'], max_tokens=100)\n",
    "\n",
    "# Update state\n",
    "state['task_name'] = result.get('task_name', f\"STIG Task: {state.get('target', 'unknown')}\")\n",
    "state['step_results']['step4'] = result\n",
    "state['metadata']['steps_completed'].append('generate_task_name')\n",
    "state['metadata']['step4_complete'] = datetime.now().isoformat()\n",
    "\n",
    "# Display results\n",
    "display_result(\"Step 4: Generate Task Name\", result)\n",
    "print(f\"\\n✅ Generated task name: {state['task_name']}\")\n",
    "\n",
    "# Note: You can manually override the result here if needed\n",
    "# state['task_name'] = 'STIG HIGH: Remove telnet package for security compliance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Review Extracted Components\n",
    "\n",
    "Let's review what we've extracted so far before assembling the playbook. You can manually adjust any values here if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review extracted components\n",
    "print(\"🔍 Extracted Components Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "components_summary = {\n",
    "    \"step_1_action_type\": state['action_type'],\n",
    "    \"step_2_target\": state['target'],\n",
    "    \"step_3_parameters\": state['parameters'],\n",
    "    \"step_4_task_name\": state['task_name'],\n",
    "    \"finding_info\": {\n",
    "        \"rule_id\": state['finding'].get('rule_id', 'unknown'),\n",
    "        \"severity\": state['finding'].get('severity', 'unknown'),\n",
    "        \"title\": state['finding'].get('title', 'unknown')\n",
    "    },\n",
    "    \"workflow_progress\": {\n",
    "        \"steps_completed\": len(state['metadata']['steps_completed']),\n",
    "        \"total_steps\": 7,\n",
    "        \"completed_steps\": state['metadata']['steps_completed']\n",
    "    }\n",
    "}\n",
    "\n",
    "display(Markdown(\"### 📊 Components Summary:\"))\n",
    "display(JSON(components_summary, expanded=True))\n",
    "\n",
    "print(\"\\n💡 Manual Override Options:\")\n",
    "print(\"You can manually adjust these values by uncommenting and modifying the lines below:\")\n",
    "print(\"# state['action_type'] = 'remove_package'\")\n",
    "print(\"# state['target'] = 'telnet'\")\n",
    "print(\"# state['parameters'] = 'absent'\")\n",
    "print(\"# state['task_name'] = 'Custom task name here'\")\n",
    "\n",
    "# Uncomment and modify these lines to manually override extracted values:\n",
    "# state['action_type'] = 'remove_package'\n",
    "# state['target'] = 'telnet'\n",
    "# state['parameters'] = 'absent'\n",
    "# state['task_name'] = 'STIG HIGH: Remove telnet package for security compliance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 5: Assemble Playbook\n",
    "\n",
    "This step combines all extracted components into a complete Ansible playbook\n",
    "\n",
    "**Prompt:** `assemble_playbook.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Assemble Playbook\n",
    "print(\"🎯 Starting Step 5: Assemble Playbook\")\n",
    "\n",
    "# Load the prompt\n",
    "prompt_data = load_prompt('assemble_playbook')\n",
    "print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "print(f\"📝 Description: {prompt_data['description']}\")\n",
    "\n",
    "# Load the template for reference (if available)\n",
    "template_content = \"\"\n",
    "template_path = '../examples/ansible_playbook_template.yaml'\n",
    "try:\n",
    "    with open(template_path, 'r') as f:\n",
    "        template_content = f.read()\n",
    "    print(\"✅ Loaded Ansible template for reference\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load template from {template_path}: {e}\")\n",
    "    # Provide a basic template as fallback\n",
    "    template_content = \"\"\"---\n",
    "- name: \"STIG Compliance Playbook\"\n",
    "  hosts: all\n",
    "  become: true\n",
    "  vars:\n",
    "    stig_enabled: true\n",
    "  \n",
    "  tasks:\n",
    "    - name: \"Sample task\"\n",
    "      debug:\n",
    "        msg: \"Replace with actual task\"\n",
    "      tags:\n",
    "        - stig\n",
    "        - security\"\"\"\n",
    "\n",
    "# Format the prompt with all components\n",
    "prompt = format_prompt(\n",
    "    prompt_data,\n",
    "    task_name=state.get('task_name', 'STIG Task'),\n",
    "    action_type=state.get('action_type', 'other'),\n",
    "    target=state.get('target', 'unknown'),\n",
    "    parameters=state.get('parameters', 'default'),\n",
    "    rule_id=state['finding'].get('rule_id', ''),\n",
    "    severity=state['finding'].get('severity', 'medium'),\n",
    "    template_content=template_content\n",
    ")\n",
    "\n",
    "# Show a truncated version of the prompt (since template can be long)\n",
    "display_prompt(prompt, max_length=800)\n",
    "\n",
    "# Make the LLM call with higher token limit for playbook generation\n",
    "result = await llm_call_with_json(prompt, ['playbook'], max_tokens=500)\n",
    "\n",
    "# Extract and clean the playbook\n",
    "playbook = result.get('playbook', '')\n",
    "\n",
    "# Clean up markdown formatting if present\n",
    "if playbook.startswith('```yaml'):\n",
    "    playbook = playbook[7:]\n",
    "if playbook.startswith('```'):\n",
    "    playbook = playbook[3:]\n",
    "if playbook.endswith('```'):\n",
    "    playbook = playbook[:-3]\n",
    "\n",
    "# Ensure it starts with document separator\n",
    "if not playbook.strip().startswith('---'):\n",
    "    playbook = '---\\n' + playbook\n",
    "\n",
    "# Update state\n",
    "state['final_playbook'] = playbook.strip()\n",
    "state['step_results']['step5'] = result\n",
    "state['metadata']['steps_completed'].append('assemble_playbook')\n",
    "state['metadata']['step5_complete'] = datetime.now().isoformat()\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n✅ Assembled playbook (length: {len(playbook)} characters)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 Generated Playbook:\")\n",
    "print(\"=\"*60)\n",
    "print(playbook)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 6: Validate and Fix Playbook\n",
    "\n",
    "This step validates the generated playbook and fixes any issues\n",
    "\n",
    "**Prompt:** `validate_and_fix_playbook.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Validate and Fix Playbook\n",
    "print(\"🎯 Starting Step 6: Validate and Fix Playbook\")\n",
    "\n",
    "if not state.get('final_playbook'):\n",
    "    print(\"❌ No playbook available for validation\")\n",
    "    state['errors'].append(\"No playbook available for validation\")\n",
    "else:\n",
    "    # Load the prompt\n",
    "    prompt_data = load_prompt('validate_and_fix_playbook')\n",
    "    print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "    print(f\"📝 Description: {prompt_data['description']}\")\n",
    "    \n",
    "    # Format the prompt with playbook and template\n",
    "    prompt = format_prompt(\n",
    "        prompt_data,\n",
    "        playbook_content=state['final_playbook'],\n",
    "        template_content=template_content if 'template_content' in locals() else ''\n",
    "    )\n",
    "    \n",
    "    # Show a truncated version\n",
    "    display_prompt(prompt, max_length=800)\n",
    "    \n",
    "    # Make the LLM call with higher token limit for validation\n",
    "    result = await llm_call_with_json(\n",
    "        prompt, \n",
    "        ['is_valid', 'issues_found', 'fixes_applied', 'fixed_playbook', 'suggestions'], \n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    # Update state\n",
    "    state['validation_result'] = result\n",
    "    state['step_results']['step6'] = result\n",
    "    state['metadata']['steps_completed'].append('validate_and_fix_playbook')\n",
    "    state['metadata']['step6_complete'] = datetime.now().isoformat()\n",
    "    \n",
    "    # If fixes were applied, update the playbook\n",
    "    if result.get('fixes_applied') and result.get('fixed_playbook'):\n",
    "        old_playbook = state['final_playbook']\n",
    "        state['final_playbook'] = result['fixed_playbook']\n",
    "        print(f\"\\n✅ Applied {len(result.get('fixes_applied', []))} fixes to playbook\")\n",
    "        print(f\"📏 Playbook length changed: {len(old_playbook)} → {len(state['final_playbook'])} characters\")\n",
    "    else:\n",
    "        print(\"\\n✅ No fixes were needed or applied\")\n",
    "    \n",
    "    # Display validation results\n",
    "    display_result(\"Step 6: Validate and Fix Playbook\", result)\n",
    "    \n",
    "    print(f\"\\n📊 Validation Summary:\")\n",
    "    print(f\"   ✅ Valid: {result.get('is_valid', False)}\")\n",
    "    print(f\"   🔍 Issues Found: {len(result.get('issues_found', []))}\")\n",
    "    print(f\"   🔧 Fixes Applied: {len(result.get('fixes_applied', []))}\")\n",
    "    print(f\"   💡 Suggestions: {len(result.get('suggestions', []))}\")\n",
    "    \n",
    "    if result.get('issues_found'):\n",
    "        print(\"\\n🔍 Issues Found:\")\n",
    "        for i, issue in enumerate(result['issues_found'], 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "    \n",
    "    if result.get('fixes_applied'):\n",
    "        print(\"\\n🔧 Fixes Applied:\")\n",
    "        for i, fix in enumerate(result['fixes_applied'], 1):\n",
    "            print(f\"   {i}. {fix}\")\n",
    "    \n",
    "    if result.get('suggestions'):\n",
    "        print(\"\\n💡 Improvement Suggestions:\")\n",
    "        for i, suggestion in enumerate(result['suggestions'], 1):\n",
    "            print(f\"   {i}. {suggestion}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📋 Final Validated Playbook:\")\n",
    "    print(\"=\"*60)\n",
    "    print(state['final_playbook'])\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 7: Annotate Playbook\n",
    "\n",
    "This step adds documentation and comments to the validated playbook\n",
    "\n",
    "**Prompt:** `annotate_playbook.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Annotate Playbook\n",
    "print(\"🎯 Starting Step 7: Annotate Playbook\")\n",
    "\n",
    "if not state.get('final_playbook'):\n",
    "    print(\"❌ No playbook available for annotation\")\n",
    "    state['errors'].append(\"No playbook available for annotation\")\n",
    "else:\n",
    "    # Load the prompt\n",
    "    prompt_data = load_prompt('annotate_playbook')\n",
    "    print(f\"📄 Prompt: {prompt_data['name']}\")\n",
    "    print(f\"📝 Description: {prompt_data['description']}\")\n",
    "    \n",
    "    # Format the prompt with playbook and finding details\n",
    "    prompt = format_prompt(\n",
    "        prompt_data,\n",
    "        playbook_content=state['final_playbook'],\n",
    "        rule_id=state['finding'].get('rule_id', ''),\n",
    "        title=state['finding'].get('title', ''),\n",
    "        severity=state['finding'].get('severity', ''),\n",
    "        description=state['finding'].get('description', ''),\n",
    "        check_text=state['finding'].get('check_text', ''),\n",
    "        fix_text=state['finding'].get('fix_text', ''),\n",
    "        references=', '.join(state['finding'].get('references', []))\n",
    "    )\n",
    "    \n",
    "    # Show a truncated version\n",
    "    display_prompt(prompt, max_length=800)\n",
    "    \n",
    "    # Make the LLM call with higher token limit for annotation\n",
    "    result = await llm_call_with_json(prompt, ['annotated_playbook'], max_tokens=600)\n",
    "    \n",
    "    # Update state\n",
    "    annotated_playbook = result.get('annotated_playbook', '')\n",
    "    \n",
    "    if annotated_playbook and annotated_playbook.strip():\n",
    "        # Clean up markdown formatting if present\n",
    "        if annotated_playbook.startswith('```yaml'):\n",
    "            annotated_playbook = annotated_playbook[7:]\n",
    "        if annotated_playbook.startswith('```'):\n",
    "            annotated_playbook = annotated_playbook[3:]\n",
    "        if annotated_playbook.endswith('```'):\n",
    "            annotated_playbook = annotated_playbook[:-3]\n",
    "        \n",
    "        state['annotated_playbook'] = annotated_playbook.strip()\n",
    "        print(\"\\n✅ Successfully annotated playbook\")\n",
    "        print(f\"📏 Annotation added {len(annotated_playbook) - len(state['final_playbook'])} characters\")\n",
    "    else:\n",
    "        state['annotated_playbook'] = state['final_playbook']\n",
    "        print(\"\\n⚠️ No annotation returned, keeping original playbook\")\n",
    "        state['errors'].append(\"Annotation step failed - no annotated playbook returned\")\n",
    "    \n",
    "    state['step_results']['step7'] = result\n",
    "    state['metadata']['steps_completed'].append('annotate_playbook')\n",
    "    state['metadata']['step7_complete'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📋 Final Annotated Playbook:\")\n",
    "    print(\"=\"*60)\n",
    "    print(state['annotated_playbook'])\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Workflow Summary\n",
    "\n",
    "Review the complete workflow results and save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize workflow metadata\n",
    "state['metadata']['workflow_complete'] = datetime.now().isoformat()\n",
    "state['metadata']['total_errors'] = len(state['errors'])\n",
    "state['metadata']['steps_completed_count'] = len(state['metadata']['steps_completed'])\n",
    "\n",
    "# Calculate workflow quality\n",
    "if state.get('final_playbook') and len(state['errors']) == 0:\n",
    "    validation = state.get('validation_result', {})\n",
    "    if validation.get('is_valid', False):\n",
    "        state['metadata']['final_quality'] = 'high'\n",
    "    else:\n",
    "        state['metadata']['final_quality'] = 'medium'\n",
    "elif state.get('final_playbook'):\n",
    "    state['metadata']['final_quality'] = 'low'\n",
    "else:\n",
    "    state['metadata']['final_quality'] = 'failed'\n",
    "\n",
    "# Workflow Summary\n",
    "print(\"🎯 WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_data = {\n",
    "    \"workflow_info\": {\n",
    "        \"finding_id\": state['finding'].get('rule_id', 'unknown'),\n",
    "        \"finding_title\": state['finding'].get('title', 'unknown'),\n",
    "        \"severity\": state['finding'].get('severity', 'unknown'),\n",
    "        \"workflow_quality\": state['metadata']['final_quality'],\n",
    "        \"total_errors\": state['metadata']['total_errors']\n",
    "    },\n",
    "    \"extracted_components\": {\n",
    "        \"action_type\": state['action_type'],\n",
    "        \"target\": state['target'],\n",
    "        \"parameters\": state['parameters'],\n",
    "        \"task_name\": state['task_name']\n",
    "    },\n",
    "    \"workflow_progress\": {\n",
    "        \"steps_completed\": state['metadata']['steps_completed_count'],\n",
    "        \"total_steps\": 7,\n",
    "        \"completed_steps\": state['metadata']['steps_completed'],\n",
    "        \"start_time\": state['metadata']['workflow_start'],\n",
    "        \"end_time\": state['metadata']['workflow_complete']\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"has_final_playbook\": bool(state.get('final_playbook')),\n",
    "        \"has_annotated_playbook\": bool(state.get('annotated_playbook')),\n",
    "        \"validation_passed\": state.get('validation_result', {}).get('is_valid', False),\n",
    "        \"playbook_length\": len(state.get('annotated_playbook') or state.get('final_playbook', '')),\n",
    "    }\n",
    "}\n",
    "\n",
    "display(Markdown(\"### 📊 Workflow Summary:\"))\n",
    "display(JSON(summary_data, expanded=True))\n",
    "\n",
    "# Show validation details if available\n",
    "validation = state.get('validation_result', {})\n",
    "if validation:\n",
    "    print(\"\\n📋 Validation Details:\")\n",
    "    print(f\"   ✅ Valid: {validation.get('is_valid', False)}\")\n",
    "    print(f\"   🔍 Issues: {len(validation.get('issues_found', []))}\")\n",
    "    print(f\"   🔧 Fixes: {len(validation.get('fixes_applied', []))}\")\n",
    "    print(f\"   💡 Suggestions: {len(validation.get('suggestions', []))}\")\n",
    "\n",
    "# Show errors if any\n",
    "if state['errors']:\n",
    "    print(\"\\n❌ Errors Encountered:\")\n",
    "    for i, error in enumerate(state['errors'], 1):\n",
    "        print(f\"   {i}. {error}\")\n",
    "\n",
    "# Display final output\n",
    "final_playbook = state.get('annotated_playbook') or state.get('final_playbook', 'No playbook generated')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 FINAL OUTPUT:\")\n",
    "print(\"=\"*60)\n",
    "print(final_playbook)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs to files\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "rule_id = state['finding'].get('rule_id', 'unknown')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../playbooks')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save final playbook\n",
    "final_playbook = state.get('annotated_playbook') or state.get('final_playbook')\n",
    "if final_playbook:\n",
    "    playbook_file = output_dir / f\"manual_{rule_id}_{timestamp}.yml\"\n",
    "    try:\n",
    "        with open(playbook_file, 'w') as f:\n",
    "            f.write(final_playbook)\n",
    "        print(f\"💾 Saved playbook to: {playbook_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save playbook: {e}\")\n",
    "\n",
    "# Save workflow state and results\n",
    "state_file = output_dir / f\"manual_{rule_id}_{timestamp}_state.json\"\n",
    "try:\n",
    "    # Create a serializable version of the state\n",
    "    serializable_state = {\n",
    "        'metadata': state['metadata'],\n",
    "        'extracted_components': {\n",
    "            'action_type': state['action_type'],\n",
    "            'target': state['target'],\n",
    "            'parameters': state['parameters'],\n",
    "            'task_name': state['task_name']\n",
    "        },\n",
    "        'validation_result': state.get('validation_result'),\n",
    "        'errors': state['errors'],\n",
    "        'step_results': state['step_results'],\n",
    "        'finding_info': {\n",
    "            'rule_id': state['finding'].get('rule_id'),\n",
    "            'title': state['finding'].get('title'),\n",
    "            'severity': state['finding'].get('severity')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(state_file, 'w') as f:\n",
    "        json.dump(serializable_state, f, indent=2)\n",
    "    print(f\"💾 Saved workflow state to: {state_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save workflow state: {e}\")\n",
    "\n",
    "# Create a transparency report\n",
    "report_file = output_dir / f\"manual_{rule_id}_{timestamp}_report.md\"\n",
    "try:\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(f\"# Manual Prompt Engineering Workflow Report\\n\\n\")\n",
    "        f.write(f\"**Generated:** {state['metadata']['workflow_complete']}\\n\")\n",
    "        f.write(f\"**Rule ID:** {rule_id}\\n\")\n",
    "        f.write(f\"**Finding Title:** {state['finding'].get('title', 'unknown')}\\n\")\n",
    "        f.write(f\"**Severity:** {state['finding'].get('severity', 'unknown')}\\n\")\n",
    "        f.write(f\"**Final Quality:** {state['metadata']['final_quality']}\\n\")\n",
    "        f.write(f\"**Total Errors:** {state['metadata']['total_errors']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Extracted Components\\n\\n\")\n",
    "        f.write(f\"1. **Action Type:** {state['action_type']}\\n\")\n",
    "        f.write(f\"2. **Target:** {state['target']}\\n\")\n",
    "        f.write(f\"3. **Parameters:** {state['parameters']}\\n\")\n",
    "        f.write(f\"4. **Task Name:** {state['task_name']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Workflow Steps\\n\\n\")\n",
    "        for i, step in enumerate(state['metadata']['steps_completed'], 1):\n",
    "            f.write(f\"{i}. ✅ {step}\\n\")\n",
    "        \n",
    "        if state['errors']:\n",
    "            f.write(\"\\n## Errors\\n\\n\")\n",
    "            for i, error in enumerate(state['errors'], 1):\n",
    "                f.write(f\"{i}. {error}\\n\")\n",
    "        \n",
    "        validation = state.get('validation_result', {})\n",
    "        if validation:\n",
    "            f.write(\"\\n## Validation Results\\n\\n\")\n",
    "            f.write(f\"- **Valid:** {validation.get('is_valid', False)}\\n\")\n",
    "            f.write(f\"- **Issues Found:** {len(validation.get('issues_found', []))}\\n\")\n",
    "            f.write(f\"- **Fixes Applied:** {len(validation.get('fixes_applied', []))}\\n\")\n",
    "            f.write(f\"- **Suggestions:** {len(validation.get('suggestions', []))}\\n\")\n",
    "    \n",
    "    print(f\"📋 Saved transparency report to: {report_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save transparency report: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Workflow completed with quality: {state['metadata']['final_quality']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Testing Different Findings\n",
    "\n",
    "You can test with different findings by selecting from the available findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available findings to test\n",
    "if all_findings:\n",
    "    print(\"🧪 Available findings to test (showing first 10):\\n\")\n",
    "    for i, finding in enumerate(all_findings[:10]):\n",
    "        severity_icon = {\"high\": \"🔴\", \"medium\": \"🟡\", \"low\": \"🟢\"}.get(finding.get('severity', '').lower(), \"⚪\")\n",
    "        print(f\"{i:2d}: {severity_icon} {finding.get('rule_id', 'unknown')} ({finding.get('severity', 'unknown')})\")\n",
    "        print(f\"     {finding.get('title', 'No title')[:70]}{'...' if len(finding.get('title', '')) > 70 else ''}\\n\")\n",
    "    \n",
    "    print(\"\\n💡 To test a different finding:\")\n",
    "    print(\"1. Choose an index from the list above\")\n",
    "    print(\"2. Update the cell below with the desired index\")\n",
    "    print(\"3. Re-run from the 'Select a test finding' cell onward\")\n",
    "    \n",
    "    print(\"\\n🔄 To test a different finding, uncomment and modify:\")\n",
    "    print(\"# selected_index = 3  # Change this to the desired finding index\")\n",
    "    print(\"# test_finding = all_findings[selected_index]\")\n",
    "    print(\"# print(f'Selected: {test_finding.get(\\\"rule_id\\\", \\\"unknown\\\")}')\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No findings available for testing\")\n",
    "\n",
    "# Uncomment and modify these lines to test a different finding:\n",
    "# selected_index = 3  # Change this to the desired finding index\n",
    "# test_finding = all_findings[selected_index]\n",
    "# print(f'🎯 Selected new finding: {test_finding.get(\"rule_id\", \"unknown\")}')\n",
    "# # Then re-run from the \"Initialize workflow state\" cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Prompt Engineering Notes\n",
    "\n",
    "Use this cell to document what worked and what didn't during your prompt engineering sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering Notes and Observations\n",
    "# \n",
    "# Date: [Add date here]\n",
    "# Model: [Add model info here]\n",
    "# Finding tested: [Add finding info here]\n",
    "# \n",
    "# 🟢 What worked well:\n",
    "# - JSON schema in prompts helps enforce structured output\n",
    "# - Short, focused prompts work better with smaller models\n",
    "# - Breaking down complex tasks into simple extractions\n",
    "# - Retry logic handles intermittent JSON parsing issues\n",
    "# - Template-based prompt formatting is consistent\n",
    "# \n",
    "# 🔴 What didn't work:\n",
    "# - Long, complex prompts with multiple instructions confuse smaller models\n",
    "# - Asking for too much output at once (>500 tokens) leads to truncation\n",
    "# - Model sometimes returns explanatory text instead of pure JSON\n",
    "# - Complex nested JSON structures are harder to extract reliably\n",
    "# \n",
    "# 🟡 Ideas to try:\n",
    "# - Add more examples in prompts for edge cases\n",
    "# - Test with different temperature settings\n",
    "# - Try alternative JSON extraction patterns\n",
    "# - Add validation of intermediate results between steps\n",
    "# - Experiment with different prompt ordering/structure\n",
    "# \n",
    "# 📊 Current Results Summary:\n",
    "# - Step 1 (Action): [Success rate / Notes]\n",
    "# - Step 2 (Target): [Success rate / Notes] \n",
    "# - Step 3 (Parameters): [Success rate / Notes]\n",
    "# - Step 4 (Task Name): [Success rate / Notes]\n",
    "# - Step 5 (Assemble): [Success rate / Notes]\n",
    "# - Step 6 (Validate): [Success rate / Notes]\n",
    "# - Step 7 (Annotate): [Success rate / Notes]\n",
    "# \n",
    "# 🔧 Prompt Modifications Made:\n",
    "# - [List any changes made to prompt files]\n",
    "# \n",
    "# 🎯 Next Steps:\n",
    "# - [List planned improvements or tests]\n",
    "\n",
    "print(\"📝 Use this cell to document your prompt engineering observations\")\n",
    "print(\"📁 Edit prompts in the ../prompts/ directory and re-run cells to test changes\")\n",
    "print(\"🔄 You can reload prompts by re-running any step that calls load_prompt()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Debugging and Troubleshooting\n",
    "\n",
    "Use these cells for debugging specific issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check current state\n",
    "print(\"🔍 Current Workflow State Debug:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "debug_info = {\n",
    "    \"workflow_status\": {\n",
    "        \"steps_completed\": len(state['metadata']['steps_completed']),\n",
    "        \"has_finding\": bool(state.get('finding')),\n",
    "        \"has_final_playbook\": bool(state.get('final_playbook')),\n",
    "        \"has_annotated_playbook\": bool(state.get('annotated_playbook')),\n",
    "        \"error_count\": len(state['errors'])\n",
    "    },\n",
    "    \"llm_status\": {\n",
    "        \"llm_initialized\": llm is not None,\n",
    "        \"llm_model\": getattr(llm, 'model_name', 'Not available') if llm else 'Not initialized'\n",
    "    },\n",
    "    \"file_status\": {\n",
    "        \"findings_loaded\": len(all_findings) if 'all_findings' in locals() else 0,\n",
    "        \"test_finding_selected\": bool(test_finding) if 'test_finding' in locals() else False\n",
    "    },\n",
    "    \"recent_errors\": state['errors'][-3:] if state['errors'] else []\n",
    "}\n",
    "\n",
    "display(JSON(debug_info, expanded=True))\n",
    "\n",
    "# Check if all required files exist\n",
    "required_files = [\n",
    "    '../src/llm_interface.py',\n",
    "    '../prompts/extract_action.yaml',\n",
    "    '../prompts/extract_target.yaml',\n",
    "    '../prompts/extract_parameters.yaml',\n",
    "    '../prompts/generate_task_name.yaml',\n",
    "    '../prompts/assemble_playbook.yaml',\n",
    "    '../prompts/validate_and_fix_playbook.yaml',\n",
    "    '../prompts/annotate_playbook.yaml'\n",
    "]\n",
    "\n",
    "print(\"\\n📁 File Check:\")\n",
    "for file_path in required_files:\n",
    "    exists = Path(file_path).exists()\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"   {status} {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Test individual prompt loading\n",
    "print(\"🔍 Testing Individual Prompt Loading:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "prompt_names = [\n",
    "    'extract_action',\n",
    "    'extract_target', \n",
    "    'extract_parameters',\n",
    "    'generate_task_name',\n",
    "    'assemble_playbook',\n",
    "    'validate_and_fix_playbook',\n",
    "    'annotate_playbook'\n",
    "]\n",
    "\n",
    "for prompt_name in prompt_names:\n",
    "    try:\n",
    "        prompt_data = load_prompt(prompt_name)\n",
    "        status = \"✅\" if 'template' in prompt_data else \"⚠️\"\n",
    "        template_length = len(prompt_data.get('template', ''))\n",
    "        print(f\"   {status} {prompt_name}: {template_length} characters\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ {prompt_name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Test LLM connection with simple prompt\n",
    "if llm is not None:\n",
    "    print(\"🔍 Testing LLM Connection:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    simple_test_prompt = 'Respond with only this JSON: {\"test\": \"success\", \"status\": \"working\"}'\n",
    "    \n",
    "    try:\n",
    "        test_result = await llm_call_with_json(simple_test_prompt, ['test', 'status'], max_tokens=50)\n",
    "        print(f\"✅ LLM Test Result: {test_result}\")\n",
    "        \n",
    "        if test_result.get('test') == 'success':\n",
    "            print(\"✅ LLM is responding correctly\")\n",
    "        else:\n",
    "            print(\"⚠️ LLM response may not be reliable\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM test failed: {e}\")\n",
    "else:\n",
    "    print(\"❌ LLM not initialized - cannot test connection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
